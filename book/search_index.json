[
["ml-industry.html", "Corporate Portfolio Bab 1 Telecommunication 1.1 Supply Chain Analytics 1.2 Incident Analysis 1.3 Customer Churn Prediction Bab 2 Finance 2.1 Credit Risk Analysis 2.2 Evaluating Customer Financial Complaints Bab 3 Retail 3.1 E-Commerce Clothing Reviews Bab 4 Insurance 4.1 Prediction of Total Claim Amount", " Corporate Portfolio Team Algoritma April 21, 2020 Bab 1 Telecommunication body { text-align: justify} 1.1 Supply Chain Analytics 1.1.1 Background Umumnya tiap perusahaan layanan telekomunikasi memiliki aplikasi yang digunakan untuk menyimpan data transaksi dari layanan-layanan yang diberikan. Dalam hal ini ada hubungan yang sangat kuat antara proses pemasangan baru di pelanggan dengan proses procurement atau pengadaan barang serta ketersediaan barang yang dibutuhkan. knitr::include_graphics(path = &quot;assets/01-telco/supply-chain-1.png&quot;) 1.1.2 Expected Outcome Yang diharapkan dari Supply Chain Management Learning Machine Model adalah selain dapat memprediksi untuk kebutuhan set top box atau ONT (modem) dapat juga untuk melihat gambaran setahun dari pelaksanaan pemasangan baru ONT/STB di service layanan. Dari data yang dianalisa kita dapat melihat mengenai sifat layanan yang disediakan oleh setiap transaksi dari data bulanan tersebut, apakah layanan tersebut sudah ‘mature’ karena layanan yang diberikan bukan layanan konsumsi tapi merupakan layanan instalasi sehingga terjadi kemungkinan terjadi kejenuhan di pelanggan terhadap layanan tersebut. Selain itu dari data timeseries tersebut masih dapat dilihat pertumbuhan dan pola penjualannya sehingga memungkinkan management untuk mengatur target terhadap teknisi dan target penjualan layanan tersebut dan memikirkan layanan baru yang dapat dijadikan tahap introduction/growth dan berlanjut ke proses perkembangan pemasaran product sebelum layanan atau produk tersebut benar-benar ditinggalkan. 1.1.3 Recommendation Setelah mengamati hasil analisa dan pengukuran serta gambaran prediksi dari hasil proses machine learning maka rekomendasi yang dapat diberikan adalah bahwa selain target dalam penjualan per bulan, barang yang harusnya dibeli jika tidak diproduksi lagi harus diback-up dengan barang/item tipe yang lain. Selain itu dari kurva yang ada dapat dilihat bahwa masa periode siklus hidup layanan tersebut masih bagus namun sebaiknya mulai disiapkan untuk layanan yang baru seperti content yang bisa dijual lewat layanan data, baik konten untuk IPTV atau TV berbayar maupun konten untuk layanan data/internetnya untuk mengantisipasi turunnya kurva life cycle product. 1.2 Incident Analysis 1.2.1 Background Indikator kepuasan pelanggan dapat ditunjukkan dengan nilai NPS (Net Promoter Score). Net Promoter Score dapat menunjukkan secara langsung tipe konsumen yang loyal dan dapat memberikan prediksi tingkah laku konsumen terhadap bisnis Telkom. Score ini menjadi indikator berapa banyak konsumen Anda yang mau merekomendasikan layanan kita ke orang lain. Selain itu, NPS Menunjukan interaksi personal berdasarkan pemahaman mendalam yang membuat konsumen menyukai produk dan layanan Telkom. Semakin tinggi nilai NPS menunjukkan tingginya indeks kepuasan konsumen. Hal paling mendasar untuk meningkatkan nilai NPS adalah memberikan customer experience yang terbaik kepada pelanggan salah satunya adalah memberikan service assurance yang excellence. Adanya Operational Support System yang saat ini sudah berjalan di operasional secara Nasional, masih perlu untuk terus dikembangkan dan disempurnakan kapabilitasnya agar mampu memberikan service assurance yang excellent untuk dapat meningkatkan customer experience, menurunkan angka gangguan (Q) sehingga dapat tercapai goal yaitu peningkatan nilai NPS. 1.2.2 Expected Outcome Melakukan permodelan machine learning untuk identifikasi dini pelanggan yang berpotensi akan mengalami gangguan internet, supaya dapat ditangani terlebih dahulu sehingga mengurangi angka komplain pelanggan dan dapat kepuasan pelanggan. 1.2.3 Recommendation 1.3 Customer Churn Prediction 1.3.1 Background Customer Churn didefinisikan sebagai kecenderungan pelanggan untuk berhenti melakukan interaksi dengan sebuah perusahaan. Perusahaan telekomunikasi memiliki kebutuhan untuk mengetahui customer yang akan berhenti berlangganan atau tidak, karena biaya mempertahankan pelanggan yang sudah ada jauh lebih sedikit dibandingkan memperoleh pelanggan baru. Perusahaan biasanya mendefinisikan 2 tipe customer churn, yaitu voluntary churn dan involuntary churn. Voluntary churn merupakan pelanggan yang sengaja berhenti dan beralih ke perusahaan lain, sedangkan involuntary churn merupakan pelanggan yang berhenti karena perpindahan lokasi, kematian, atau alasan lain yang sulit dikontrol. Analisis voluntary churn tentunya tidak sulit untuk mempelajari karakteristik pelanggan yang dapat dilihat dari data profil pelanggan. Permasalah diatas dapat dijawab dengan membuat model prediksi customer churn. Harapannya dengan adanya model prediksi customer churn, dapat mempermudah pihak perusahaan telekomunikasi untuk memperoleh informasi mengenai pelanggan yang berpeluang besar untuk churn. 1.3.2 Modelling Analysis 1.3.2.1 Import Data Data yang digunakan merupakan data profil pelanggan perusahaan telekomunikasi yang diperoleh dar link berikut. Data tersebut berisikan 7043 observasi dengan 21 kolom. Target variabel pada data ini adalah Churn, kita akan memprediksi apakah pelanggan akan berhenti berlangganan produk atau akan tetep berlangganan. customer &lt;- read.csv(&quot;assets/01-telco/WA_Fn-UseC_-Telco-Customer-Churn.csv&quot;) head(customer) #&gt; customerID gender SeniorCitizen Partner Dependents tenure PhoneService #&gt; 1 7590-VHVEG Female 0 Yes No 1 No #&gt; 2 5575-GNVDE Male 0 No No 34 Yes #&gt; 3 3668-QPYBK Male 0 No No 2 Yes #&gt; 4 7795-CFOCW Male 0 No No 45 No #&gt; 5 9237-HQITU Female 0 No No 2 Yes #&gt; 6 9305-CDSKC Female 0 No No 8 Yes #&gt; MultipleLines InternetService OnlineSecurity OnlineBackup DeviceProtection #&gt; 1 No phone service DSL No Yes No #&gt; 2 No DSL Yes No Yes #&gt; 3 No DSL Yes Yes No #&gt; 4 No phone service DSL Yes No Yes #&gt; 5 No Fiber optic No No No #&gt; 6 Yes Fiber optic No No Yes #&gt; TechSupport StreamingTV StreamingMovies Contract PaperlessBilling #&gt; 1 No No No Month-to-month Yes #&gt; 2 No No No One year No #&gt; 3 No No No Month-to-month Yes #&gt; 4 Yes No No One year No #&gt; 5 No No No Month-to-month Yes #&gt; 6 No Yes Yes Month-to-month Yes #&gt; PaymentMethod MonthlyCharges TotalCharges Churn #&gt; 1 Electronic check 29.85 29.85 No #&gt; 2 Mailed check 56.95 1889.50 No #&gt; 3 Mailed check 53.85 108.15 Yes #&gt; 4 Bank transfer (automatic) 42.30 1840.75 No #&gt; 5 Electronic check 70.70 151.65 Yes #&gt; 6 Electronic check 99.65 820.50 Yes Berikut ini merupakan deskripsi untuk setiap variabel: CustomerID: Customer ID Gender: Gender pelanggan yaitu Female dan Male SeniorCitizen: Apakah pelanggan merupakan senio citizen (0: No, 1: Yes) Partner: Apakah pelanggan memiliki partner atau tidak (Yes, No) Dependents: Apakah pelanggan memiliki tanggungan atau tidak (Yes, No) Tenure: Jumlah bulan dalam menggunakan produk perusahaan MultipleLines: Apakah pelanggan memiliki banyak saluran atau tidak (Yes, No, No phone service) OnlineSecurity: Apakah pelanggan memiliki keamanan online atau tidak OnlineBackup: Apakah pelanggan memiliki cadangan online atau tidak DeviceProtection: Apakah pelanggan memiliki perlindungan perangkat atau tidak TechSupport: Apakah pelanggan memiliki dukungan teknis atau tidak StreamingTV: Apakah pelanggan berlangganan TV streaming atau tidak StreamingMovies: Apakah pelanggan berlangganan movies streaming atau tidak Contract: Ketentuan kontrak berlangganan (Month-to-month, One year, Two year) PaperlessBilling: Apakah pelanggan memiliki tagihan tanpa kertas atau tidak (Yes, No) PaymentMethod: Metode pembayaran (Electronic check, Mailed check, Bank transfer (automatic), Credit card (automatic)) MonthlyCharges: Jumlah pembayaran yang dilakukan setiap bulan TotalCharges: Jumlah total yang dibebankan oleh pelanggan Churn: Apakah pelanggan Churn atau tidak (Yes or No) 1.3.2.2 Exploratory Data Sebelum eksplorasi lebih lanjut, perlu diketahui kelengkapan data yang dimiliki: colSums(is.na(customer)) #&gt; customerID gender SeniorCitizen Partner #&gt; 0 0 0 0 #&gt; Dependents tenure PhoneService MultipleLines #&gt; 0 0 0 0 #&gt; InternetService OnlineSecurity OnlineBackup DeviceProtection #&gt; 0 0 0 0 #&gt; TechSupport StreamingTV StreamingMovies Contract #&gt; 0 0 0 0 #&gt; PaperlessBilling PaymentMethod MonthlyCharges TotalCharges #&gt; 0 0 0 11 #&gt; Churn #&gt; 0 Dari 7043 observasi ternyata terdapat missing values sebanyak 11 observasi pada kolom TotalCharges. Karena jumlah missing values cukup sedikit kita dapat membuat observasi tersebut. Selain itu, perlu kita buang variabel yang tidak dibutuhkan pada pemodelan yaitu customerID dan juga sesuaikan tipe data yang seharusnya. customer &lt;- customer %&gt;% select(-customerID) %&gt;% na.omit() %&gt;% mutate(SeniorCitizen = as.factor(SeniorCitizen)) Untuk mengetahui proporsi kelas pada setiap variable kategori, kita dapat menggunakan function inspect_cat dari package inspectdf seperti berikut: customer %&gt;% inspect_cat() %&gt;% show_plot() Dari hasil plot diatas dapat diketahui proporsi kelas untuk target variabel cenderung lebih banyak dikategori No namun masih seimbang. Sedangkan untuk variabel lainnya untuk proporsi setiap level nya mayoritas seimbang. Berikutnya kita dapat eksplorasi persebaran untuk variabel data numerik dengan function inspect_num dari package inspectdf seperti berikut: customer %&gt;% inspect_num() %&gt;% show_plot() Dari ketiga variabel numerik yang dimiliki, persebaran data cukup beragam untuk setiap nilai. 1.3.2.3 Modelling Sebelum masuk ke tahap modelling, kita perlu membagi data menjadi data_train dan data_test dengan proporsi 80:20. set.seed(100) idx &lt;- initial_split(data = customer,prop = 0.8,strata = Churn) data_train &lt;- training(idx) data_test &lt;- testing(idx) Berikutnya bentuk model random forest menggunakan package caret, tentukan banyaknya cross validation dan repetition pada model dan juga target variabel dan prediktor yang digunakan. set.seed(100) ctrl &lt;- trainControl(method=&quot;repeatedcv&quot;, number=5, repeats=3) # model_forest &lt;- train(Churn ~ ., data=data_train, method=&quot;rf&quot;, trControl = ctrl) import model yang sudah dijalankan pada chunk sebelumnya menggunakan readRDS. #saveRDS(model_forest,&quot;assets/01-telco/model_forest.rds&quot;) model_forest &lt;- readRDS(&quot;assets/01-telco/model_forest.rds&quot;) model_forest #&gt; Random Forest #&gt; #&gt; 5627 samples #&gt; 19 predictor #&gt; 2 classes: &#39;No&#39;, &#39;Yes&#39; #&gt; #&gt; No pre-processing #&gt; Resampling: Cross-Validated (5 fold, repeated 3 times) #&gt; Summary of sample sizes: 4501, 4502, 4501, 4502, 4502, 4501, ... #&gt; Resampling results across tuning parameters: #&gt; #&gt; mtry Accuracy Kappa #&gt; 2 0.7837817 0.3252122 #&gt; 16 0.7750746 0.3779712 #&gt; 30 0.7731203 0.3727503 #&gt; #&gt; Accuracy was used to select the optimal model using the largest value. #&gt; The final value used for the model was mtry = 2. Dari hasil yang diperoleh pada model_forest, didapatkan accuraci sebesar 0.78 dengan mtry sebanyak 2. Selanjutnya, akan dilakukan tuning model dengan melakukan upsample data. Artinya, kita akan membuat proporsi dari target variabel sama besar. up_train &lt;- upSample(x = data_train[,-20], y = data_train$Churn, yname = &quot;Churn&quot;) Dilakukan pembuat model random forest dengan data upsample: set.seed(100) # ctrl &lt;- trainControl(method=&quot;repeatedcv&quot;, number=5, repeats=3) # forest_upc &lt;- train(Churn ~ ., data=up_train, method=&quot;rf&quot;, trControl = ctrl) #saveRDS(forest_upc,&quot;assets/01-telco/model_caret.rds&quot;) forest_upc &lt;- readRDS(&quot;assets/01-telco/model_caret.rds&quot;) Dari hasil model kedua diperoleh hasil sebagai berikut: forest_upc #&gt; Random Forest #&gt; #&gt; 8262 samples #&gt; 19 predictor #&gt; 2 classes: &#39;No&#39;, &#39;Yes&#39; #&gt; #&gt; No pre-processing #&gt; Resampling: Cross-Validated (5 fold, repeated 3 times) #&gt; Summary of sample sizes: 6609, 6610, 6609, 6610, 6610, 6610, ... #&gt; Resampling results across tuning parameters: #&gt; #&gt; mtry Accuracy Kappa #&gt; 2 0.7760017 0.5520022 #&gt; 16 0.8911472 0.7822945 #&gt; 30 0.8875167 0.7750336 #&gt; #&gt; Accuracy was used to select the optimal model using the largest value. #&gt; The final value used for the model was mtry = 16. Setelah dilakukan upsample data, terlihat nilai accuracy yang diperoleh lebih besar dibandingkan model sebelumnya sebesar 0.89 dengan mtry sebanyak 16. Selanjutnya, akan dilakukan prediksi terhadap data_test: pred &lt;- predict(forest_upc,newdata = data_test,type = &quot;prob&quot;) pred$result &lt;- as.factor(ifelse(pred$Yes &gt; 0.45, &quot;Yes&quot;,&quot;No&quot;)) confusionMatrix(pred$result, data_test$Churn,positive = &quot;Yes&quot;) #&gt; Confusion Matrix and Statistics #&gt; #&gt; Reference #&gt; Prediction No Yes #&gt; No 849 109 #&gt; Yes 183 264 #&gt; #&gt; Accuracy : 0.7922 #&gt; 95% CI : (0.77, 0.8131) #&gt; No Information Rate : 0.7345 #&gt; P-Value [Acc &gt; NIR] : 0.00000031 #&gt; #&gt; Kappa : 0.4989 #&gt; #&gt; Mcnemar&#39;s Test P-Value : 0.00001937 #&gt; #&gt; Sensitivity : 0.7078 #&gt; Specificity : 0.8227 #&gt; Pos Pred Value : 0.5906 #&gt; Neg Pred Value : 0.8862 #&gt; Prevalence : 0.2655 #&gt; Detection Rate : 0.1879 #&gt; Detection Prevalence : 0.3181 #&gt; Balanced Accuracy : 0.7652 #&gt; #&gt; &#39;Positive&#39; Class : Yes #&gt; Pada kasus ini kita ingin memperoleh nila sensitivity/recall yang lebih besar, dengan menggunakan threshold sebesar 0.4 diperoleh nilai recall sebesar 0.70 dengan accuracy sebesar 0.79 dan precision sebesar 0.59. Dari model yang telah terbentuk kita dapat memperoleh nilai AUC pada model: library(ROCR) pred_prob &lt;- predict(object = forest_upc,newdata = data_test,type = &quot;prob&quot;) pred &lt;- prediction(pred_prob[,2],labels = data_test$Churn) perf &lt;- performance(prediction.obj = pred,measure = &quot;tpr&quot;,x.measure = &quot;fpr&quot;) plot(perf) auc &lt;- performance(pred,measure = &quot;auc&quot;) auc@y.values[[1]] #&gt; [1] 0.8513259 1.3.3 Conclusion library(lime) test_x &lt;- data_test %&gt;% dplyr::select(-Churn) explainer &lt;- lime(test_x, forest_upc) explanation &lt;- lime::explain(test_x[1:2,], explainer, labels = c(&quot;Yes&quot;), n_features = 8) plot_features(explanation) Setelah adanya model prediksi customer churn, pihak perusahaan telekomunikasi dapat dengan mudah mengetahui pelanggan yang memiliki kecendurungan akan churn. Kedua plot diatas memperlihatkan prediksi dua customer, kedua customer memiliki peluang besar untuk churn dan kita dapat mengetahui variabel mana yang supports dan contradicts terhadap hasil prediksi. Bab 2 Finance body { text-align: justify} 2.1 Credit Risk Analysis 2.1.1 Background Credit scoring membutuhkan berbagai data profil calon peminjam sehingga tingkat resiko dapat dihitung dengan tepat. Semakin benar dan lengkap data yang disediakan, maka semakin akurat perhitungan yang dilakukan. Proses tersebut tentunya merupakan hal yang baik, namun di sisi calon peminjam proses yang harus dilalui dirasa sangat merepotkan dan membutuhkan waktu untuk menunggu. Dan seiring tingkat kompetisi yang samkin tinggi di industri finansial, customer memiliki banyak alternatif. Semakin cepat proses yang ditawarkan, semakin tinggi kesempatan untuk mendapatkan peminjam. Tantangan pun muncul, bagaimana mendapatkan pelanggan dengan proses yang efisien namun akurasi dari credit scoring tetap tinggi. Disinilah machine learning dapat membantu menganalisa data - data profil peminjam dan proses pembayaran sehingga dapat mengeluarkan rekomendasi profil pelanggan yang beresiko rendah. Harapannya setelah mempunyai model machine learning dengan perfomance model yang baik, pegawai bank dapat dengan mudah mengidentifikasi karakteristik customer yang memiliki peluang besar untuk melunasi pinjaman dengan lancar. Dengan adanya model machine learning ini tentunya akan mengurangi biaya dan waktu yang lebih cepat. 2.1.2 Modelling Analysis loan &lt;- read_csv(&quot;assets/02-finance/bankloans-copy.csv&quot;)%&gt;% mutate(default = factor(default, levels = c(0,1), labels = c(&quot;No&quot;,&quot;Yes&quot;))) %&gt;% rename(debtcred = creddebt) glimpse(loan) #&gt; Observations: 800 #&gt; Variables: 9 #&gt; $ age &lt;dbl&gt; 37, 25, 37, 29, 38, 32, 51, 27, 31, 37, 34, 31, 30, 31, 28, … #&gt; $ ed &lt;dbl&gt; 1, 4, 3, 1, 2, 1, 2, 3, 1, 1, 4, 1, 1, 1, 2, 1, 1, 1, 4, 2, … #&gt; $ employ &lt;dbl&gt; 20, 0, 16, 1, 13, 8, 22, 3, 1, 5, 7, 3, 11, 1, 1, 6, 14, 10,… #&gt; $ address &lt;dbl&gt; 2, 1, 14, 8, 0, 6, 23, 4, 1, 11, 15, 5, 1, 6, 4, 9, 8, 0, 2,… #&gt; $ income &lt;dbl&gt; 56, 18, 50, 31, 59, 26, 120, 35, 24, 27, 40, 16, 33, 21, 16,… #&gt; $ debtinc &lt;dbl&gt; 1.9, 33.4, 36.6, 8.0, 2.4, 4.1, 7.6, 13.3, 4.5, 9.4, 6.4, 32… #&gt; $ debtcred &lt;dbl&gt; 0.542640, 2.801592, 7.320000, 0.156240, 0.407808, 0.326196, … #&gt; $ othdebt &lt;dbl&gt; 0.521360, 3.210408, 10.980000, 2.323760, 1.008192, 0.739804,… #&gt; $ default &lt;fct&gt; No, Yes, Yes, No, No, No, No, No, No, NA, No, NA, No, Yes, N… head(loan) #&gt; # A tibble: 6 x 9 #&gt; age ed employ address income debtinc debtcred othdebt default #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; #&gt; 1 37 1 20 2 56 1.9 0.543 0.521 No #&gt; 2 25 4 0 1 18 33.4 2.80 3.21 Yes #&gt; 3 37 3 16 14 50 36.6 7.32 11.0 Yes #&gt; 4 29 1 1 8 31 8 0.156 2.32 No #&gt; 5 38 2 13 0 59 2.4 0.408 1.01 No #&gt; 6 32 1 8 6 26 4.1 0.326 0.740 No 2.1.2.1 Exploratory Data Analysis Eksplorasi hubungan target variabel dengan variabel prediktor loan %&gt;% na.omit() %&gt;% ggplot(aes(x = debtinc, fill = default)) + geom_density(alpha = 0.5, colour = FALSE) + scale_x_continuous( expand = expand_scale(mult = c(0, 0)) ) + scale_y_continuous(expand = expand_scale(mult = c(0, 0.1))) + scale_fill_manual(values = c(&quot;dodgerblue4&quot;,&quot;darkred&quot;)) + labs( title = &quot;Debt to Income Rario distribution&quot;, subtitle = &quot;estimated using kernel density function&quot;, x = &quot;Debt to Income Ratio&quot;, y = &quot;Income&quot;, fill = &quot;Default Status&quot; ) + theme_minimal() + theme( legend.position = &quot;top&quot;, legend.key.height = unit(12, &quot;pt&quot;), legend.key.width = unit(36, &quot;pt&quot;), ) loan %&gt;% na.omit() %&gt;% ggplot(aes(x = debtcred, y = income)) + geom_point(color = &quot;darkred&quot;) + geom_smooth(method = &quot;loess&quot;, se = FALSE, color = &quot;dodgerblue4&quot;) + facet_wrap(facets = vars(default), labeller = &quot;label_both&quot;) + scale_y_continuous(labels = dollar_format(scale = 1e-3, suffix = &quot;K&quot;)) + labs( title = &quot;The relation of credit to debt ratio and income&quot;, subtitle = &quot;for each default status&quot;, x = &quot;Debt to Credit Ratio&quot;, y = &quot;Income&quot; ) + theme_minimal() Check missing value loan %&gt;% is.na() %&gt;% colSums() %&gt;% enframe() %&gt;% arrange(desc(value)) #&gt; # A tibble: 9 x 2 #&gt; name value #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 default 150 #&gt; 2 age 0 #&gt; 3 ed 0 #&gt; 4 employ 0 #&gt; 5 address 0 #&gt; 6 income 0 #&gt; 7 debtinc 0 #&gt; 8 debtcred 0 #&gt; 9 othdebt 0 Berikutnya akan digunakan data observasi tanpa adanya data missing value loan &lt;- loan %&gt;% na.omit() 2.1.2.2 Modelling Split data train dan data test dengan proporsi 80:20. set.seed(100) intrain_loan &lt;- initial_split(data = loan, prop = 0.8, strata = &quot;default&quot;) train_loan &lt;- training(intrain_loan) test_loan &lt;- testing(intrain_loan) Cek proporsi dari target variabel prop.table(table(train_loan$default)) #&gt; #&gt; No Yes #&gt; 0.6149425 0.3850575 Untuk membuat data observasi menjadi lebih seimbang, dapat dilakukan upSample dari package caret. set.seed(47) train_loan_up &lt;- upSample(x = select(train_loan, -default), y = train_loan$default, yname = &quot;default&quot;) prop.table(table(train_loan_up$default)) #&gt; #&gt; No Yes #&gt; 0.5 0.5 Bentuk model random forest dengan 5 k-fold dan 3 repetition set.seed(47) ctrl &lt;- trainControl(method = &quot;repeatedcv&quot;, number = 5, repeats = 3, allowParallel=FALSE) model_forest &lt;- caret::train(default ~., data = train_loan_up, method = &quot;rf&quot;, trControl = ctrl) model_forest #&gt; Random Forest #&gt; #&gt; 642 samples #&gt; 8 predictor #&gt; 2 classes: &#39;No&#39;, &#39;Yes&#39; #&gt; #&gt; No pre-processing #&gt; Resampling: Cross-Validated (5 fold, repeated 3 times) #&gt; Summary of sample sizes: 514, 513, 513, 514, 514, 513, ... #&gt; Resampling results across tuning parameters: #&gt; #&gt; mtry Accuracy Kappa #&gt; 2 0.8483850 0.6967960 #&gt; 5 0.8359335 0.6718959 #&gt; 8 0.8317749 0.6635641 #&gt; #&gt; Accuracy was used to select the optimal model using the largest value. #&gt; The final value used for the model was mtry = 2. Setelah dilakukan 3 repetition pada model, repetition pertama memiliki accuracy paling besar dengan jumlah mtry sebanyak 2. Confusion matrix yang diperoleh data observasi yang tidak digunakan sebagai sampel diperoleh sebagai berikut: model_forest$finalModel #&gt; #&gt; Call: #&gt; randomForest(x = x, y = y, mtry = param$mtry) #&gt; Type of random forest: classification #&gt; Number of trees: 500 #&gt; No. of variables tried at each split: 2 #&gt; #&gt; OOB estimate of error rate: 12.15% #&gt; Confusion matrix: #&gt; No Yes class.error #&gt; No 269 52 0.16199377 #&gt; Yes 26 295 0.08099688 Selanjutnya akan dilakukan prediksi untuk data test dan mencari nilai confusion matrix pada hasil prediksi. test_loan &lt;- test_loan %&gt;% mutate(pred_forest = predict(model_forest, newdata = test_loan)) confmat_loan_tune &lt;- confusionMatrix(test_loan$pred_forest, test_loan$default, mode = &quot;prec_recall&quot;, positive = &quot;Yes&quot;) eval_rf &lt;- tidy(confmat_loan_tune) %&gt;% mutate(model = &quot;Random Forest&quot;) %&gt;% select(model, term, estimate) %&gt;% filter(term %in% c(&quot;accuracy&quot;, &quot;precision&quot;, &quot;recall&quot;, &quot;specificity&quot;)) eval_rf #&gt; # A tibble: 4 x 3 #&gt; model term estimate #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 Random Forest accuracy 0.836 #&gt; 2 Random Forest specificity 0.886 #&gt; 3 Random Forest precision 0.804 #&gt; 4 Random Forest recall 0.755 Dari hasil confusion matrix dapat diketahui, kemampuan model memprediksi target variabel dapat dikatakan cukup baik. Selanjutnya kita akan mengetahui variable importance pada model yang diperoleh. plot(varImp(model_forest),main = &quot;Variable Importance&quot;,) Dapat diketahui variabel debtinc, employ, debtcred, othdebt, dan income merupakan 5 variable yang paling berpengaruh dan paling sering digunakan dalam pembuatan pohon. 2.1.3 Recommendation eval_rf #&gt; # A tibble: 4 x 3 #&gt; model term estimate #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 Random Forest accuracy 0.836 #&gt; 2 Random Forest specificity 0.886 #&gt; 3 Random Forest precision 0.804 #&gt; 4 Random Forest recall 0.755 Model machine learning untuk memprediksi kredit pinjaman customer yang lancar dan tidak lancar memiliki perfomance model yang cukup baik. Nantinya, pegawai bank dapat menggunakan model tersebut dengan mengisikan data pribadi setiap customer, kemudian hasil yang diperoleh dapat di visualisasikan sebagai berikut: library(lime) train_x &lt;- train_loan %&gt;% select(-c(default)) test_x &lt;- test_loan %&gt;% select(-c(default, pred_forest)) explainer &lt;- lime(test_x, model_forest) explanation &lt;- lime::explain(test_x[2:3,], explainer, labels = c(&quot;Yes&quot;), n_features = 8) plot_features(explanation) Hasil visualisasi tersebut adalah contoh prediksi salah satu customer, customer tersebut terprediksi yes yang memiliki arti customer tersebut berpeluang besar sebagai customer yang lancar melunasi pembayaran. Tentunya ketika hasil prediksi menyatakan customer tersebut berpeluang besar untuk kredit lancar, artinya bank akan memberikan pinjaman kepada customer tersebut. Dari hasil visual tersebut juga ditunjukkan variabel mana yang support dan contradicts terhadap hasil prediksi yang dihasilkan. 2.2 Evaluating Customer Financial Complaints 2.2.1 Background Penanganan complain customer pada perusahaan saat ini menjadi salah satu kunci utama suatu perusahaan dapat terus tumbuh dan berkembang, karena apabila nasabah merasa tidak mendapatkan layanan yang baik saat menyampaikan keluhan maka nasabah akan mudah berpindah ke perusahaan lain yang dianggap bisa memberikan layanan terhadap komplain dengan baik. Nasabah yang merasa tidak mendapatkan layanan baik biasanya akan mengajukan keluhan ke Consumer Financial Protection Bureau (CFPB), CFPB merupakan instansi yang bertanggung jawab atas perlindungan konsumen di sektor keuangan. CFPB menyediakan data yang berisi keluhan dari customer financial, data keluhan tersebut dapat dianalisa untuk dijadikan pertimbangan pihak perusahaan untuk mengetahui indikator yang memerlukan perbaikan demi meningkatkan kualitas layanan. 2.2.2 Exploratory Data Analysis customer &lt;- read_csv(&quot;assets/02-finance/consumer_complaints.csv&quot;)%&gt;% mutate_if(is.character, as.factor) %&gt;% data.frame() Data diperoleh dari Consumer Financial Protection Bureau (CFPB) yang mengatur penawaran dan penyediaan produk atau layanan nasabah keuangan. CFPB menyediakan pertanyaan-pertanyaan umum dan dapat membantu nasabah terhubung dengan perusahaan keuangan yang terlibat. Data tersebut berisikan keluhan nasabah dari berbagai bank di Amerika Serikat. top_company &lt;- customer %&gt;% na.omit(Consumer.complaint.narrative) %&gt;% group_by(Company) %&gt;% summarise(total = n()) %&gt;% arrange(desc(total)) %&gt;% head(1) Dari 4504 perusahaan pada data, perusahaan yang paling banyak memiliki complain adalah Transunion Intermediate Holdings. Perlu diketahui bahwa banyaknya complain yang diperhitungkan tidak mempertimbangkan volume perusahaan. Misalnya, perusahaan dengan lebih banyak customer tentunya memiliki kemungkinan banyak complain dibandingkan perusahaan yang lebih sedikit pelanggannya dan juga pada analisa ini kita hanya memperhitungkan complain yang dilengkapi dengan narasi dari customer tersebut. Berikutnya kita akan fokus untuk menganalisa complai dari perusahaan Transunion Intermediate Holdings yang memiliki paling banyak narasi complain dari data. Setelah memperoleh data observasi, selanjutnya membersihkan data text: data_clean &lt;- data_complaint %&gt;% select(Consumer.complaint.narrative) %&gt;% mutate(Consumer.complaint.narrative = Consumer.complaint.narrative %&gt;% tolower() %&gt;% str_trim() %&gt;% str_remove_all(pattern = &quot;[[:punct:]]&quot;) %&gt;% str_remove_all(pattern = &quot;[0-9]&quot;) %&gt;% str_remove_all(pattern = &quot;xxxx&quot;) %&gt;% replace_contraction() %&gt;% replace_word_elongation() %&gt;% replace_white() %&gt;% str_squish()) head(data_clean) #&gt; Consumer.complaint.narrative #&gt; 1 this legal notice being sent and delivered to you persuant to florida statutes notice of order to cease and desist from using personal and private information fl statute violation title xlvi crimes chapter fraudulent practices view entire chapter criminal use of personal identification information as used in this section the term a access device means any card plate code account number electronic serial number mobile identification number personal identification number or other telecommunications service equipment or instrument identifier or other means of account access that can be used alone or in conjunction with another access device to obtain money goods services or any other thing of value or that can be used to initiate a transfer of funds other than a transfer originated solely by paper instrument b authorization means empowerment permission or competence to act c harass means to engage in conduct directed at a specific person that is intended to cause substantial emotional distress to such person and serves no legitimate purpose harass does not mean to use personal identification information for accepted commercial purposes the term does not include constitutionally protected conduct such as organized protests or the use of personal identification information for accepted commercial purposes d individual means a single human being and does not mean a firm association of individuals corporation partnership joint venture sole proprietorship or any other entity e person means a person as defined in s f personal identification information means any name or number that may be used alone or in conjunction with any other information to identify a specific individual including any name postal or electronic mail address telephone number social security number date of birth mother s maiden name official stateissued or united statesissued driver license or identification number alien registration number government passport number employer or taxpayer identification number or food assistance account number bank account number credit or debit card number or personal identification number or code assigned to the holder of a debit card by the issuer to permit authorized electronic use of such card unique biometric data such as fingerprint voice print retina or iris image or other unique physical representation unique electronic identification number address or routing code medical records telecommunication identifying information or access device or other number or information that can be used to access a person s financial resources g counterfeit or fictitious personal identification information means any counterfeit fictitious or fabricated information in the similitude of the data outlined in paragraph f that although not truthful or accurate would in context lead a reasonably prudent person to credit its truthfulness and accuracy a any person who willfully and without authorization fraudulently uses or possesses with intent to fraudulently use personal identification information concerning an individual without first obtaining that individual s consent commits the offense of fraudulent use of personal identification information which is a felony of the third degree punishable as provided in s s or s b any person who willfully and without authorization fraudulently uses personal identification information concerning an individual without first obtaining that individual s consent commits a felony of the second degree punishable as provided in s s or s if the pecuniary benefit the value of the services #&gt; 2 transunion continues to report inaccurate negative items on my report that i have previously dispu ted ti methey continuos to report account with no account number inaccurate information that is hurting my credit #&gt; 3 my credit report has an incorrect address the address on the report is nv other than when i was in the i have lived in my entire life associated with this lv address are bills from and and both amounts are less than $ every time i dispute these bills and ask for source documents to prove i owe the bills are sold to collection agencies when these receivables are sold as a package the collection agencies reopen the outstanding debt and it reappears on my report i have a substantial credit history with mortgage car loans and credit cards i have never been late on any amount i owe there should be some protection against collection companies windmilling these bogus receivables thank you for all you do #&gt; 4 i recently discovered that reported my account as a charge off on my credit report when i was denied a loan by due to the charge off i immediately contacted by phone in an attempt to rectify the error i was informed by the customer service representative that my last payment was received on and that my account was reported as a charge off on we both agreed that it did not make sense that my account would be reported as a charge off days after my payment was received at the time reported my account as a charge off it was paid in full with a i have spent hours on the phone with numerous customer service representatives the phone calls have been extremely frustrating and have not produced a positive outcome every representative has told me the same thing due to the age of this account they do not have account records and can not provide me with the information that i am requesting yet they refuse to remove the inaccurate information from my credit report i was told that a supervisors would call me back on separate occasions to this day i have not received a call back from a supervisor i have sent numerous certified letters disputing the charge off that is currently being reported on my credit report and negatively effecting my credit i have asked for validation of my account including details explaining my why my account is being reported as a charge off with a balance i have asked for them to send me a copy of my payment history along with account notes the letters that i received in response to my letters have been a one paragraph response stating that they only report accurate information to the credit bureaus #&gt; 5 sent letter to credit reporting agencies telling them the inquiries on my report are unjustified the actions fail to comply with fcra section the letters are attached below credit agencies replied stating they did not need to investigate verify or remove inquiries credit agencies failed to prove requirements and fa iled to remove inquiries from my credit reports #&gt; 6 student loans have been discharged and i have a letter from the stating all loans have been discharged the credit bureaus refuse to accept the letter Setelah membersihkan data text, selanjutnya kita akan melakukan proses tokenization yaitu memecah 1 kalimat menjadi beberapa term, pada proses berikut ini juga diperoleh frekuensi dari setiap term yang muncul. text.dat &lt;- data_clean %&gt;% rowid_to_column(&quot;id&quot;) %&gt;% unnest_tokens(word, Consumer.complaint.narrative) %&gt;% anti_join(stop_words) %&gt;% count(word, sort = T) %&gt;% rename(words = word, freq = n) %&gt;% filter(words != is.na(words), freq &gt; 50) head(text.dat) #&gt; # A tibble: 6 x 2 #&gt; words freq #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 credit 1572 #&gt; 2 report 774 #&gt; 3 account 613 #&gt; 4 transunion 529 #&gt; 5 information 503 #&gt; 6 reporting 358 Kata yang sudah diperoleh akan divisualisasikan dengan wordcloud. Semakin sering suatu kata digunakan, maka semakin besar pula ukuran kata tersebut ditampilkan dalam wordcloud. Artinya kita dapat mengetahui kata yang paling sering digunakan oleh customer Transunion Intermediate Holdings. Kata credit, report, dan account merupakan kata yang paling sering digunakan oleh customer saat complain. wordcloud2(data = text.dat, size = 1, color = &#39;random-dark&#39;, shuffle = 1) 2.2.3 Comparing Sentiment Dictionaries Semakin banyak informasi yang ditampilkan, dapat membantu pihak marketing mengembangkan strategi yang efektif dalam meningkatkan pelayanan, berikutnya tidak hanya kata yang sering muncul yang akan ditampilkan, namun juga informasi mengenai kata tersebut merupakan kata positif atau negatif yang digunakan oleh customer saat mengajukan complain. text_dat &lt;- data_clean %&gt;% rowid_to_column(&quot;id&quot;) %&gt;% unnest_tokens(word, Consumer.complaint.narrative) %&gt;% anti_join(stop_words) %&gt;% mutate(word = wordStem(word)) %&gt;% count(word, sort = T) %&gt;% filter(word != is.na(word)) head(text_dat,20) #&gt; # A tibble: 20 x 2 #&gt; word n #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 credit 1573 #&gt; 2 report 1462 #&gt; 3 account 869 #&gt; 4 inform 584 #&gt; 5 transunion 533 #&gt; 6 remov 423 #&gt; 7 inquiri 420 #&gt; 8 disput 415 #&gt; 9 file 345 #&gt; 10 request 329 #&gt; 11 letter 309 #&gt; 12 loan 287 #&gt; 13 payment 269 #&gt; 14 bureau 263 #&gt; 15 verifi 242 #&gt; 16 call 228 #&gt; 17 time 227 #&gt; 18 compani 223 #&gt; 19 receiv 218 #&gt; 20 agenc 213 bing_word &lt;- text_dat %&gt;% inner_join(get_sentiments(&quot;bing&quot;)) head(bing_word) #&gt; # A tibble: 6 x 3 #&gt; word n sentiment #&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; #&gt; 1 debt 206 negative #&gt; 2 correct 150 positive #&gt; 3 complaint 115 negative #&gt; 4 fraud 96 negative #&gt; 5 incorrect 89 negative #&gt; 6 hard 82 negative library(reshape2) library(wordcloud) bing_word %&gt;% acast(word ~ sentiment, value.var = &quot;n&quot;, fill = 0) %&gt;% comparison.cloud(colors = c(&quot;gray70&quot;,&quot;gray20&quot;), max.words = 200) Sentiment Analysis yang dilakukan sebelumnya kita memperhitungan kemunculan kata positif dan negatif. Salah satu kelemahan pada pendekatan tersebut terkadang dapat disalah artikan penggunaannya pada sebuah kata, misal correct dan support akan dianggap sebagai kata positif. Namun, arti kata tersebut akan berubah jika terdapat kata not didepannya. Pada analisis berikut ini kita akan menggunakan n-gram untuk melihat seberapa sering word1 diikuti oleh word2. Tokenisasi menggunakan n-gram berguna untuk eksplorasi kata yang memiliki hubungan. Ketika kita mengatur n = 2 artinya kita akan menampilkan dua kata berturut-turut atau sering disebut dengam bigrams. Hasil dari visualisasi berikut ini menampilkan kata-kata yang berhubungan dengan kata not. dat_bigrams &lt;- data_clean %&gt;% unnest_tokens(bigram, Consumer.complaint.narrative, token = &quot;ngrams&quot;, n= 2) %&gt;% separate(bigram, c(&quot;word1&quot;,&quot;word2&quot;), sep = &quot; &quot;) %&gt;% filter(word1 == &quot;not&quot;) %&gt;% inner_join(get_sentiments(&quot;afinn&quot;), by = c(word2 = &quot;word&quot;)) %&gt;% count(word1,word2, value, sort = T) %&gt;% mutate(contribution = n*value) %&gt;% arrange(desc(abs(contribution))) %&gt;% group_by(word1) %&gt;% slice(seq_len(20)) %&gt;% arrange(word1, desc(contribution)) %&gt;% ungroup() graph_bigram &lt;- dat_bigrams %&gt;% graph_from_data_frame() set.seed(123) a &lt;- grid::arrow(type = &quot;closed&quot;, length = unit(.15, &quot;inches&quot;)) ggraph(graph_bigram, layout = &quot;fr&quot;) + geom_edge_link(alpha = .25) + geom_edge_density(aes(fill = value)) + geom_node_point(color = &quot;lightblue&quot;, size = 5) + geom_node_text(aes(label = name), repel = TRUE) + theme_void() + theme(legend.position = &quot;none&quot;, plot.title = element_text(hjust = 0.5)) + ggtitle(&quot;Negation Bigram Network&quot;) 2.2.4 Correlation Pairs Analisis berikutnya, akan dilakukan eksplorasi untuk mengetahui kata-kata yang memiliki kecenderungan muncul bersama pada complain nasabah dengan mencari nilai korelasi antar kata. data_clean_cor &lt;- data_complaint %&gt;% select(Consumer.complaint.narrative,Issue,Product) %&gt;% mutate(Consumer.complaint.narrative = Consumer.complaint.narrative %&gt;% tolower() %&gt;% str_trim() %&gt;% str_remove_all(pattern = &quot;[[:punct:]]&quot;) %&gt;% str_remove_all(pattern = &quot;[0-9]&quot;) %&gt;% str_remove_all(pattern = &quot;xxxx&quot;) %&gt;% replace_contraction() %&gt;% replace_word_elongation() %&gt;% replace_white() %&gt;% str_squish()) head(data_clean_cor) #&gt; Consumer.complaint.narrative #&gt; 1 this legal notice being sent and delivered to you persuant to florida statutes notice of order to cease and desist from using personal and private information fl statute violation title xlvi crimes chapter fraudulent practices view entire chapter criminal use of personal identification information as used in this section the term a access device means any card plate code account number electronic serial number mobile identification number personal identification number or other telecommunications service equipment or instrument identifier or other means of account access that can be used alone or in conjunction with another access device to obtain money goods services or any other thing of value or that can be used to initiate a transfer of funds other than a transfer originated solely by paper instrument b authorization means empowerment permission or competence to act c harass means to engage in conduct directed at a specific person that is intended to cause substantial emotional distress to such person and serves no legitimate purpose harass does not mean to use personal identification information for accepted commercial purposes the term does not include constitutionally protected conduct such as organized protests or the use of personal identification information for accepted commercial purposes d individual means a single human being and does not mean a firm association of individuals corporation partnership joint venture sole proprietorship or any other entity e person means a person as defined in s f personal identification information means any name or number that may be used alone or in conjunction with any other information to identify a specific individual including any name postal or electronic mail address telephone number social security number date of birth mother s maiden name official stateissued or united statesissued driver license or identification number alien registration number government passport number employer or taxpayer identification number or food assistance account number bank account number credit or debit card number or personal identification number or code assigned to the holder of a debit card by the issuer to permit authorized electronic use of such card unique biometric data such as fingerprint voice print retina or iris image or other unique physical representation unique electronic identification number address or routing code medical records telecommunication identifying information or access device or other number or information that can be used to access a person s financial resources g counterfeit or fictitious personal identification information means any counterfeit fictitious or fabricated information in the similitude of the data outlined in paragraph f that although not truthful or accurate would in context lead a reasonably prudent person to credit its truthfulness and accuracy a any person who willfully and without authorization fraudulently uses or possesses with intent to fraudulently use personal identification information concerning an individual without first obtaining that individual s consent commits the offense of fraudulent use of personal identification information which is a felony of the third degree punishable as provided in s s or s b any person who willfully and without authorization fraudulently uses personal identification information concerning an individual without first obtaining that individual s consent commits a felony of the second degree punishable as provided in s s or s if the pecuniary benefit the value of the services #&gt; 2 transunion continues to report inaccurate negative items on my report that i have previously dispu ted ti methey continuos to report account with no account number inaccurate information that is hurting my credit #&gt; 3 my credit report has an incorrect address the address on the report is nv other than when i was in the i have lived in my entire life associated with this lv address are bills from and and both amounts are less than $ every time i dispute these bills and ask for source documents to prove i owe the bills are sold to collection agencies when these receivables are sold as a package the collection agencies reopen the outstanding debt and it reappears on my report i have a substantial credit history with mortgage car loans and credit cards i have never been late on any amount i owe there should be some protection against collection companies windmilling these bogus receivables thank you for all you do #&gt; 4 i recently discovered that reported my account as a charge off on my credit report when i was denied a loan by due to the charge off i immediately contacted by phone in an attempt to rectify the error i was informed by the customer service representative that my last payment was received on and that my account was reported as a charge off on we both agreed that it did not make sense that my account would be reported as a charge off days after my payment was received at the time reported my account as a charge off it was paid in full with a i have spent hours on the phone with numerous customer service representatives the phone calls have been extremely frustrating and have not produced a positive outcome every representative has told me the same thing due to the age of this account they do not have account records and can not provide me with the information that i am requesting yet they refuse to remove the inaccurate information from my credit report i was told that a supervisors would call me back on separate occasions to this day i have not received a call back from a supervisor i have sent numerous certified letters disputing the charge off that is currently being reported on my credit report and negatively effecting my credit i have asked for validation of my account including details explaining my why my account is being reported as a charge off with a balance i have asked for them to send me a copy of my payment history along with account notes the letters that i received in response to my letters have been a one paragraph response stating that they only report accurate information to the credit bureaus #&gt; 5 sent letter to credit reporting agencies telling them the inquiries on my report are unjustified the actions fail to comply with fcra section the letters are attached below credit agencies replied stating they did not need to investigate verify or remove inquiries credit agencies failed to prove requirements and fa iled to remove inquiries from my credit reports #&gt; 6 student loans have been discharged and i have a letter from the stating all loans have been discharged the credit bureaus refuse to accept the letter #&gt; Issue #&gt; 1 Disclosure verification of debt #&gt; 2 Problem with a credit reporting company&#39;s investigation into an existing problem #&gt; 3 Incorrect information on your report #&gt; 4 Problem with a credit reporting company&#39;s investigation into an existing problem #&gt; 5 Problem with a credit reporting company&#39;s investigation into an existing problem #&gt; 6 Incorrect information on your report #&gt; Product #&gt; 1 Debt collection #&gt; 2 Credit reporting, credit repair services, or other personal consumer reports #&gt; 3 Credit reporting, credit repair services, or other personal consumer reports #&gt; 4 Credit reporting, credit repair services, or other personal consumer reports #&gt; 5 Credit reporting, credit repair services, or other personal consumer reports #&gt; 6 Credit reporting, credit repair services, or other personal consumer reports text_dat_cor &lt;- data_clean_cor %&gt;% rowid_to_column(&quot;id&quot;) %&gt;% unnest_tokens(word,Consumer.complaint.narrative) %&gt;% anti_join(stop_words) Untuk memperoleh korelasi antar kata dapat menggunakan function pairwise_cor() dari package widyr words_cors &lt;- text_dat_cor %&gt;% group_by(word) %&gt;% filter(n() &gt; 100) %&gt;% pairwise_cor(word, Issue, sort = T) Korelasi antar kata dapat kita tampilkan secar visual menggunakan package ggraph. Pada visualisasi berikut kita hanya ingin menampilkan kata yang memiliki korelasi lebih dari 0.9. Artinya korelasi pada visualisasi berikut memiliki kecenderungan muncul bersamaan saat nasabah mengajukan complain. set.seed(100) words_cors %&gt;% filter(correlation &gt; .9) %&gt;% graph_from_data_frame() %&gt;% ggraph(layout = &quot;fr&quot;) + geom_edge_link(aes(edge_alpha = correlation)) + geom_node_point(color = &quot;lightblue&quot;, size = 5) + geom_node_text(aes(label = name), repel = TRUE) + theme_void() + ggtitle(&quot;Correlation between Words&quot;)+ theme(legend.position = &quot;none&quot;, plot.title = element_text(hjust = 0.5,face = &quot;bold&quot;)) Manfaat dari Sentiment Analysis yang telah dilakukan adalah kita dapat mengetahui pesan utama dari pendapat dan pemikiran customer terhadap suatu company atau product. Selain itu, output dari sentiment analysis dapat memberikan gambaran mengenai pelayanan atau product yang belum sesuai. Hal tersebut dapat membantu tim marketing untuk meneliti trend yang dibutuhkan customer dengan lebih baik. Seiring dengan peningkatan kualitas layanan dan pengembangan produk yang lebih baik, tentunya akan mengurangi tingkat churn customer. Bab 3 Retail body { text-align: justify} 3.1 E-Commerce Clothing Reviews 3.1.1 Background Perkembangan teknologi membuat pergeseran perilaku customer dari pembelian offline menjadi pembelian online atau melalui e-commerce. Perbedaan utama saat berbelanja secara online atau offline adalah saat akan berbelanja secara online, calon customer tidak dapat memeriksa barang yang akan dibeli secara langsung dan biasanya dibantuk oleh gambar atau deskripsi yang diberikan oleh penjual. Tentunya customer akan mencari informasi mengenai produk yang akan dibeli untuk meminimalisir dampak negatif yang didapat. Untuk membantu customer dalam menentukan product yang akan dibeli, mayoritas e-commerce sekarang ini menyediakan fitur online customer review, dimana online customer review ini dijadikan sebagai salah satu media customer mendapatkan informasi tentang produk dari customer yang telah membeli produk tersebut. Meningkatnya e-commerce di Indonesia, kebutuhan analisa mengenai online customer review dirasa perlu dilakukan untuk mendukung agar customer dapat memiliki pengalaman belanja online yang lebih baik daripada belanja offline. Salah satu implementasi data review customer tersebut dapat dimanfaatkan untuk membuat model yang dapat memprediksi apakah product tersebut direkomendasikan atau tidak direkomendasikan. Harapannya setelah perusahaan dapat menilai product mana yang direkomendasikan dan yang tidak direkomendasikan, dapat membantu perusahaan dalam pertimbangan penentuan top seller. Untuk seller yang memiliki banyak product yang direkomendasikan, dapat dijadikan sebagai top seller. reviews &lt;- read.csv(&quot;assets/03- retail/Womens Clothing E-Commerce Reviews.csv&quot;) head(reviews) #&gt; X Clothing.ID Age Title #&gt; 1 0 767 33 #&gt; 2 1 1080 34 #&gt; 3 2 1077 60 Some major design flaws #&gt; 4 3 1049 50 My favorite buy! #&gt; 5 4 847 47 Flattering shirt #&gt; 6 5 1080 49 Not for the very petite #&gt; Review.Text #&gt; 1 Absolutely wonderful - silky and sexy and comfortable #&gt; 2 Love this dress! it&#39;s sooo pretty. i happened to find it in a store, and i&#39;m glad i did bc i never would have ordered it online bc it&#39;s petite. i bought a petite and am 5&#39;8&quot;. i love the length on me- hits just a little below the knee. would definitely be a true midi on someone who is truly petite. #&gt; 3 I had such high hopes for this dress and really wanted it to work for me. i initially ordered the petite small (my usual size) but i found this to be outrageously small. so small in fact that i could not zip it up! i reordered it in petite medium, which was just ok. overall, the top half was comfortable and fit nicely, but the bottom half had a very tight under layer and several somewhat cheap (net) over layers. imo, a major design flaw was the net over layer sewn directly into the zipper - it c #&gt; 4 I love, love, love this jumpsuit. it&#39;s fun, flirty, and fabulous! every time i wear it, i get nothing but great compliments! #&gt; 5 This shirt is very flattering to all due to the adjustable front tie. it is the perfect length to wear with leggings and it is sleeveless so it pairs well with any cardigan. love this shirt!!! #&gt; 6 I love tracy reese dresses, but this one is not for the very petite. i am just under 5 feet tall and usually wear a 0p in this brand. this dress was very pretty out of the package but its a lot of dress. the skirt is long and very full so it overwhelmed my small frame. not a stranger to alterations, shortening and narrowing the skirt would take away from the embellishment of the garment. i love the color and the idea of the style but it just did not work on me. i returned this dress. #&gt; Rating Recommended.IND Positive.Feedback.Count Division.Name Department.Name #&gt; 1 4 1 0 Initmates Intimate #&gt; 2 5 1 4 General Dresses #&gt; 3 3 0 0 General Dresses #&gt; 4 5 1 0 General Petite Bottoms #&gt; 5 5 1 6 General Tops #&gt; 6 2 0 4 General Dresses #&gt; Class.Name #&gt; 1 Intimates #&gt; 2 Dresses #&gt; 3 Dresses #&gt; 4 Pants #&gt; 5 Blouses #&gt; 6 Dresses Data yang digunakan merupakan data women e-commerce clothing reviews. Terdapat dua variabel yang menjadi fokus analisis ini yaitu Review.Text dan Recommended.IND. Variabel Review.Text merupakan review yang diberikan oleh customer terhadap product dari berbagai e-commerce, sedangkan Recommended.IND merupakan penilaian rekomendasi dari customer, 1 artinya product tersebut recommended dan 0 artinya product tersebut not recommended. Sebelum masuk cleaning data, kita ingin mengetahui proporsi dari target variabel: prop.table(table(reviews$Recommended.IND)) #&gt; #&gt; 0 1 #&gt; 0.1776377 0.8223623 3.1.2 Cleaning Data Untuk mengolah data text, kita perlu mengubah data teks dari vector menjadi corpus dengan function Vcorpus(). reviews_corpus &lt;- VCorpus(VectorSource(reviews$Review.Text)) reviews_corpus #&gt; &lt;&lt;VCorpus&gt;&gt; #&gt; Metadata: corpus specific: 0, document level (indexed): 0 #&gt; Content: documents: 23486 Selanjutnya, kita melakukan text cleansing dengan beberapa langkah sebagai berikut: tolower digunakan untuk mengubah semua karakter menjadi lowercase. removePunctuation digunakan untuk menghilangkan semua tanda baca. removeNumbers digunakan untuk menghilangkan semua angka. stopwords digunakan untuk menghilangkan kata-kata umum (am,and,or,if). stripWhitespace digunakan untuk menghapus karakter spasi yang berlebihan. data_clean &lt;- reviews_corpus %&gt;% tm_map(content_transformer(tolower)) %&gt;% tm_map(removePunctuation) %&gt;% tm_map(removeNumbers) %&gt;% tm_map(removeWords, stopwords(&quot;en&quot;)) %&gt;% tm_map(content_transformer(stripWhitespace)) inspect(data_clean[[1]]) #&gt; &lt;&lt;PlainTextDocument&gt;&gt; #&gt; Metadata: 7 #&gt; Content: chars: 43 #&gt; #&gt; absolutely wonderful silky sexy comfortable Setelah melakukan text cleansing, text tersebut akan diubah menjadi Document Term Matrix(DTM) melalui proses tokenization. Tokenization berfungsi memecah 1 teks atau kalimat menjadi beberapa term. Terim bisa berupa 1 kata, 2 kata, dan seterusnya. Pada format DTM, 1 kata akan menjadi 1 feature, secara default nilainya adalah jumlah kata pada dokumen tersebut. dtm_text &lt;- DocumentTermMatrix(data_clean) Sebelum membentuk model, tentunya kita perlu split data menjadi data train dan data test dengan proporsi 80:20. set.seed(100) idx &lt;- sample(nrow(dtm_text), nrow(dtm_text)*0.8) train &lt;- dtm_text[idx,] test &lt;- dtm_text[-idx,] train_label &lt;- reviews[idx,&quot;Recommended.IND&quot;] test_label &lt;- reviews[-idx,&quot;Recommended.IND&quot;] Term yang digunakan pada model ini, kita hanya mengambil term yang muncul paling sedikit 100 kali dari seluruh observasi dengan findFreqTerms(). freq &lt;- findFreqTerms(dtm_text, 100) train_r &lt;- train[, freq] test_r &lt;- test[, freq] inspect(train_r) #&gt; &lt;&lt;DocumentTermMatrix (documents: 18788, terms: 870)&gt;&gt; #&gt; Non-/sparse entries: 389603/15955957 #&gt; Sparsity : 98% #&gt; Maximal term length: 13 #&gt; Weighting : term frequency (tf) #&gt; Sample : #&gt; Terms #&gt; Docs dress fabric fit great just like love size top wear #&gt; 12348 0 1 0 1 0 1 0 1 0 1 #&gt; 12812 0 1 0 1 0 0 1 0 0 0 #&gt; 15905 0 1 2 1 0 1 0 2 0 1 #&gt; 1775 3 0 0 0 0 0 1 3 3 1 #&gt; 18527 0 1 0 1 1 0 0 0 2 0 #&gt; 19547 4 0 1 0 0 0 0 0 0 0 #&gt; 21091 0 0 0 0 0 1 0 2 0 1 #&gt; 22039 1 0 1 0 0 2 0 1 2 1 #&gt; 4789 1 0 2 0 0 1 0 1 0 1 #&gt; 6317 0 0 0 1 1 2 0 0 2 0 Nilai dari setiap matrix masih berupa angka numerik, dengan range 0-inf. Naive bayes akan memiliki performa lebih bagus ketika variabel numerik diubah menjadi kategorik. Salah satu caranya dengan Bernoulli Converter, yaitu jika jumlah kata yang muncul lebih dari 1, maka kita akan anggap nilainya adalah 1, jika 0 artinya tidak ada kata tersebut. bernoulli_conv &lt;- function(x){ x &lt;- as.factor(ifelse(x &gt; 0, 1, 0)) return(x) } train.bern &lt;- apply(train_r, MARGIN = 2, FUN = bernoulli_conv) test.bern &lt;- apply(test_r, MARGIN = 2, FUN = bernoulli_conv) 3.1.3 Modelling Selanjutnya, pembentukan model menggunakan naive bayes dan diikuti dengan prediksi data test. model.nb &lt;- naiveBayes(x = train.bern, y = as.factor(train_label), laplace = 1) pred.nb &lt;- predict(object = model.nb, newdata= test.bern) Dai hasil prediksi data test, kita akan menampilkan Confusion Matrix untuk mengetahui performa model. confusionMatrix(data = as.factor(pred.nb), reference = as.factor(test_label), positive = &quot;1&quot;) #&gt; Confusion Matrix and Statistics #&gt; #&gt; Reference #&gt; Prediction 0 1 #&gt; 0 614 365 #&gt; 1 253 3466 #&gt; #&gt; Accuracy : 0.8685 #&gt; 95% CI : (0.8585, 0.878) #&gt; No Information Rate : 0.8155 #&gt; P-Value [Acc &gt; NIR] : &lt; 0.00000000000000022 #&gt; #&gt; Kappa : 0.5837 #&gt; #&gt; Mcnemar&#39;s Test P-Value : 0.000008004 #&gt; #&gt; Sensitivity : 0.9047 #&gt; Specificity : 0.7082 #&gt; Pos Pred Value : 0.9320 #&gt; Neg Pred Value : 0.6272 #&gt; Prevalence : 0.8155 #&gt; Detection Rate : 0.7378 #&gt; Detection Prevalence : 0.7916 #&gt; Balanced Accuracy : 0.8065 #&gt; #&gt; &#39;Positive&#39; Class : 1 #&gt; 3.1.4 Visualize Data Text Selanjutnya, kita akan coba lakukan prediksi terhadap data test dan juga menampilkan visualisasi text tersebut menggunakan package lime. set.seed(100) idx &lt;- sample(nrow(reviews), nrow(reviews)*0.8) train_lime &lt;- reviews[idx,] test_lime &lt;- reviews[-idx,] tokenize_text &lt;- function(text){ #create corpus data_corpus &lt;- VCorpus(VectorSource(text)) # cleansing data_clean &lt;- data_corpus %&gt;% tm_map(content_transformer(tolower)) %&gt;% tm_map(removePunctuation) %&gt;% tm_map(removeNumbers) %&gt;% tm_map(removeWords, stopwords(&quot;en&quot;)) %&gt;% tm_map(content_transformer(stripWhitespace)) #dtm dtm_text &lt;- DocumentTermMatrix(data_clean) #convert to bernoulli data_text &lt;- apply(dtm_text, MARGIN = 2, FUN = bernoulli_conv) return(data_text) } model_type.naiveBayes &lt;- function(x){ return(&quot;classification&quot;) } predict_model.naiveBayes &lt;- function(x, newdata, type = &quot;raw&quot;) { # return classification probabilities only res &lt;- predict(x, newdata, type = &quot;raw&quot;) %&gt;% as.data.frame() return(res) } text_train &lt;- train_lime$Review.Text %&gt;% as.character() explainer &lt;- lime(text_train, model = model.nb, preprocess = tokenize_text) text_test &lt;- test_lime$Review.Text %&gt;% as.character() set.seed(100) explanation &lt;- explain(text_test[5:10], explainer = explainer, n_labels =1, n_features = 50, single_explanation = F) plot_text_explanations(explanation) Dari hasil output observasi kedua terprediksi product tersebut recommended dengan probability 96.31% dan nilai explainer fit menunjukkan seberapa baik LIME dalam menginterpretasikan prediksi untuk observasi ini sebesar 0.89 artinya dapat dikatakan cukup akurat. Teks berlabel biru menunjukkan kata tersebut meningkatkan kemungkinan product tersebut untuk direkomendasikan, sedangkan teks berlabel merah berarti bahwa kata tersebut bertentangan/mengurangi kemungkinan product tersebut untuk direkomendasikan. Bab 4 Insurance body { text-align: justify} 4.1 Prediction of Total Claim Amount 4.1.1 Background Seiring tingkat kompetisi yang semakin tinggi di industri asuransi, perusahaan dituntut untuk selalu memberikan terobosan dan strategi untuk memberikan layanan yang terbaik untuk nasabahnya. Salah satu aset utama perusahaan asuransi tentunya adalah data nasabah dan riwayat polis. Tentunya dengan adanya data yang dimiliki oleh perusahaan, dapat dimanfaatkan dalam upaya pengambilan keputusan strategis. Perusahaan memiliki kebutuhan untuk memperhitungkan pembayaran klaim di masa depan. Tanggung jawab tersebut biasa dikenal sebagai cadangan klaim. Karena cadangkan klaim adalah kewajiban yang harus dipersiapkan untuk masa yang akan datang, nilai pastinya tidak diketahui dan harus diperkirakan. Risiko yang dimiliki oleh setiap nasabah tentunya bervariasi, faktor-faktor yang berhubungan dengan risiko tentunya membantu dalam memprediksi biaya klaim yang harus dibayarkan. Tujuan dari analysis ini adalah untuk memprediksi besarnya klaim yang harus diberikan oleh perusahaan untuk setiap nasabahnya, hasil prediksi diperoleh dengan mempelajari karakteristik dan profil dari nasabah tersebut. 4.1.2 Modelling Analysis 4.1.2.1 Import Data Data yang digunakan merupakan profil data nasabah asuransi kendaraan beserta total claim dari masing-masing nasabah yang diperoleh dari link berikut. Data tersebut berisikan 9134 observasi atau sebanyak jumlah nasabah yang dimiliki, beserta 26 kolom. Target variabel pada data ini adalah Total.Claim.Amount, kita akan memprediksi total claim amount untuk setiap nasabah, harapannya perusahaan asuransi dapat mengetahui dana yang harus disiapkan untuk membayar klaim. insurance &lt;- read.csv(&quot;assets/04-insurance/Auto_Insurance_Claims_Sample.csv&quot;) head(insurance) #&gt; Customer Country State.Code State Claim.Amount Response Coverage Education #&gt; 1 BU79786 US KS Kansas 276.3519 No Basic Bachelor #&gt; 2 QZ44356 US NE Nebraska 697.9536 No Extended Bachelor #&gt; 3 AI49188 US OK Oklahoma 1288.7432 No Premium Bachelor #&gt; 4 WW63253 US MO Missouri 764.5862 No Basic Bachelor #&gt; 5 HB64268 US KS Kansas 281.3693 No Basic Bachelor #&gt; 6 OC83172 US IA Iowa 825.6298 Yes Basic Bachelor #&gt; Effective.To.Date EmploymentStatus Gender Income Location.Code Marital.Status #&gt; 1 2/24/11 Employed F 56274 Suburban Married #&gt; 2 1/31/11 Unemployed F 0 Suburban Single #&gt; 3 2/19/11 Employed F 48767 Suburban Married #&gt; 4 1/20/11 Unemployed M 0 Suburban Married #&gt; 5 2/3/11 Employed M 43836 Rural Single #&gt; 6 1/25/11 Employed F 62902 Rural Married #&gt; Monthly.Premium.Auto Months.Since.Last.Claim Months.Since.Policy.Inception #&gt; 1 69 32 5 #&gt; 2 94 13 42 #&gt; 3 108 18 38 #&gt; 4 106 18 65 #&gt; 5 73 12 44 #&gt; 6 69 14 94 #&gt; Number.of.Open.Complaints Number.of.Policies Policy.Type Policy #&gt; 1 0 1 Corporate Auto Corporate L3 #&gt; 2 0 8 Personal Auto Personal L3 #&gt; 3 0 2 Personal Auto Personal L3 #&gt; 4 0 7 Corporate Auto Corporate L2 #&gt; 5 0 1 Personal Auto Personal L1 #&gt; 6 0 2 Personal Auto Personal L3 #&gt; Claim.Reason Sales.Channel Total.Claim.Amount Vehicle.Class Vehicle.Size #&gt; 1 Collision Agent 384.8111 Two-Door Car Medsize #&gt; 2 Scratch/Dent Agent 1131.4649 Four-Door Car Medsize #&gt; 3 Collision Agent 566.4722 Two-Door Car Medsize #&gt; 4 Collision Call Center 529.8813 SUV Medsize #&gt; 5 Collision Agent 138.1309 Four-Door Car Medsize #&gt; 6 Hail Web 159.3830 Two-Door Car Medsize 4.1.2.2 Exploratory Data Selanjutnya melihat structure data dari masing-masing variabel, jika terdapat variabel yang belum sesuai tipe datanya perlu dilakukan explicit coercion. str(insurance) #&gt; &#39;data.frame&#39;: 9134 obs. of 26 variables: #&gt; $ Customer : Factor w/ 9134 levels &quot;AA10041&quot;,&quot;AA11235&quot;,..: 601 5947 97 8017 2489 4948 8434 756 1352 548 ... #&gt; $ Country : Factor w/ 1 level &quot;US&quot;: 1 1 1 1 1 1 1 1 1 1 ... #&gt; $ State.Code : Factor w/ 5 levels &quot;IA&quot;,&quot;KS&quot;,&quot;MO&quot;,..: 2 4 5 3 2 1 1 4 1 1 ... #&gt; $ State : Factor w/ 5 levels &quot;Iowa&quot;,&quot;Kansas&quot;,..: 2 4 5 3 2 1 1 4 1 1 ... #&gt; $ Claim.Amount : num 276 698 1289 765 281 ... #&gt; $ Response : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 1 1 1 1 1 2 2 1 2 1 ... #&gt; $ Coverage : Factor w/ 3 levels &quot;Basic&quot;,&quot;Extended&quot;,..: 1 2 3 1 1 1 1 3 1 2 ... #&gt; $ Education : Factor w/ 5 levels &quot;Bachelor&quot;,&quot;College&quot;,..: 1 1 1 1 1 1 2 5 1 2 ... #&gt; $ Effective.To.Date : Factor w/ 59 levels &quot;1/1/11&quot;,&quot;1/10/11&quot;,..: 48 25 42 13 53 18 48 10 19 40 ... #&gt; $ EmploymentStatus : Factor w/ 5 levels &quot;Disabled&quot;,&quot;Employed&quot;,..: 2 5 2 5 2 2 2 5 3 2 ... #&gt; $ Gender : Factor w/ 2 levels &quot;F&quot;,&quot;M&quot;: 1 1 1 2 2 1 1 2 2 1 ... #&gt; $ Income : int 56274 0 48767 0 43836 62902 55350 0 14072 28812 ... #&gt; $ Location.Code : Factor w/ 3 levels &quot;Rural&quot;,&quot;Suburban&quot;,..: 2 2 2 2 1 1 2 3 2 3 ... #&gt; $ Marital.Status : Factor w/ 3 levels &quot;Divorced&quot;,&quot;Married&quot;,..: 2 3 2 2 3 2 2 3 1 2 ... #&gt; $ Monthly.Premium.Auto : int 69 94 108 106 73 69 67 101 71 93 ... #&gt; $ Months.Since.Last.Claim : int 32 13 18 18 12 14 0 0 13 17 ... #&gt; $ Months.Since.Policy.Inception: int 5 42 38 65 44 94 13 68 3 7 ... #&gt; $ Number.of.Open.Complaints : int 0 0 0 0 0 0 0 0 0 0 ... #&gt; $ Number.of.Policies : int 1 8 2 7 1 2 9 4 2 8 ... #&gt; $ Policy.Type : Factor w/ 3 levels &quot;Corporate Auto&quot;,..: 1 2 2 1 2 2 1 1 1 3 ... #&gt; $ Policy : Factor w/ 9 levels &quot;Corporate L1&quot;,..: 3 6 6 2 4 6 3 3 3 8 ... #&gt; $ Claim.Reason : Factor w/ 4 levels &quot;Collision&quot;,&quot;Hail&quot;,..: 1 4 1 1 1 2 1 1 1 2 ... #&gt; $ Sales.Channel : Factor w/ 4 levels &quot;Agent&quot;,&quot;Branch&quot;,..: 1 1 1 3 1 4 1 1 1 2 ... #&gt; $ Total.Claim.Amount : num 385 1131 566 530 138 ... #&gt; $ Vehicle.Class : Factor w/ 6 levels &quot;Four-Door Car&quot;,..: 6 1 6 5 1 6 1 1 1 1 ... #&gt; $ Vehicle.Size : Factor w/ 3 levels &quot;Large&quot;,&quot;Medsize&quot;,..: 2 2 2 2 2 2 2 2 2 2 ... Berikutnya kita perlu inspect persebaran data yang dimilih baik data kategorik dan numerik, kita dapat menggunakan package inspectdf untuk eksplorasi berikut ini. insurance %&gt;% inspect_cat() %&gt;% show_plot() insurance %&gt;% inspect_num() %&gt;% show_plot() Dari hasil kedua plot diatas berikutnya membuang variabel yang tidak dibutuhkan dalam model. Variabel customer merupakan data unique dari ID setiap customer, oleh karena itu kita akan membuang variabel tersebut. Variabel country tidak banyak memberikan informasi, karena semua observasi berisikan informasi yang sama. Variabel State.Code juga memberikan informasi yang sama dengan variabel State, oleh karena itu kita akan menggunakan salah satu dari kedua variabel tersebut yaitu variabel State. Sedangkan untuk variabel Policy kita hilangkan karena informasi yang diberikan juga sama dengan variabel Policy.Type. insurance &lt;- insurance %&gt;% select(-c(Customer, Country, State.Code, Effective.To.Date, Policy)) Selanjutnya, split data menjadi data train dan data test dengan proporsi 80:20. set.seed(100) idx &lt;- initial_split(data = insurance,prop = 0.8) claim_train &lt;- training(idx) claim_test &lt;- testing(idx) 4.1.2.3 Modelling Kemudian bentuk model random forest, tentukan target variabel dan prediktor yang digunakan. library(randomForest) forest_claim &lt;- randomForest(Total.Claim.Amount~.,data = claim_train, localImp = TRUE) #saveRDS(forest_claim,&quot;forest_claim.RDS&quot;) forest_claim &lt;- readRDS(&quot;assets/04-insurance/forest_claim.RDS&quot;) forest_claim #&gt; #&gt; Call: #&gt; randomForest(formula = Total.Claim.Amount ~ ., data = claim_train, localImp = TRUE) #&gt; Type of random forest: regression #&gt; Number of trees: 500 #&gt; No. of variables tried at each split: 6 #&gt; #&gt; Mean of squared residuals: 12764.93 #&gt; % Var explained: 84.8 Model memiliki kemampuan menjelaskan variasi data sebesar 84.8%, sedangkan sisanya sebesar 15.2% dijelaskan oleh variabel lain yang tidak digunakan pada model. Untuk mengetahui variabel yang paling berpengaruh pada model, kita dapat melihat variabel importance. varImpPlot(forest_claim, main = &quot;Variable Importance&quot;,n.var = 5) Nilai importance atau tingkat kepentingannya terdapat dua penilaian yaitu IncMSE dan IncNodePurity. Untuk IncMSE diperoleh dari error pada OOB (out of bag) data, kemudian di rata-ratakan untuk semua pohon, dan dinormalisasi dengan standar deviasi. Untuk IncNodePurity merupakan total penurunan impurity dari masing-masing variabel. Untuk kasus klasifikasi node impurity diperoleh dari nilai gini index, sedangkan untuk kasus regresi diperoleh dari SSE (Sum Square Error). Untuk mengetahui peran variabel dalam pembuatan model, kita dapat memanfaatkan package randomForestExplainer yang menyediakan beberapa function untuk memperoleh informasi mengenai variabel importance. mindepth_frame &lt;- min_depth_distribution(forest_claim) #saveRDS(mindepth_frame, &quot;mindepthframe.rds&quot;) mindepth_frame &lt;- readRDS(&quot;assets/04-insurance/mindepthframe.rds&quot;) plot_min_depth_distribution(mindepth_frame, mean_sample = &quot;top_trees&quot;) Plot tersebut memberikan informasi mengenai nilai mean minimal dept untuk setiap variabel. Semakin kecil nilai minimal depth artinya semakin penting variabel tersebut pada model. Semakin besar proporsi minimal dept pada warna merah mudah (mendekati 0), artinya variabel tersebut sering dijadikan sebagai root node, yaitu variabel utama yang digunakan untuk menentukan nilai target. imp_frame &lt;- measure_importance(forest_claim) #saveRDS(imp_frame,&quot;imp_frame.rds&quot;) imp_frame &lt;- readRDS(&quot;assets/04-insurance/imp_frame.rds&quot;) plot_multi_way_importance(imp_frame, size_measure = &quot;no_of_nodes&quot;,no_of_labels = 6) plot_multi_way_importance(imp_frame, x_measure = &quot;mse_increase&quot;, size_measure = &quot;p_value&quot;, no_of_labels = 6) Perbandingan dari ketiga plot, terdapat 5 variabel yaitu location code, monthly premium auto, vehicle class, income, dan claim amount yang selalu muncul dari ketiga plot tersebut. Artinya kelima variabel tersebut dapat dikatakan variabel yang paling berpengaruh dan banyak digunakan dalam pembuatan pohon. Berikutnya lakukan prediksi untuk data test, kemudian cari nilai error dari hasil prediksi claim_test$pred &lt;- predict(object = forest_claim,newdata = claim_test) Mencari nilai RMSE (Root Mean Squared Error) RMSE(y_pred = claim_test$pred,y_true = claim_test$Total.Claim.Amount) #&gt; [1] 62.59351 RMSE merupakan nilai rata rata dari jumlah kuadrat error yang menyatakan ukuran besarnya kesalahan yang dihasilkan oleh model. Nilai RMSE rendah menunjukkan bahwa variasi nilai yang dihasilkan oleh model mendekasi variasi nilai observasinya. Jika dilihat dari 5 number summary variabel total claim amount, nilai RMSE yang diperoleh sebesar 119.9 dapat dikatakan sudah cukup baik. 4.1.3 Conclusion Untuk memprediksi nilai Total Claim Amount model ini memiliki kemampuan menjelaskan variasi data sebesar 84.8% dan variabel yang paling mempengaruhi target adalah variabel location code, monthly premium auto, vehicle class, income, dan claim amount. Hasil error yang diperoleh dari model tersebut cukup baik dalam memprediksi data. "]
]
