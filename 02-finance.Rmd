# Finance

```{r message=F, warning=F, echo=FALSE}
library(tidyverse)
library(rsample)
library(tidymodels)
library(caret)
library(lime)
library(xgboost)
library(ROCR)
library(inspectdf)
```
## Credit Risk Analysis 

### Background

Credit scoring merupakan sistem yang digunakan oleh bank atau lembaga keuangan lain untuk menentukan apakah seorang nasabah layak atau tidak mendapatkan pinjaman. Credit scoring membutuhkan berbagai data profil calon peminjam sehingga tingkat resiko dapat dihitung dengan tepat. Semakin tepat dan lengkap data yang disediakan, maka semakin akurat perhitungan yang dilakukan. 

Proses tersebut tentunya merupakan hal yang baik, namun di sisi calon peminjam proses yang harus dilalui dirasa sangat merepotkan dan membutuhkan waktu untuk menunggu dan seiring tingginya tingkat kompetisi yang ada di industri finansial, menjadikan nasabah memiliki banyak alternatif. Semakin cepat proses yang ditawarkan, semakin tinggi kesempatan untuk mendapatkan peminjam.

Tantangan pun muncul, bagaimana mendapatkan peminjam dengan proses yang efisien namun akurasi dari credit scoring tetap tinggi. Disinilah machine learning dapat membantu menganalisa data-data profil peminjam dan proses pembayaran sehingga dapat mengetahui profil peminjam yang memiliki peluang besar untuk melunasi pinjaman dengan lancar.

Harapannya setelah mempunyai model machine learning dengan perfomance model yang baik, pegawai bank dapat dengan mudah mengidentifikasi karakteristik customer yang memiliki peluang besar untuk melunasi pinjaman dengan lancar. Dengan adanya model machine learning ini tentunya akan mengurangi biaya dan waktu yang lebih cepat.

### Modelling Analysis

#### Cleaning data
```{r}
credit <- read_csv("assets/02-finance/credit_record.csv")
application <- read_csv("assets/02-finance/application_record.csv")
```

Data Description:

**Credit**

- ID : Client number	
- MONTHS_BALANCE : Record month	The month of the extracted data is the starting point, backwards, 0 is the current month, -1 is the previous month, and so on
- STATUS : Status	
    - 0: 1-29 days past due 
    - 1: 30-59 days past due 
    - 2: 60-89 days overdue 
    - 3: 90-119 days overdue 
    - 4: 120-149 days overdue 
    - 5: Overdue or bad debts, write-offs for more than 150 days 
    - C: paid off that month 
    - X: No loan for the month

**Application**

- ID	: Client number	
- CODE_GENDER : Gender	
- FLAG_OWN_CAR : Is there a car	
- FLAG_OWN_REALTY ; Is there a property	
- CNT_CHILDREN : Number of children	
- AMT_INCOME_TOTAL : Annual income	
- NAME_INCOME_TYPE	: Income category	
- NAME_EDUCATION_TYPE :	Education level	
- NAME_FAMILY_STATUS	: Marital status	
- NAME_HOUSING_TYPE	: Way of living	
- DAYS_BIRTH	: Birthday	Count backwards from current day (0), -1 means yesterday
- DAYS_EMPLOYED	: Start date of employment	Count backwards from current day(0). If positive, it means - - the person currently unemployed.
- FLAG_MOBIL	: Is there a mobile phone	
- FLAG_WORK_PHONE	: Is there a work phone	
- FLAG_PHONE	: Is there a phone	
- FLAG_EMAIL	: Is there an email	
- OCCUPATION_TYPE	: Occupation	
- CNT_FAM_MEMBERS	:Family size

**Check missing values**

Pada data credit tidak terdapat missing value
```{r}
colSums(is.na(credit))
```

```{r}
colSums(is.na(application))
```

Pada data application terdapat variabel `OCCUPATION_TYPE` yang memiliki banyak data missing, kita dapat membuang variabel tersebut. Serta kita akan membuang variabel `DAYS_BIRTH` dan `DAYS_EMPLOYED` yang tidak dibutuhkan pada model.
 
```{r}
application <- application %>% 
               select(-c(OCCUPATION_TYPE, DAYS_BIRTH, DAYS_EMPLOYED))
```

**Menyesuaikan tipe data**

Tahap berikutnya adalah menggabunkan data credit dan application serta menyesuaikan tipe data kategorik yang masih terbaca sebagai character.
```{r}
data_clean <- credit %>% 
              left_join(application) %>% 
              na.omit() %>% 
              select(-ID) %>% 
              filter(STATUS != "X") %>% 
              mutate(STATUS = as.factor(ifelse(STATUS == "C", "good credit", "bad credit"))) %>% 
              mutate_at(.vars = c("FLAG_MOBIL", "FLAG_WORK_PHONE",
                                  "FLAG_PHONE", "FLAG_EMAIL"), as.factor) %>% 
              mutate_if(is.character, as.factor) %>% 
              data.frame()
str(data_clean)
```
```{r, echo=FALSE}
data_clean <- data_clean %>% head(100000)
```


#### Exploratory Data Analysis (EDA)

Pada data EDA kita ingin mengetahui bagaimana sebaran data kategorik maupun numerik. 
```{r}
data_clean %>% inspect_cat() %>% show_plot()
```
Pada visualisasi berikut kita akan mendapatkan informasi apakah terdapat variabel yang tidak memiliki banyak informasi pada data, contohnya adalah variabel `FLAG_MOBIL` dimana keseluruhan data berisikan 1, artinya semua nasabah kita yang melakukan pinjaman memiliki mobil. Data yang tidak memiliki variansi seperti ini tidak diikutsertakan pada model.
```{r}
data_clean <- data_clean %>% 
              select(-c(FLAG_MOBIL,FLAG_EMAIL))
```

```{r}
data_clean %>% inspect_num() %>% show_plot()
```

#### Modelling Random Forest

Split data train dan data test dengan proporsi 80:20. Data train akan digunakan untuk modelling, sedangkan data test akan digunakan untuk evaluasi.
```{r}
set.seed(100)
index <- initial_split(data = data_clean, prop = 0.8, strata = "STATUS")
train <- training(index)
test <- testing(index)
```

Cek proporsi dari target variabel
```{r}
prop.table(table(train$STATUS))
```

Bentuk model random forest dengan 3 k-fold dan 2 repetition
```{r}
# set.seed(100)
# 
# ctrl <- trainControl(method = "repeatedcv",
#                      number = 3, 
#                      repeats = 2,
#                      allowParallel=FALSE)
# 
# model_forest <- caret::train(STATUS ~.,
#                              data = train, 
#                              method = "rf", 
#                              trControl = ctrl)

#saveRDS(model_forest, "model_forest.RDS")

model_forest <- readRDS("assets/02-finance/model_forest.RDS")
```

```{r}
model_forest
```

Setelah dilakukan 3 repetition pada model, repetition kedua memiliki accuracy paling tinggi dengan jumlah mtry sebanyak 14. 

Selanjutnya akan dilakukan prediksi untuk data test dan mencari nilai confusion matrix pada hasil prediksi.
```{r}
pred_rf<- predict(model_forest, newdata = test, type = "prob") %>% 
          mutate(result = as.factor(ifelse(`bad credit` > 0.45, "bad credit", "good credit")),
                 actual = ifelse(test$STATUS == 'good credit', 0, 1))
confmat_rf <- confusionMatrix(pred_rf$result, 
                                 test$STATUS,
                                 mode = "prec_recall",
                                 positive = "bad credit")

eval_rf <- tidy(confmat_rf) %>% 
  mutate(model = "Random Forest") %>% 
  select(model, term, estimate) %>% 
  filter(term %in% c("accuracy", "precision", "recall", "specificity"))

eval_rf
```


#### Modelling XGBoost

Tahap selanjutnya kita akan implementasikan data menggunakan model XGBoost, kita perlu menyiapkan data untuk model XGBoost terlebih dahulu

```{r}
data_xgb <- data_clean %>% 
            mutate(STATUS = ifelse(STATUS == "good credit", 0, 1)) %>% 
            data.frame()
```


```{r}
set.seed(100)
index <- initial_split(data = data_xgb, prop = 0.8, strata = "STATUS")
train_xgb <- training(index)
test_xgb <- testing(index)
```

```{r}
label_train <- as.numeric(train_xgb$STATUS)
label_test <- as.numeric(test_xgb$STATUS)
```

```{r}
train_matrix <- data.matrix(train_xgb[,-2])
test_matrix <- data.matrix(test_xgb[,-2])
# convert data to Dmatrix
dtrain <- xgb.DMatrix(data = train_matrix, label = label_train)
dtest <- xgb.DMatrix(data = test_matrix, label = label_test)
```

```{r}
params <- list(booster = "gbtree",
               objective = "binary:logistic",
               eta=0.7, 
               gamma=10, 
               max_depth=10, 
               min_child_weight=3, 
               subsample=1, 
               colsample_bytree=0.5)
```


```{r}
xgbcv <- xgb.cv( params = params, 
                 data = dtrain,
                 nrounds = 1000, 
                 showsd = T, 
                 nfold = 10,
                 stratified = T, 
                 print_every_n = 50, 
                 early_stopping_rounds = 20, 
                 maximize = F)
print(xgbcv)
```

```{r}
xgb1 <- xgb.train (params = params, 
                   data = dtrain, 
                   nrounds = xgbcv$best_iteration, 
                   watchlist = list(val=dtest,train=dtrain),
                   print_every_n = 100, 
                   early_stoping_rounds = 10, 
                   maximize = F , 
                   eval_metric = "error",
                   verbosity = 0)

xgbpred_prob <-predict(object = xgb1, newdata = dtest)
xgbpred <- ifelse (xgbpred_prob > 0.45,1,0)

```

```{r}
confmat_xgb <- confusionMatrix(as.factor(xgbpred), as.factor(label_test), positive = "1")
confmat_xgb
```
```{r}
confmat_rf <- confusionMatrix(pred_rf$result, 
                                 test$STATUS,
                                 mode = "prec_recall",
                                 positive = "bad credit")

eval_rf <- tidy(confmat_rf) %>% 
  mutate(model = "Random Forest") %>% 
  select(model, term, estimate) %>% 
  filter(term %in% c("accuracy", "precision", "recall", "specificity"))

confmat_xgb <- confusionMatrix(as.factor(xgbpred), as.factor(label_test), positive = "1")

eval_xgb <- tidy(confmat_xgb) %>% 
  mutate(model = "XGBoost") %>% 
  select(model, term, estimate) %>% 
  filter(term %in% c("accuracy", "precision", "recall", "specificity"))

```

Setelah diperoleh perfomance model XGBoost kita akan membandingkan dengan perfomance model random forest.
```{r}
eval_result <- rbind(eval_rf, eval_xgb)
eval_result
```
Metrics evaluasi yang kita utamakan adalah recall karena kita ingin meminimalisir mungkin keadaan dimana data actual nasabah tersebut *bad credit* namun terprediksi sebagai *good credit*. Dari hasil evaluasi dapat diketahui model XGBoost memiliki nilai recall lebih tinggi dibandingkan model random forest. 

```{r}
var_imp <- xgb.importance(model = xgb1,
                          feature_names = dimnames(dtrain)[[2]])
xgb.ggplot.importance(var_imp,top_n = 10) + 
  theme_minimal()+
  theme(legend.position = "none")
```
Grafik di atas menampilkan informasi mengenai 10 variabel yang paling berpengaruh pada model. Annual income dan months balance merupakan dua variabel terpenting pada model ini.

```{r}
xgb_result <- data.frame(class1 = xgbpred_prob, actual = as.factor(label_test))

auc_xgb <- roc_auc(data = xgb_result, truth = actual,class1) 
value_roc_xgb <- prediction(predictions = xgbpred_prob,
                        labels = label_test)

# ROC curve
plot(performance(value_roc_xgb, "tpr", "fpr"))

```

```{r}
value_auc_xgb <- performance(value_roc_xgb, measure = "auc")
value_auc_xgb@y.values
```
Nilai AUC yang diperoleh pada model model ini sebesar 0.83 artinya model dapat memprediksi dengan baik kedua target class yaitu `good credit` dan `bad credit`. Harapannya model ini dapat digunakan oleh pihak bank untuk menentukan credit scoring dengan mengisikan data profil nasabah, kemudian hasil yang diperoleh dapat di visualisasikan sebagai berikut:

```{r}
explainer <- lime(train_matrix %>% as.data.frame(), xgb1)
explanation <- explain(test_matrix[11:12,] %>% as.data.frame(),
                             explainer, 
                             labels = "1",
                             n_features = 3,
                             n_permutations = 5000,
                             dist_fun = "manhattan",
                             kernel_width = 0.75,
                             feature_select = "highest_weights")

plot_features(explanation)

```

Hasil dari visualisasi tersebut untuk nasabah 1 dan 2 memiliki probability 0.22 dan 0.17 artinya kedua nasabah tersebut akan dikategorikan sebagai `good credit`. Kedua nasabah tersebut memiliki karakteristik yang mirip karena hasil prediksi mereka didukung oleh kepemilikan model dan juga total income.


## Evaluating Customer Financial Complaints

### Background

Penanganan complain customer pada perusahaan saat ini menjadi salah satu kunci utama suatu perusahaan dapat terus tumbuh dan berkembang, karena apabila nasabah merasa tidak mendapatkan layanan yang baik saat menyampaikan keluhan maka nasabah akan mudah berpindah ke perusahaan lain yang dianggap bisa memberikan layanan terhadap komplain dengan baik. Nasabah yang merasa tidak mendapatkan layanan baik biasanya akan mengajukan keluhan ke Consumer Financial Protection Bureau (CFPB), CFPB merupakan instansi yang bertanggung jawab atas perlindungan konsumen di sektor keuangan. CFPB menyediakan data yang berisi keluhan dari customer financial, data keluhan tersebut dapat dianalisa untuk dijadikan pertimbangan pihak perusahaan untuk mengetahui indikator yang memerlukan perbaikan demi meningkatkan kualitas layanan.

### Exploratory Data Analysis

```{r, warning=FALSE, message=FALSE, echo=FALSE}
library(tidyverse)
library(textclean)
library(tidytext)
library(wordcloud2)
library(SnowballC)
library(sentimentr)
library(reshape2)
library(widyr)
library(igraph)
library(ggraph)
```

```{r, eval=FALSE}
customer <- read_csv("assets/02-finance/data_complaint.csv")%>% 
              mutate_if(is.character, as.factor) %>% 
              data.frame()
```


Data diperoleh dari [Consumer Financial Protection Bureau (CFPB)](https://www.consumerfinance.gov/)  yang mengatur penawaran dan penyediaan produk atau layanan nasabah keuangan. CFPB menyediakan pertanyaan-pertanyaan umum dan dapat membantu nasabah terhubung dengan perusahaan keuangan yang terlibat. Data tersebut berisikan keluhan nasabah dari berbagai bank di Amerika Serikat.

```{r, eval=FALSE}
top_company <- customer %>% 
  na.omit(Consumer.complaint.narrative) %>% 
  group_by(Company) %>% 
  summarise(total = n()) %>% 
  arrange(desc(total)) %>% 
  head(1)
```

Dari 4504 perusahaan pada data, perusahaan yang paling banyak memiliki complain adalah `Transunion Intermediate Holdings`. Perlu diketahui bahwa banyaknya complain yang diperhitungkan tidak mempertimbangkan volume perusahaan. Misalnya, perusahaan dengan lebih banyak customer tentunya memiliki kemungkinan banyak complain dibandingkan perusahaan yang lebih sedikit pelanggannya dan juga pada analisa ini kita hanya memperhitungkan complain yang dilengkapi dengan narasi dari customer tersebut.

Berikutnya kita akan fokus untuk menganalisa complai dari perusahaan `Transunion Intermediate Holdings` yang memiliki paling banyak narasi complain dari data.

```{r, eval=FALSE, echo = FALSE}
data_complaint <- customer %>%
  na.omit(Consumer.complaint.narrative) %>% 
  filter(Company %in% top_company$Company) %>%
  droplevels()
```

```{r, echo = FALSE}
#write.csv(data_complaint,"assets/02-finance/data_complaint.csv", row.names = F)
data_complaint <- read.csv("assets/02-finance/data_complaint.csv")
```

Setelah memperoleh data observasi, selanjutnya membersihkan data text:
```{r}
data_clean <- data_complaint %>% 
  select(Consumer.complaint.narrative) %>% 
  mutate(Consumer.complaint.narrative = Consumer.complaint.narrative %>% 
  tolower() %>% 
  str_trim() %>% 
  str_remove_all(pattern = "[[:punct:]]") %>% 
  str_remove_all(pattern = "[0-9]") %>% 
  str_remove_all(pattern = "xxxx") %>% 
  replace_contraction() %>% 
  replace_word_elongation() %>% 
  replace_white() %>% 
  str_squish())
head(data_clean)
```

Setelah membersihkan data text, selanjutnya kita akan melakukan proses `tokenization` yaitu memecah 1 kalimat menjadi beberapa `term`, pada proses berikut ini juga diperoleh frekuensi dari setiap term yang muncul.
```{r}
text.dat <- data_clean %>% 
  rowid_to_column("id") %>% 
  unnest_tokens(word, Consumer.complaint.narrative) %>% 
  anti_join(stop_words) %>% 
  count(word, sort = T) %>% 
  rename(words = word,
         freq = n) %>% 
  filter(words != is.na(words),
         freq > 50)
head(text.dat)
```

Kata yang sudah diperoleh akan divisualisasikan dengan wordcloud. Semakin sering suatu kata digunakan, maka semakin besar pula ukuran kata tersebut ditampilkan dalam wordcloud. Artinya kita dapat mengetahui kata yang paling sering digunakan oleh customer `Transunion Intermediate Holdings`. Kata `credit`, `report`, dan `account` merupakan kata yang paling sering digunakan oleh customer saat complain.
```{r}
wordcloud2(data = text.dat, size = 1, color = 'random-dark', shuffle = 1)
```

### Comparing Sentiment Dictionaries

Semakin banyak informasi yang ditampilkan, dapat membantu pihak marketing mengembangkan strategi yang efektif dalam meningkatkan pelayanan, berikutnya tidak hanya kata yang sering muncul yang akan ditampilkan, namun juga informasi mengenai kata tersebut merupakan kata positif atau negatif yang digunakan oleh customer saat mengajukan complain.

```{r}
text_dat <-  data_clean %>% 
  rowid_to_column("id") %>% 
  unnest_tokens(word, Consumer.complaint.narrative) %>% 
  anti_join(stop_words) %>% 
  mutate(word = wordStem(word)) %>% 
  count(word, sort = T) %>% 
  filter(word != is.na(word))
head(text_dat,20)
```

```{r}
bing_word <- text_dat %>% 
  inner_join(get_sentiments("bing")) 
head(bing_word)
```

```{r}
library(reshape2)
library(wordcloud)
bing_word %>% 
  acast(word ~ sentiment, value.var = "n", fill = 0) %>% 
  comparison.cloud(colors = c("gray70","gray20"), max.words = 200)
```

Sentiment Analysis yang dilakukan sebelumnya kita memperhitungan kemunculan kata positif dan negatif. Salah satu kelemahan pada pendekatan tersebut terkadang dapat disalah artikan penggunaannya pada sebuah kata, misal `correct` dan `support` akan dianggap sebagai kata positif. Namun, arti kata tersebut akan berubah jika terdapat kata `not` didepannya. Pada analisis berikut ini kita akan menggunakan n-gram untuk melihat seberapa sering `word1` diikuti oleh `word2`. Tokenisasi menggunakan n-gram berguna untuk eksplorasi kata yang memiliki hubungan. Ketika kita mengatur `n = 2` artinya kita akan menampilkan dua kata berturut-turut atau sering disebut dengam bigrams. Hasil dari visualisasi berikut ini menampilkan kata-kata yang berhubungan dengan kata `not`.

```{r}
dat_bigrams <- data_clean %>% 
  unnest_tokens(bigram, Consumer.complaint.narrative, token = "ngrams", n= 2) %>%
  separate(bigram, c("word1","word2"), sep = " ") %>% filter(word1 == "not") %>% 
  inner_join(get_sentiments("afinn"), by = c(word2 = "word")) %>% 
  count(word1,word2, value, sort = T) %>% 
  mutate(contribution = n*value) %>% 
  arrange(desc(abs(contribution))) %>% 
  group_by(word1) %>% 
  dplyr::slice(seq_len(20)) %>% 
  arrange(word1, desc(contribution)) %>% 
  ungroup() 
```

```{r}
graph_bigram <- dat_bigrams %>% 
                graph_from_data_frame()

set.seed(123)

a <- grid::arrow(type = "closed", length = unit(.15, "inches"))

ggraph(graph_bigram, layout = "fr") +
  geom_edge_link(alpha = .25) +
  geom_edge_density(aes(fill = value)) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name),  repel = TRUE) +
  theme_void() + theme(legend.position = "none",
                       plot.title = element_text(hjust = 0.5)) +
  ggtitle("Negation Bigram Network")

```


### Correlation Pairs

Analisis berikutnya, akan dilakukan eksplorasi untuk mengetahui kata-kata yang memiliki kecenderungan muncul bersama pada complain nasabah dengan mencari nilai korelasi antar kata.

```{r}
data_clean_cor <- data_complaint %>% 
  select(Consumer.complaint.narrative,Issue,Product) %>% 
  mutate(Consumer.complaint.narrative = Consumer.complaint.narrative %>% 
  tolower() %>% 
  str_trim() %>% 
  str_remove_all(pattern = "[[:punct:]]") %>% 
  str_remove_all(pattern = "[0-9]") %>% 
  str_remove_all(pattern = "xxxx") %>% 
  replace_contraction() %>% 
  replace_word_elongation() %>% 
  replace_white() %>% 
  str_squish())
head(data_clean_cor)
```

```{r}
text_dat_cor <-  data_clean_cor %>% 
                rowid_to_column("id") %>% 
                unnest_tokens(word,Consumer.complaint.narrative) %>% 
                anti_join(stop_words)
```


Untuk memperoleh korelasi antar kata dapat menggunakan function `pairwise_cor()` dari package `widyr`
```{r}
words_cors <- text_dat_cor %>% 
  group_by(word) %>% 
  filter(n() > 100) %>%
  pairwise_cor(word, Issue, sort = T) 

```

Korelasi antar kata dapat kita tampilkan secar visual menggunakan package `ggraph`. Pada visualisasi berikut kita hanya ingin menampilkan kata yang memiliki korelasi lebih dari 0.9. Artinya korelasi pada visualisasi berikut memiliki kecenderungan muncul bersamaan saat nasabah mengajukan complain.
```{r}
set.seed(100)

words_cors %>%
  filter(correlation > .9) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = correlation)) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), repel = TRUE) +
  theme_void() +
  ggtitle("Correlation between Words")+
  theme(legend.position = "none",
                       plot.title = element_text(hjust = 0.5,face = "bold"))
```

Manfaat dari Sentiment Analysis yang telah dilakukan adalah kita dapat mengetahui pesan utama dari pendapat dan pemikiran customer terhadap suatu company atau product. Selain itu, output dari sentiment analysis dapat memberikan gambaran mengenai pelayanan atau product yang belum sesuai. Hal tersebut dapat membantu tim marketing untuk meneliti trend yang dibutuhkan customer dengan lebih baik. Seiring dengan peningkatan kualitas layanan dan pengembangan produk yang lebih baik, tentunya akan mengurangi tingkat churn customer.





