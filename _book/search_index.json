[
["index.html", "Machine Learning Application in Industry Chapter 1 Introduction", " Machine Learning Application in Industry Algoritma Team 24, April 2020 Chapter 1 Introduction The following bookdown is produced by the team at Algoritma for Machine Learning Application in Industry. Algoritma is a data science education center based in Jakarta. We organize workshops and training programs to help working professionals and students gain mastery in various data science sub-fields: data visualization, machine learning, data modeling, statistical inference, etc. "],
["telecommunication.html", "Chapter 2 Telecommunication 2.1 Customer Churn Prediction", " Chapter 2 Telecommunication 2.1 Customer Churn Prediction 2.1.1 Background Customer Churn didefinisikan sebagai kecenderungan pelanggan untuk berhenti melakukan interaksi dengan sebuah perusahaan. Perusahaan telekomunikasi memiliki kebutuhan untuk mengetahui customer yang akan berhenti berlangganan atau tidak, karena biaya mempertahankan pelanggan yang sudah ada jauh lebih sedikit dibandingkan memperoleh pelanggan baru. Perusahaan biasanya mendefinisikan 2 tipe customer churn, yaitu voluntary churn dan involuntary churn. Voluntary churn merupakan pelanggan yang sengaja berhenti dan beralih ke perusahaan lain, sedangkan involuntary churn merupakan pelanggan yang berhenti karena perpindahan lokasi, kematian, atau alasan lain yang sulit dikontrol. Analisis voluntary churn tentunya tidak sulit untuk mempelajari karakteristik pelanggan yang dapat dilihat dari data profil pelanggan. Permasalah diatas dapat dijawab dengan membuat model prediksi customer churn. Harapannya dengan adanya model prediksi customer churn, dapat mempermudah pihak perusahaan telekomunikasi untuk memperoleh informasi mengenai pelanggan yang berpeluang besar untuk churn. 2.1.2 Modelling Analysis 2.1.2.1 Import Data Data yang digunakan merupakan data profil pelanggan perusahaan telekomunikasi yang diperoleh dar link berikut. Data tersebut berisikan 7043 observasi dengan 21 kolom. Target variabel pada data ini adalah Churn, kita akan memprediksi apakah pelanggan akan berhenti berlangganan produk atau akan tetep berlangganan. customer &lt;- read.csv(&quot;assets/01-telco/WA_Fn-UseC_-Telco-Customer-Churn.csv&quot;) head(customer) #&gt; customerID gender SeniorCitizen Partner Dependents tenure #&gt; 1 7590-VHVEG Female 0 Yes No 1 #&gt; 2 5575-GNVDE Male 0 No No 34 #&gt; 3 3668-QPYBK Male 0 No No 2 #&gt; 4 7795-CFOCW Male 0 No No 45 #&gt; 5 9237-HQITU Female 0 No No 2 #&gt; 6 9305-CDSKC Female 0 No No 8 #&gt; PhoneService MultipleLines InternetService OnlineSecurity #&gt; 1 No No phone service DSL No #&gt; 2 Yes No DSL Yes #&gt; 3 Yes No DSL Yes #&gt; 4 No No phone service DSL Yes #&gt; 5 Yes No Fiber optic No #&gt; 6 Yes Yes Fiber optic No #&gt; OnlineBackup DeviceProtection TechSupport StreamingTV #&gt; 1 Yes No No No #&gt; 2 No Yes No No #&gt; 3 Yes No No No #&gt; 4 No Yes Yes No #&gt; 5 No No No No #&gt; 6 No Yes No Yes #&gt; StreamingMovies Contract PaperlessBilling #&gt; 1 No Month-to-month Yes #&gt; 2 No One year No #&gt; 3 No Month-to-month Yes #&gt; 4 No One year No #&gt; 5 No Month-to-month Yes #&gt; 6 Yes Month-to-month Yes #&gt; PaymentMethod MonthlyCharges TotalCharges Churn #&gt; 1 Electronic check 29.85 29.85 No #&gt; 2 Mailed check 56.95 1889.50 No #&gt; 3 Mailed check 53.85 108.15 Yes #&gt; 4 Bank transfer (automatic) 42.30 1840.75 No #&gt; 5 Electronic check 70.70 151.65 Yes #&gt; 6 Electronic check 99.65 820.50 Yes Berikut ini merupakan deskripsi untuk setiap variabel: CustomerID: Customer ID Gender: Gender pelanggan yaitu Female dan Male SeniorCitizen: Apakah pelanggan merupakan senio citizen (0: No, 1: Yes) Partner: Apakah pelanggan memiliki partner atau tidak (Yes, No) Dependents: Apakah pelanggan memiliki tanggungan atau tidak (Yes, No) Tenure: Jumlah bulan dalam menggunakan produk perusahaan MultipleLines: Apakah pelanggan memiliki banyak saluran atau tidak (Yes, No, No phone service) OnlineSecurity: Apakah pelanggan memiliki keamanan online atau tidak OnlineBackup: Apakah pelanggan memiliki cadangan online atau tidak DeviceProtection: Apakah pelanggan memiliki perlindungan perangkat atau tidak TechSupport: Apakah pelanggan memiliki dukungan teknis atau tidak StreamingTV: Apakah pelanggan berlangganan TV streaming atau tidak StreamingMovies: Apakah pelanggan berlangganan movies streaming atau tidak Contract: Ketentuan kontrak berlangganan (Month-to-month, One year, Two year) PaperlessBilling: Apakah pelanggan memiliki tagihan tanpa kertas atau tidak (Yes, No) PaymentMethod: Metode pembayaran (Electronic check, Mailed check, Bank transfer (automatic), Credit card (automatic)) MonthlyCharges: Jumlah pembayaran yang dilakukan setiap bulan TotalCharges: Jumlah total yang dibebankan oleh pelanggan Churn: Apakah pelanggan Churn atau tidak (Yes or No) 2.1.2.2 Exploratory Data Sebelum eksplorasi lebih lanjut, perlu diketahui kelengkapan data yang dimiliki: colSums(is.na(customer)) #&gt; customerID gender SeniorCitizen Partner #&gt; 0 0 0 0 #&gt; Dependents tenure PhoneService MultipleLines #&gt; 0 0 0 0 #&gt; InternetService OnlineSecurity OnlineBackup DeviceProtection #&gt; 0 0 0 0 #&gt; TechSupport StreamingTV StreamingMovies Contract #&gt; 0 0 0 0 #&gt; PaperlessBilling PaymentMethod MonthlyCharges TotalCharges #&gt; 0 0 0 11 #&gt; Churn #&gt; 0 Dari 7043 observasi ternyata terdapat missing values sebanyak 11 observasi pada kolom TotalCharges. Karena jumlah missing values cukup sedikit kita dapat membuat observasi tersebut. Selain itu, perlu kita buang variabel yang tidak dibutuhkan pada pemodelan yaitu customerID dan juga sesuaikan tipe data yang seharusnya. customer &lt;- customer %&gt;% select(-customerID) %&gt;% na.omit() %&gt;% mutate(SeniorCitizen = as.factor(SeniorCitizen)) Untuk mengetahui proporsi kelas pada setiap variable kategori, kita dapat menggunakan function inspect_cat dari package inspectdf seperti berikut: customer %&gt;% inspect_cat() %&gt;% show_plot() Dari hasil plot diatas dapat diketahui proporsi kelas untuk target variabel cenderung lebih banyak dikategori No namun masih seimbang. Sedangkan untuk variabel lainnya untuk proporsi setiap level nya mayoritas seimbang. Berikutnya kita dapat eksplorasi persebaran untuk variabel data numerik dengan function inspect_num dari package inspectdf seperti berikut: customer %&gt;% inspect_num() %&gt;% show_plot() Dari ketiga variabel numerik yang dimiliki, persebaran data cukup beragam untuk setiap nilai. 2.1.2.3 Modelling Sebelum masuk ke tahap modelling, kita perlu membagi data menjadi data_train dan data_test dengan proporsi 80:20. set.seed(100) idx &lt;- initial_split(data = customer,prop = 0.8,strata = Churn) data_train &lt;- training(idx) data_test &lt;- testing(idx) Berikutnya bentuk model random forest menggunakan package caret, tentukan banyaknya cross validation dan repetition pada model dan juga target variabel dan prediktor yang digunakan. set.seed(100) ctrl &lt;- trainControl(method=&quot;repeatedcv&quot;, number=5, repeats=3) # model_forest &lt;- train(Churn ~ ., data=data_train, method=&quot;rf&quot;, trControl = ctrl) import model yang sudah dijalankan pada chunk sebelumnya menggunakan readRDS. #saveRDS(model_forest,&quot;assets/01-telco/model_forest.rds&quot;) model_forest &lt;- readRDS(&quot;assets/01-telco/model_forest.rds&quot;) model_forest #&gt; Random Forest #&gt; #&gt; 5627 samples #&gt; 19 predictor #&gt; 2 classes: &#39;No&#39;, &#39;Yes&#39; #&gt; #&gt; No pre-processing #&gt; Resampling: Cross-Validated (5 fold, repeated 3 times) #&gt; Summary of sample sizes: 4501, 4502, 4501, 4502, 4502, 4501, ... #&gt; Resampling results across tuning parameters: #&gt; #&gt; mtry Accuracy Kappa #&gt; 2 0.7837817 0.3252122 #&gt; 16 0.7750746 0.3779712 #&gt; 30 0.7731203 0.3727503 #&gt; #&gt; Accuracy was used to select the optimal model using the #&gt; largest value. #&gt; The final value used for the model was mtry = 2. Dari hasil yang diperoleh pada model_forest, didapatkan accuraci sebesar 0.78 dengan mtry sebanyak 2. Selanjutnya, akan dilakukan tuning model dengan melakukan upsample data. Artinya, kita akan membuat proporsi dari target variabel sama besar. up_train &lt;- upSample(x = data_train[,-20], y = data_train$Churn, yname = &quot;Churn&quot;) Dilakukan pembuat model random forest dengan data upsample: set.seed(100) # ctrl &lt;- trainControl(method=&quot;repeatedcv&quot;, number=5, repeats=3) # forest_upc &lt;- train(Churn ~ ., data=up_train, method=&quot;rf&quot;, trControl = ctrl) #saveRDS(forest_upc,&quot;assets/01-telco/model_caret.rds&quot;) forest_upc &lt;- readRDS(&quot;assets/01-telco/model_caret.rds&quot;) Dari hasil model kedua diperoleh hasil sebagai berikut: forest_upc #&gt; Random Forest #&gt; #&gt; 8262 samples #&gt; 19 predictor #&gt; 2 classes: &#39;No&#39;, &#39;Yes&#39; #&gt; #&gt; No pre-processing #&gt; Resampling: Cross-Validated (5 fold, repeated 3 times) #&gt; Summary of sample sizes: 6609, 6610, 6609, 6610, 6610, 6610, ... #&gt; Resampling results across tuning parameters: #&gt; #&gt; mtry Accuracy Kappa #&gt; 2 0.7760017 0.5520022 #&gt; 16 0.8911472 0.7822945 #&gt; 30 0.8875167 0.7750336 #&gt; #&gt; Accuracy was used to select the optimal model using the #&gt; largest value. #&gt; The final value used for the model was mtry = 16. Setelah dilakukan upsample data, terlihat nilai accuracy yang diperoleh lebih besar dibandingkan model sebelumnya sebesar 0.89 dengan mtry sebanyak 16. Selanjutnya, akan dilakukan prediksi terhadap data_test: pred &lt;- predict(forest_upc,newdata = data_test,type = &quot;prob&quot;) pred$result &lt;- as.factor(ifelse(pred$Yes &gt; 0.45, &quot;Yes&quot;,&quot;No&quot;)) confusionMatrix(pred$result, data_test$Churn,positive = &quot;Yes&quot;) #&gt; Confusion Matrix and Statistics #&gt; #&gt; Reference #&gt; Prediction No Yes #&gt; No 849 109 #&gt; Yes 183 264 #&gt; #&gt; Accuracy : 0.7922 #&gt; 95% CI : (0.77, 0.8131) #&gt; No Information Rate : 0.7345 #&gt; P-Value [Acc &gt; NIR] : 0.00000031 #&gt; #&gt; Kappa : 0.4989 #&gt; #&gt; Mcnemar&#39;s Test P-Value : 0.00001937 #&gt; #&gt; Sensitivity : 0.7078 #&gt; Specificity : 0.8227 #&gt; Pos Pred Value : 0.5906 #&gt; Neg Pred Value : 0.8862 #&gt; Prevalence : 0.2655 #&gt; Detection Rate : 0.1879 #&gt; Detection Prevalence : 0.3181 #&gt; Balanced Accuracy : 0.7652 #&gt; #&gt; &#39;Positive&#39; Class : Yes #&gt; Pada kasus ini kita ingin memperoleh nila sensitivity/recall yang lebih besar, dengan menggunakan threshold sebesar 0.4 diperoleh nilai recall sebesar 0.70 dengan accuracy sebesar 0.79 dan precision sebesar 0.59. Dari model yang telah terbentuk kita dapat memperoleh nilai AUC pada model: library(ROCR) pred_prob &lt;- predict(object = forest_upc,newdata = data_test,type = &quot;prob&quot;) pred &lt;- prediction(pred_prob[,2],labels = data_test$Churn) perf &lt;- performance(prediction.obj = pred,measure = &quot;tpr&quot;,x.measure = &quot;fpr&quot;) plot(perf) auc &lt;- performance(pred,measure = &quot;auc&quot;) auc@y.values[[1]] #&gt; [1] 0.8513259 2.1.3 Conclusion library(lime) test_x &lt;- data_test %&gt;% dplyr::select(-Churn) explainer &lt;- lime(test_x, forest_upc) explanation &lt;- lime::explain(test_x[1:2,], explainer, labels = c(&quot;Yes&quot;), n_features = 8) plot_features(explanation) Setelah adanya model prediksi customer churn, pihak perusahaan telekomunikasi dapat dengan mudah mengetahui pelanggan yang memiliki kecendurungan akan churn. Kedua plot diatas memperlihatkan prediksi dua customer, kedua customer memiliki peluang besar untuk churn dan kita dapat mengetahui variabel mana yang supports dan contradicts terhadap hasil prediksi. "],
["finance.html", "Chapter 3 Finance 3.1 Credit Risk Analysis 3.2 Evaluating Customer Financial Complaints", " Chapter 3 Finance 3.1 Credit Risk Analysis 3.1.1 Background Credit scoring membutuhkan berbagai data profil calon peminjam sehingga tingkat resiko dapat dihitung dengan tepat. Semakin benar dan lengkap data yang disediakan, maka semakin akurat perhitungan yang dilakukan. Proses tersebut tentunya merupakan hal yang baik, namun di sisi calon peminjam proses yang harus dilalui dirasa sangat merepotkan dan membutuhkan waktu untuk menunggu. Dan seiring tingkat kompetisi yang samkin tinggi di industri finansial, customer memiliki banyak alternatif. Semakin cepat proses yang ditawarkan, semakin tinggi kesempatan untuk mendapatkan peminjam. Tantangan pun muncul, bagaimana mendapatkan pelanggan dengan proses yang efisien namun akurasi dari credit scoring tetap tinggi. Disinilah machine learning dapat membantu menganalisa data - data profil peminjam dan proses pembayaran sehingga dapat mengeluarkan rekomendasi profil pelanggan yang beresiko rendah. Harapannya setelah mempunyai model machine learning dengan perfomance model yang baik, pegawai bank dapat dengan mudah mengidentifikasi karakteristik customer yang memiliki peluang besar untuk melunasi pinjaman dengan lancar. Dengan adanya model machine learning ini tentunya akan mengurangi biaya dan waktu yang lebih cepat. 3.1.2 Modelling Analysis loan &lt;- read_csv(&quot;assets/02-finance/bankloans-copy.csv&quot;)%&gt;% mutate(default = factor(default, levels = c(0,1), labels = c(&quot;No&quot;,&quot;Yes&quot;))) %&gt;% rename(debtcred = creddebt) glimpse(loan) #&gt; Observations: 800 #&gt; Variables: 9 #&gt; $ age &lt;dbl&gt; 37, 25, 37, 29, 38, 32, 51, 27, 31, 37, 34, 31,... #&gt; $ ed &lt;dbl&gt; 1, 4, 3, 1, 2, 1, 2, 3, 1, 1, 4, 1, 1, 1, 2, 1,... #&gt; $ employ &lt;dbl&gt; 20, 0, 16, 1, 13, 8, 22, 3, 1, 5, 7, 3, 11, 1, ... #&gt; $ address &lt;dbl&gt; 2, 1, 14, 8, 0, 6, 23, 4, 1, 11, 15, 5, 1, 6, 4... #&gt; $ income &lt;dbl&gt; 56, 18, 50, 31, 59, 26, 120, 35, 24, 27, 40, 16... #&gt; $ debtinc &lt;dbl&gt; 1.9, 33.4, 36.6, 8.0, 2.4, 4.1, 7.6, 13.3, 4.5,... #&gt; $ debtcred &lt;dbl&gt; 0.542640, 2.801592, 7.320000, 0.156240, 0.40780... #&gt; $ othdebt &lt;dbl&gt; 0.521360, 3.210408, 10.980000, 2.323760, 1.0081... #&gt; $ default &lt;fct&gt; No, Yes, Yes, No, No, No, No, No, No, NA, No, N... head(loan) #&gt; # A tibble: 6 x 9 #&gt; age ed employ address income debtinc debtcred othdebt default #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; #&gt; 1 37 1 20 2 56 1.9 0.543 0.521 No #&gt; 2 25 4 0 1 18 33.4 2.80 3.21 Yes #&gt; 3 37 3 16 14 50 36.6 7.32 11.0 Yes #&gt; 4 29 1 1 8 31 8 0.156 2.32 No #&gt; 5 38 2 13 0 59 2.4 0.408 1.01 No #&gt; 6 32 1 8 6 26 4.1 0.326 0.740 No 3.1.2.1 Exploratory Data Analysis Eksplorasi hubungan target variabel dengan variabel prediktor loan %&gt;% na.omit() %&gt;% ggplot(aes(x = debtinc, fill = default)) + geom_density(alpha = 0.5, colour = FALSE) + scale_x_continuous( expand = expand_scale(mult = c(0, 0)) ) + scale_y_continuous(expand = expand_scale(mult = c(0, 0.1))) + scale_fill_manual(values = c(&quot;dodgerblue4&quot;,&quot;darkred&quot;)) + labs( title = &quot;Debt to Income Rario distribution&quot;, subtitle = &quot;estimated using kernel density function&quot;, x = &quot;Debt to Income Ratio&quot;, y = &quot;Income&quot;, fill = &quot;Default Status&quot; ) + theme_minimal() + theme( legend.position = &quot;top&quot;, legend.key.height = unit(12, &quot;pt&quot;), legend.key.width = unit(36, &quot;pt&quot;), ) loan %&gt;% na.omit() %&gt;% ggplot(aes(x = debtcred, y = income)) + geom_point(color = &quot;darkred&quot;) + geom_smooth(method = &quot;loess&quot;, se = FALSE, color = &quot;dodgerblue4&quot;) + facet_wrap(facets = vars(default), labeller = &quot;label_both&quot;) + scale_y_continuous(labels = dollar_format(scale = 1e-3, suffix = &quot;K&quot;)) + labs( title = &quot;The relation of credit to debt ratio and income&quot;, subtitle = &quot;for each default status&quot;, x = &quot;Debt to Credit Ratio&quot;, y = &quot;Income&quot; ) + theme_minimal() Check missing value loan %&gt;% is.na() %&gt;% colSums() %&gt;% enframe() %&gt;% arrange(desc(value)) #&gt; # A tibble: 9 x 2 #&gt; name value #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 default 150 #&gt; 2 age 0 #&gt; 3 ed 0 #&gt; 4 employ 0 #&gt; 5 address 0 #&gt; 6 income 0 #&gt; 7 debtinc 0 #&gt; 8 debtcred 0 #&gt; 9 othdebt 0 Berikutnya akan digunakan data observasi tanpa adanya data missing value loan &lt;- loan %&gt;% na.omit() 3.1.2.2 Modelling Split data train dan data test dengan proporsi 80:20. set.seed(100) intrain_loan &lt;- initial_split(data = loan, prop = 0.8, strata = &quot;default&quot;) train_loan &lt;- training(intrain_loan) test_loan &lt;- testing(intrain_loan) Cek proporsi dari target variabel prop.table(table(train_loan$default)) #&gt; #&gt; No Yes #&gt; 0.6149425 0.3850575 Untuk membuat data observasi menjadi lebih seimbang, dapat dilakukan upSample dari package caret. set.seed(47) train_loan_up &lt;- upSample(x = select(train_loan, -default), y = train_loan$default, yname = &quot;default&quot;) prop.table(table(train_loan_up$default)) #&gt; #&gt; No Yes #&gt; 0.5 0.5 Bentuk model random forest dengan 5 k-fold dan 3 repetition set.seed(47) ctrl &lt;- trainControl(method = &quot;repeatedcv&quot;, number = 5, repeats = 3, allowParallel=FALSE) model_forest &lt;- caret::train(default ~., data = train_loan_up, method = &quot;rf&quot;, trControl = ctrl) model_forest #&gt; Random Forest #&gt; #&gt; 642 samples #&gt; 8 predictor #&gt; 2 classes: &#39;No&#39;, &#39;Yes&#39; #&gt; #&gt; No pre-processing #&gt; Resampling: Cross-Validated (5 fold, repeated 3 times) #&gt; Summary of sample sizes: 514, 513, 513, 514, 514, 513, ... #&gt; Resampling results across tuning parameters: #&gt; #&gt; mtry Accuracy Kappa #&gt; 2 0.8483850 0.6967960 #&gt; 5 0.8359335 0.6718959 #&gt; 8 0.8317749 0.6635641 #&gt; #&gt; Accuracy was used to select the optimal model using the #&gt; largest value. #&gt; The final value used for the model was mtry = 2. Setelah dilakukan 3 repetition pada model, repetition pertama memiliki accuracy paling besar dengan jumlah mtry sebanyak 2. Confusion matrix yang diperoleh data observasi yang tidak digunakan sebagai sampel diperoleh sebagai berikut: model_forest$finalModel #&gt; #&gt; Call: #&gt; randomForest(x = x, y = y, mtry = param$mtry) #&gt; Type of random forest: classification #&gt; Number of trees: 500 #&gt; No. of variables tried at each split: 2 #&gt; #&gt; OOB estimate of error rate: 12.15% #&gt; Confusion matrix: #&gt; No Yes class.error #&gt; No 269 52 0.16199377 #&gt; Yes 26 295 0.08099688 Selanjutnya akan dilakukan prediksi untuk data test dan mencari nilai confusion matrix pada hasil prediksi. test_loan &lt;- test_loan %&gt;% mutate(pred_forest = predict(model_forest, newdata = test_loan)) confmat_loan_tune &lt;- confusionMatrix(test_loan$pred_forest, test_loan$default, mode = &quot;prec_recall&quot;, positive = &quot;Yes&quot;) eval_rf &lt;- tidy(confmat_loan_tune) %&gt;% mutate(model = &quot;Random Forest&quot;) %&gt;% select(model, term, estimate) %&gt;% filter(term %in% c(&quot;accuracy&quot;, &quot;precision&quot;, &quot;recall&quot;, &quot;specificity&quot;)) eval_rf #&gt; # A tibble: 4 x 3 #&gt; model term estimate #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 Random Forest accuracy 0.836 #&gt; 2 Random Forest specificity 0.886 #&gt; 3 Random Forest precision 0.804 #&gt; 4 Random Forest recall 0.755 Dari hasil confusion matrix dapat diketahui, kemampuan model memprediksi target variabel dapat dikatakan cukup baik. Selanjutnya kita akan mengetahui variable importance pada model yang diperoleh. plot(varImp(model_forest),main = &quot;Variable Importance&quot;,) Dapat diketahui variabel debtinc, employ, debtcred, othdebt, dan income merupakan 5 variable yang paling berpengaruh dan paling sering digunakan dalam pembuatan pohon. 3.1.3 Recommendation eval_rf #&gt; # A tibble: 4 x 3 #&gt; model term estimate #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 Random Forest accuracy 0.836 #&gt; 2 Random Forest specificity 0.886 #&gt; 3 Random Forest precision 0.804 #&gt; 4 Random Forest recall 0.755 Model machine learning untuk memprediksi kredit pinjaman customer yang lancar dan tidak lancar memiliki perfomance model yang cukup baik. Nantinya, pegawai bank dapat menggunakan model tersebut dengan mengisikan data pribadi setiap customer, kemudian hasil yang diperoleh dapat di visualisasikan sebagai berikut: library(lime) train_x &lt;- train_loan %&gt;% select(-c(default)) test_x &lt;- test_loan %&gt;% select(-c(default, pred_forest)) explainer &lt;- lime(test_x, model_forest) explanation &lt;- lime::explain(test_x[2:3,], explainer, labels = c(&quot;Yes&quot;), n_features = 8) plot_features(explanation) Hasil visualisasi tersebut adalah contoh prediksi salah satu customer, customer tersebut terprediksi yes yang memiliki arti customer tersebut berpeluang besar sebagai customer yang lancar melunasi pembayaran. Tentunya ketika hasil prediksi menyatakan customer tersebut berpeluang besar untuk kredit lancar, artinya bank akan memberikan pinjaman kepada customer tersebut. Dari hasil visual tersebut juga ditunjukkan variabel mana yang support dan contradicts terhadap hasil prediksi yang dihasilkan. 3.2 Evaluating Customer Financial Complaints 3.2.1 Background Penanganan complain customer pada perusahaan saat ini menjadi salah satu kunci utama suatu perusahaan dapat terus tumbuh dan berkembang, karena apabila nasabah merasa tidak mendapatkan layanan yang baik saat menyampaikan keluhan maka nasabah akan mudah berpindah ke perusahaan lain yang dianggap bisa memberikan layanan terhadap komplain dengan baik. Nasabah yang merasa tidak mendapatkan layanan baik biasanya akan mengajukan keluhan ke Consumer Financial Protection Bureau (CFPB), CFPB merupakan instansi yang bertanggung jawab atas perlindungan konsumen di sektor keuangan. CFPB menyediakan data yang berisi keluhan dari customer financial, data keluhan tersebut dapat dianalisa untuk dijadikan pertimbangan pihak perusahaan untuk mengetahui indikator yang memerlukan perbaikan demi meningkatkan kualitas layanan. 3.2.2 Exploratory Data Analysis customer &lt;- read_csv(&quot;assets/02-finance/consumer_complaints.csv&quot;)%&gt;% mutate_if(is.character, as.factor) %&gt;% data.frame() Data diperoleh dari Consumer Financial Protection Bureau (CFPB) yang mengatur penawaran dan penyediaan produk atau layanan nasabah keuangan. CFPB menyediakan pertanyaan-pertanyaan umum dan dapat membantu nasabah terhubung dengan perusahaan keuangan yang terlibat. Data tersebut berisikan keluhan nasabah dari berbagai bank di Amerika Serikat. top_company &lt;- customer %&gt;% na.omit(Consumer.complaint.narrative) %&gt;% group_by(Company) %&gt;% summarise(total = n()) %&gt;% arrange(desc(total)) %&gt;% head(1) Dari 4504 perusahaan pada data, perusahaan yang paling banyak memiliki complain adalah Transunion Intermediate Holdings. Perlu diketahui bahwa banyaknya complain yang diperhitungkan tidak mempertimbangkan volume perusahaan. Misalnya, perusahaan dengan lebih banyak customer tentunya memiliki kemungkinan banyak complain dibandingkan perusahaan yang lebih sedikit pelanggannya dan juga pada analisa ini kita hanya memperhitungkan complain yang dilengkapi dengan narasi dari customer tersebut. Berikutnya kita akan fokus untuk menganalisa complai dari perusahaan Transunion Intermediate Holdings yang memiliki paling banyak narasi complain dari data. Setelah memperoleh data observasi, selanjutnya membersihkan data text: data_clean &lt;- data_complaint %&gt;% select(Consumer.complaint.narrative) %&gt;% mutate(Consumer.complaint.narrative = Consumer.complaint.narrative %&gt;% tolower() %&gt;% str_trim() %&gt;% str_remove_all(pattern = &quot;[[:punct:]]&quot;) %&gt;% str_remove_all(pattern = &quot;[0-9]&quot;) %&gt;% str_remove_all(pattern = &quot;xxxx&quot;) %&gt;% replace_contraction() %&gt;% replace_word_elongation() %&gt;% replace_white() %&gt;% str_squish()) head(data_clean) #&gt; Consumer.complaint.narrative #&gt; 1 this legal notice being sent and delivered to you persuant to florida statutes notice of order to cease and desist from using personal and private information fl statute violation title xlvi crimes chapter fraudulent practices view entire chapter criminal use of personal identification information as used in this section the term a access device means any card plate code account number electronic serial number mobile identification number personal identification number or other telecommunications service equipment or instrument identifier or other means of account access that can be used alone or in conjunction with another access device to obtain money goods services or any other thing of value or that can be used to initiate a transfer of funds other than a transfer originated solely by paper instrument b authorization means empowerment permission or competence to act c harass means to engage in conduct directed at a specific person that is intended to cause substantial emotional distress to such person and serves no legitimate purpose harass does not mean to use personal identification information for accepted commercial purposes the term does not include constitutionally protected conduct such as organized protests or the use of personal identification information for accepted commercial purposes d individual means a single human being and does not mean a firm association of individuals corporation partnership joint venture sole proprietorship or any other entity e person means a person as defined in s f personal identification information means any name or number that may be used alone or in conjunction with any other information to identify a specific individual including any name postal or electronic mail address telephone number social security number date of birth mother s maiden name official stateissued or united statesissued driver license or identification number alien registration number government passport number employer or taxpayer identification number or food assistance account number bank account number credit or debit card number or personal identification number or code assigned to the holder of a debit card by the issuer to permit authorized electronic use of such card unique biometric data such as fingerprint voice print retina or iris image or other unique physical representation unique electronic identification number address or routing code medical records telecommunication identifying information or access device or other number or information that can be used to access a person s financial resources g counterfeit or fictitious personal identification information means any counterfeit fictitious or fabricated information in the similitude of the data outlined in paragraph f that although not truthful or accurate would in context lead a reasonably prudent person to credit its truthfulness and accuracy a any person who willfully and without authorization fraudulently uses or possesses with intent to fraudulently use personal identification information concerning an individual without first obtaining that individual s consent commits the offense of fraudulent use of personal identification information which is a felony of the third degree punishable as provided in s s or s b any person who willfully and without authorization fraudulently uses personal identification information concerning an individual without first obtaining that individual s consent commits a felony of the second degree punishable as provided in s s or s if the pecuniary benefit the value of the services #&gt; 2 transunion continues to report inaccurate negative items on my report that i have previously dispu ted ti methey continuos to report account with no account number inaccurate information that is hurting my credit #&gt; 3 my credit report has an incorrect address the address on the report is nv other than when i was in the i have lived in my entire life associated with this lv address are bills from and and both amounts are less than $ every time i dispute these bills and ask for source documents to prove i owe the bills are sold to collection agencies when these receivables are sold as a package the collection agencies reopen the outstanding debt and it reappears on my report i have a substantial credit history with mortgage car loans and credit cards i have never been late on any amount i owe there should be some protection against collection companies windmilling these bogus receivables thank you for all you do #&gt; 4 i recently discovered that reported my account as a charge off on my credit report when i was denied a loan by due to the charge off i immediately contacted by phone in an attempt to rectify the error i was informed by the customer service representative that my last payment was received on and that my account was reported as a charge off on we both agreed that it did not make sense that my account would be reported as a charge off days after my payment was received at the time reported my account as a charge off it was paid in full with a i have spent hours on the phone with numerous customer service representatives the phone calls have been extremely frustrating and have not produced a positive outcome every representative has told me the same thing due to the age of this account they do not have account records and can not provide me with the information that i am requesting yet they refuse to remove the inaccurate information from my credit report i was told that a supervisors would call me back on separate occasions to this day i have not received a call back from a supervisor i have sent numerous certified letters disputing the charge off that is currently being reported on my credit report and negatively effecting my credit i have asked for validation of my account including details explaining my why my account is being reported as a charge off with a balance i have asked for them to send me a copy of my payment history along with account notes the letters that i received in response to my letters have been a one paragraph response stating that they only report accurate information to the credit bureaus #&gt; 5 sent letter to credit reporting agencies telling them the inquiries on my report are unjustified the actions fail to comply with fcra section the letters are attached below credit agencies replied stating they did not need to investigate verify or remove inquiries credit agencies failed to prove requirements and fa iled to remove inquiries from my credit reports #&gt; 6 student loans have been discharged and i have a letter from the stating all loans have been discharged the credit bureaus refuse to accept the letter Setelah membersihkan data text, selanjutnya kita akan melakukan proses tokenization yaitu memecah 1 kalimat menjadi beberapa term, pada proses berikut ini juga diperoleh frekuensi dari setiap term yang muncul. text.dat &lt;- data_clean %&gt;% rowid_to_column(&quot;id&quot;) %&gt;% unnest_tokens(word, Consumer.complaint.narrative) %&gt;% anti_join(stop_words) %&gt;% count(word, sort = T) %&gt;% rename(words = word, freq = n) %&gt;% filter(words != is.na(words), freq &gt; 50) head(text.dat) #&gt; # A tibble: 6 x 2 #&gt; words freq #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 credit 1572 #&gt; 2 report 774 #&gt; 3 account 613 #&gt; 4 transunion 529 #&gt; 5 information 503 #&gt; 6 reporting 358 Kata yang sudah diperoleh akan divisualisasikan dengan wordcloud. Semakin sering suatu kata digunakan, maka semakin besar pula ukuran kata tersebut ditampilkan dalam wordcloud. Artinya kita dapat mengetahui kata yang paling sering digunakan oleh customer Transunion Intermediate Holdings. Kata credit, report, dan account merupakan kata yang paling sering digunakan oleh customer saat complain. wordcloud2(data = text.dat, size = 1, color = &#39;random-dark&#39;, shuffle = 1) 3.2.3 Comparing Sentiment Dictionaries Semakin banyak informasi yang ditampilkan, dapat membantu pihak marketing mengembangkan strategi yang efektif dalam meningkatkan pelayanan, berikutnya tidak hanya kata yang sering muncul yang akan ditampilkan, namun juga informasi mengenai kata tersebut merupakan kata positif atau negatif yang digunakan oleh customer saat mengajukan complain. text_dat &lt;- data_clean %&gt;% rowid_to_column(&quot;id&quot;) %&gt;% unnest_tokens(word, Consumer.complaint.narrative) %&gt;% anti_join(stop_words) %&gt;% mutate(word = wordStem(word)) %&gt;% count(word, sort = T) %&gt;% filter(word != is.na(word)) head(text_dat,20) #&gt; # A tibble: 20 x 2 #&gt; word n #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 credit 1573 #&gt; 2 report 1462 #&gt; 3 account 869 #&gt; 4 inform 584 #&gt; 5 transunion 533 #&gt; 6 remov 423 #&gt; 7 inquiri 420 #&gt; 8 disput 415 #&gt; 9 file 345 #&gt; 10 request 329 #&gt; 11 letter 309 #&gt; 12 loan 287 #&gt; 13 payment 269 #&gt; 14 bureau 263 #&gt; 15 verifi 242 #&gt; 16 call 228 #&gt; 17 time 227 #&gt; 18 compani 223 #&gt; 19 receiv 218 #&gt; 20 agenc 213 bing_word &lt;- text_dat %&gt;% inner_join(get_sentiments(&quot;bing&quot;)) head(bing_word) #&gt; # A tibble: 6 x 3 #&gt; word n sentiment #&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; #&gt; 1 debt 206 negative #&gt; 2 correct 150 positive #&gt; 3 complaint 115 negative #&gt; 4 fraud 96 negative #&gt; 5 incorrect 89 negative #&gt; 6 hard 82 negative library(reshape2) library(wordcloud) bing_word %&gt;% acast(word ~ sentiment, value.var = &quot;n&quot;, fill = 0) %&gt;% comparison.cloud(colors = c(&quot;gray70&quot;,&quot;gray20&quot;), max.words = 200) Sentiment Analysis yang dilakukan sebelumnya kita memperhitungan kemunculan kata positif dan negatif. Salah satu kelemahan pada pendekatan tersebut terkadang dapat disalah artikan penggunaannya pada sebuah kata, misal correct dan support akan dianggap sebagai kata positif. Namun, arti kata tersebut akan berubah jika terdapat kata not didepannya. Pada analisis berikut ini kita akan menggunakan n-gram untuk melihat seberapa sering word1 diikuti oleh word2. Tokenisasi menggunakan n-gram berguna untuk eksplorasi kata yang memiliki hubungan. Ketika kita mengatur n = 2 artinya kita akan menampilkan dua kata berturut-turut atau sering disebut dengam bigrams. Hasil dari visualisasi berikut ini menampilkan kata-kata yang berhubungan dengan kata not. dat_bigrams &lt;- data_clean %&gt;% unnest_tokens(bigram, Consumer.complaint.narrative, token = &quot;ngrams&quot;, n= 2) %&gt;% separate(bigram, c(&quot;word1&quot;,&quot;word2&quot;), sep = &quot; &quot;) %&gt;% filter(word1 == &quot;not&quot;) %&gt;% inner_join(get_sentiments(&quot;afinn&quot;), by = c(word2 = &quot;word&quot;)) %&gt;% count(word1,word2, value, sort = T) %&gt;% mutate(contribution = n*value) %&gt;% arrange(desc(abs(contribution))) %&gt;% group_by(word1) %&gt;% slice(seq_len(20)) %&gt;% arrange(word1, desc(contribution)) %&gt;% ungroup() graph_bigram &lt;- dat_bigrams %&gt;% graph_from_data_frame() set.seed(123) a &lt;- grid::arrow(type = &quot;closed&quot;, length = unit(.15, &quot;inches&quot;)) ggraph(graph_bigram, layout = &quot;fr&quot;) + geom_edge_link(alpha = .25) + geom_edge_density(aes(fill = value)) + geom_node_point(color = &quot;lightblue&quot;, size = 5) + geom_node_text(aes(label = name), repel = TRUE) + theme_void() + theme(legend.position = &quot;none&quot;, plot.title = element_text(hjust = 0.5)) + ggtitle(&quot;Negation Bigram Network&quot;) 3.2.4 Correlation Pairs Analisis berikutnya, akan dilakukan eksplorasi untuk mengetahui kata-kata yang memiliki kecenderungan muncul bersama pada complain nasabah dengan mencari nilai korelasi antar kata. data_clean_cor &lt;- data_complaint %&gt;% select(Consumer.complaint.narrative,Issue,Product) %&gt;% mutate(Consumer.complaint.narrative = Consumer.complaint.narrative %&gt;% tolower() %&gt;% str_trim() %&gt;% str_remove_all(pattern = &quot;[[:punct:]]&quot;) %&gt;% str_remove_all(pattern = &quot;[0-9]&quot;) %&gt;% str_remove_all(pattern = &quot;xxxx&quot;) %&gt;% replace_contraction() %&gt;% replace_word_elongation() %&gt;% replace_white() %&gt;% str_squish()) head(data_clean_cor) #&gt; Consumer.complaint.narrative #&gt; 1 this legal notice being sent and delivered to you persuant to florida statutes notice of order to cease and desist from using personal and private information fl statute violation title xlvi crimes chapter fraudulent practices view entire chapter criminal use of personal identification information as used in this section the term a access device means any card plate code account number electronic serial number mobile identification number personal identification number or other telecommunications service equipment or instrument identifier or other means of account access that can be used alone or in conjunction with another access device to obtain money goods services or any other thing of value or that can be used to initiate a transfer of funds other than a transfer originated solely by paper instrument b authorization means empowerment permission or competence to act c harass means to engage in conduct directed at a specific person that is intended to cause substantial emotional distress to such person and serves no legitimate purpose harass does not mean to use personal identification information for accepted commercial purposes the term does not include constitutionally protected conduct such as organized protests or the use of personal identification information for accepted commercial purposes d individual means a single human being and does not mean a firm association of individuals corporation partnership joint venture sole proprietorship or any other entity e person means a person as defined in s f personal identification information means any name or number that may be used alone or in conjunction with any other information to identify a specific individual including any name postal or electronic mail address telephone number social security number date of birth mother s maiden name official stateissued or united statesissued driver license or identification number alien registration number government passport number employer or taxpayer identification number or food assistance account number bank account number credit or debit card number or personal identification number or code assigned to the holder of a debit card by the issuer to permit authorized electronic use of such card unique biometric data such as fingerprint voice print retina or iris image or other unique physical representation unique electronic identification number address or routing code medical records telecommunication identifying information or access device or other number or information that can be used to access a person s financial resources g counterfeit or fictitious personal identification information means any counterfeit fictitious or fabricated information in the similitude of the data outlined in paragraph f that although not truthful or accurate would in context lead a reasonably prudent person to credit its truthfulness and accuracy a any person who willfully and without authorization fraudulently uses or possesses with intent to fraudulently use personal identification information concerning an individual without first obtaining that individual s consent commits the offense of fraudulent use of personal identification information which is a felony of the third degree punishable as provided in s s or s b any person who willfully and without authorization fraudulently uses personal identification information concerning an individual without first obtaining that individual s consent commits a felony of the second degree punishable as provided in s s or s if the pecuniary benefit the value of the services #&gt; 2 transunion continues to report inaccurate negative items on my report that i have previously dispu ted ti methey continuos to report account with no account number inaccurate information that is hurting my credit #&gt; 3 my credit report has an incorrect address the address on the report is nv other than when i was in the i have lived in my entire life associated with this lv address are bills from and and both amounts are less than $ every time i dispute these bills and ask for source documents to prove i owe the bills are sold to collection agencies when these receivables are sold as a package the collection agencies reopen the outstanding debt and it reappears on my report i have a substantial credit history with mortgage car loans and credit cards i have never been late on any amount i owe there should be some protection against collection companies windmilling these bogus receivables thank you for all you do #&gt; 4 i recently discovered that reported my account as a charge off on my credit report when i was denied a loan by due to the charge off i immediately contacted by phone in an attempt to rectify the error i was informed by the customer service representative that my last payment was received on and that my account was reported as a charge off on we both agreed that it did not make sense that my account would be reported as a charge off days after my payment was received at the time reported my account as a charge off it was paid in full with a i have spent hours on the phone with numerous customer service representatives the phone calls have been extremely frustrating and have not produced a positive outcome every representative has told me the same thing due to the age of this account they do not have account records and can not provide me with the information that i am requesting yet they refuse to remove the inaccurate information from my credit report i was told that a supervisors would call me back on separate occasions to this day i have not received a call back from a supervisor i have sent numerous certified letters disputing the charge off that is currently being reported on my credit report and negatively effecting my credit i have asked for validation of my account including details explaining my why my account is being reported as a charge off with a balance i have asked for them to send me a copy of my payment history along with account notes the letters that i received in response to my letters have been a one paragraph response stating that they only report accurate information to the credit bureaus #&gt; 5 sent letter to credit reporting agencies telling them the inquiries on my report are unjustified the actions fail to comply with fcra section the letters are attached below credit agencies replied stating they did not need to investigate verify or remove inquiries credit agencies failed to prove requirements and fa iled to remove inquiries from my credit reports #&gt; 6 student loans have been discharged and i have a letter from the stating all loans have been discharged the credit bureaus refuse to accept the letter #&gt; Issue #&gt; 1 Disclosure verification of debt #&gt; 2 Problem with a credit reporting company&#39;s investigation into an existing problem #&gt; 3 Incorrect information on your report #&gt; 4 Problem with a credit reporting company&#39;s investigation into an existing problem #&gt; 5 Problem with a credit reporting company&#39;s investigation into an existing problem #&gt; 6 Incorrect information on your report #&gt; Product #&gt; 1 Debt collection #&gt; 2 Credit reporting, credit repair services, or other personal consumer reports #&gt; 3 Credit reporting, credit repair services, or other personal consumer reports #&gt; 4 Credit reporting, credit repair services, or other personal consumer reports #&gt; 5 Credit reporting, credit repair services, or other personal consumer reports #&gt; 6 Credit reporting, credit repair services, or other personal consumer reports text_dat_cor &lt;- data_clean_cor %&gt;% rowid_to_column(&quot;id&quot;) %&gt;% unnest_tokens(word,Consumer.complaint.narrative) %&gt;% anti_join(stop_words) Untuk memperoleh korelasi antar kata dapat menggunakan function pairwise_cor() dari package widyr words_cors &lt;- text_dat_cor %&gt;% group_by(word) %&gt;% filter(n() &gt; 100) %&gt;% pairwise_cor(word, Issue, sort = T) Korelasi antar kata dapat kita tampilkan secar visual menggunakan package ggraph. Pada visualisasi berikut kita hanya ingin menampilkan kata yang memiliki korelasi lebih dari 0.9. Artinya korelasi pada visualisasi berikut memiliki kecenderungan muncul bersamaan saat nasabah mengajukan complain. set.seed(100) words_cors %&gt;% filter(correlation &gt; .9) %&gt;% graph_from_data_frame() %&gt;% ggraph(layout = &quot;fr&quot;) + geom_edge_link(aes(edge_alpha = correlation)) + geom_node_point(color = &quot;lightblue&quot;, size = 5) + geom_node_text(aes(label = name), repel = TRUE) + theme_void() + ggtitle(&quot;Correlation between Words&quot;)+ theme(legend.position = &quot;none&quot;, plot.title = element_text(hjust = 0.5,face = &quot;bold&quot;)) Manfaat dari Sentiment Analysis yang telah dilakukan adalah kita dapat mengetahui pesan utama dari pendapat dan pemikiran customer terhadap suatu company atau product. Selain itu, output dari sentiment analysis dapat memberikan gambaran mengenai pelayanan atau product yang belum sesuai. Hal tersebut dapat membantu tim marketing untuk meneliti trend yang dibutuhkan customer dengan lebih baik. Seiring dengan peningkatan kualitas layanan dan pengembangan produk yang lebih baik, tentunya akan mengurangi tingkat churn customer. "],
["retail.html", "Chapter 4 Retail 4.1 E-Commerce Clothing Reviews", " Chapter 4 Retail 4.1 E-Commerce Clothing Reviews 4.1.1 Background Perkembangan teknologi membuat pergeseran perilaku customer dari pembelian offline menjadi pembelian online atau melalui e-commerce. Perbedaan utama saat berbelanja secara online atau offline adalah saat akan berbelanja secara online, calon customer tidak dapat memeriksa barang yang akan dibeli secara langsung dan biasanya dibantuk oleh gambar atau deskripsi yang diberikan oleh penjual. Tentunya customer akan mencari informasi mengenai produk yang akan dibeli untuk meminimalisir dampak negatif yang didapat. Untuk membantu customer dalam menentukan product yang akan dibeli, mayoritas e-commerce sekarang ini menyediakan fitur online customer review, dimana online customer review ini dijadikan sebagai salah satu media customer mendapatkan informasi tentang produk dari customer yang telah membeli produk tersebut. Meningkatnya e-commerce di Indonesia, kebutuhan analisa mengenai online customer review dirasa perlu dilakukan untuk mendukung agar customer dapat memiliki pengalaman belanja online yang lebih baik daripada belanja offline. Salah satu implementasi data review customer tersebut dapat dimanfaatkan untuk membuat model yang dapat memprediksi apakah product tersebut direkomendasikan atau tidak direkomendasikan. Harapannya setelah perusahaan dapat menilai product mana yang direkomendasikan dan yang tidak direkomendasikan, dapat membantu perusahaan dalam pertimbangan penentuan top seller. Untuk seller yang memiliki banyak product yang direkomendasikan, dapat dijadikan sebagai top seller. reviews &lt;- read.csv(&quot;assets/03- retail/Womens Clothing E-Commerce Reviews.csv&quot;) head(reviews) #&gt; X Clothing.ID Age Title #&gt; 1 0 767 33 #&gt; 2 1 1080 34 #&gt; 3 2 1077 60 Some major design flaws #&gt; 4 3 1049 50 My favorite buy! #&gt; 5 4 847 47 Flattering shirt #&gt; 6 5 1080 49 Not for the very petite #&gt; Review.Text #&gt; 1 Absolutely wonderful - silky and sexy and comfortable #&gt; 2 Love this dress! it&#39;s sooo pretty. i happened to find it in a store, and i&#39;m glad i did bc i never would have ordered it online bc it&#39;s petite. i bought a petite and am 5&#39;8&quot;. i love the length on me- hits just a little below the knee. would definitely be a true midi on someone who is truly petite. #&gt; 3 I had such high hopes for this dress and really wanted it to work for me. i initially ordered the petite small (my usual size) but i found this to be outrageously small. so small in fact that i could not zip it up! i reordered it in petite medium, which was just ok. overall, the top half was comfortable and fit nicely, but the bottom half had a very tight under layer and several somewhat cheap (net) over layers. imo, a major design flaw was the net over layer sewn directly into the zipper - it c #&gt; 4 I love, love, love this jumpsuit. it&#39;s fun, flirty, and fabulous! every time i wear it, i get nothing but great compliments! #&gt; 5 This shirt is very flattering to all due to the adjustable front tie. it is the perfect length to wear with leggings and it is sleeveless so it pairs well with any cardigan. love this shirt!!! #&gt; 6 I love tracy reese dresses, but this one is not for the very petite. i am just under 5 feet tall and usually wear a 0p in this brand. this dress was very pretty out of the package but its a lot of dress. the skirt is long and very full so it overwhelmed my small frame. not a stranger to alterations, shortening and narrowing the skirt would take away from the embellishment of the garment. i love the color and the idea of the style but it just did not work on me. i returned this dress. #&gt; Rating Recommended.IND Positive.Feedback.Count Division.Name #&gt; 1 4 1 0 Initmates #&gt; 2 5 1 4 General #&gt; 3 3 0 0 General #&gt; 4 5 1 0 General Petite #&gt; 5 5 1 6 General #&gt; 6 2 0 4 General #&gt; Department.Name Class.Name #&gt; 1 Intimate Intimates #&gt; 2 Dresses Dresses #&gt; 3 Dresses Dresses #&gt; 4 Bottoms Pants #&gt; 5 Tops Blouses #&gt; 6 Dresses Dresses Data yang digunakan merupakan data women e-commerce clothing reviews. Terdapat dua variabel yang menjadi fokus analisis ini yaitu Review.Text dan Recommended.IND. Variabel Review.Text merupakan review yang diberikan oleh customer terhadap product dari berbagai e-commerce, sedangkan Recommended.IND merupakan penilaian rekomendasi dari customer, 1 artinya product tersebut recommended dan 0 artinya product tersebut not recommended. Sebelum masuk cleaning data, kita ingin mengetahui proporsi dari target variabel: prop.table(table(reviews$Recommended.IND)) #&gt; #&gt; 0 1 #&gt; 0.1776377 0.8223623 4.1.2 Cleaning Data Untuk mengolah data text, kita perlu mengubah data teks dari vector menjadi corpus dengan function Vcorpus(). reviews_corpus &lt;- VCorpus(VectorSource(reviews$Review.Text)) reviews_corpus #&gt; &lt;&lt;VCorpus&gt;&gt; #&gt; Metadata: corpus specific: 0, document level (indexed): 0 #&gt; Content: documents: 23486 Selanjutnya, kita melakukan text cleansing dengan beberapa langkah sebagai berikut: tolower digunakan untuk mengubah semua karakter menjadi lowercase. removePunctuation digunakan untuk menghilangkan semua tanda baca. removeNumbers digunakan untuk menghilangkan semua angka. stopwords digunakan untuk menghilangkan kata-kata umum (am,and,or,if). stripWhitespace digunakan untuk menghapus karakter spasi yang berlebihan. data_clean &lt;- reviews_corpus %&gt;% tm_map(content_transformer(tolower)) %&gt;% tm_map(removePunctuation) %&gt;% tm_map(removeNumbers) %&gt;% tm_map(removeWords, stopwords(&quot;en&quot;)) %&gt;% tm_map(content_transformer(stripWhitespace)) inspect(data_clean[[1]]) #&gt; &lt;&lt;PlainTextDocument&gt;&gt; #&gt; Metadata: 7 #&gt; Content: chars: 43 #&gt; #&gt; absolutely wonderful silky sexy comfortable Setelah melakukan text cleansing, text tersebut akan diubah menjadi Document Term Matrix(DTM) melalui proses tokenization. Tokenization berfungsi memecah 1 teks atau kalimat menjadi beberapa term. Terim bisa berupa 1 kata, 2 kata, dan seterusnya. Pada format DTM, 1 kata akan menjadi 1 feature, secara default nilainya adalah jumlah kata pada dokumen tersebut. dtm_text &lt;- DocumentTermMatrix(data_clean) Sebelum membentuk model, tentunya kita perlu split data menjadi data train dan data test dengan proporsi 80:20. set.seed(100) idx &lt;- sample(nrow(dtm_text), nrow(dtm_text)*0.8) train &lt;- dtm_text[idx,] test &lt;- dtm_text[-idx,] train_label &lt;- reviews[idx,&quot;Recommended.IND&quot;] test_label &lt;- reviews[-idx,&quot;Recommended.IND&quot;] Term yang digunakan pada model ini, kita hanya mengambil term yang muncul paling sedikit 100 kali dari seluruh observasi dengan findFreqTerms(). freq &lt;- findFreqTerms(dtm_text, 100) train_r &lt;- train[, freq] test_r &lt;- test[, freq] inspect(train_r) #&gt; &lt;&lt;DocumentTermMatrix (documents: 18788, terms: 870)&gt;&gt; #&gt; Non-/sparse entries: 389603/15955957 #&gt; Sparsity : 98% #&gt; Maximal term length: 13 #&gt; Weighting : term frequency (tf) #&gt; Sample : #&gt; Terms #&gt; Docs dress fabric fit great just like love size top wear #&gt; 12348 0 1 0 1 0 1 0 1 0 1 #&gt; 12812 0 1 0 1 0 0 1 0 0 0 #&gt; 15905 0 1 2 1 0 1 0 2 0 1 #&gt; 1775 3 0 0 0 0 0 1 3 3 1 #&gt; 18527 0 1 0 1 1 0 0 0 2 0 #&gt; 19547 4 0 1 0 0 0 0 0 0 0 #&gt; 21091 0 0 0 0 0 1 0 2 0 1 #&gt; 22039 1 0 1 0 0 2 0 1 2 1 #&gt; 4789 1 0 2 0 0 1 0 1 0 1 #&gt; 6317 0 0 0 1 1 2 0 0 2 0 Nilai dari setiap matrix masih berupa angka numerik, dengan range 0-inf. Naive bayes akan memiliki performa lebih bagus ketika variabel numerik diubah menjadi kategorik. Salah satu caranya dengan Bernoulli Converter, yaitu jika jumlah kata yang muncul lebih dari 1, maka kita akan anggap nilainya adalah 1, jika 0 artinya tidak ada kata tersebut. bernoulli_conv &lt;- function(x){ x &lt;- as.factor(ifelse(x &gt; 0, 1, 0)) return(x) } train.bern &lt;- apply(train_r, MARGIN = 2, FUN = bernoulli_conv) test.bern &lt;- apply(test_r, MARGIN = 2, FUN = bernoulli_conv) 4.1.3 Modelling Selanjutnya, pembentukan model menggunakan naive bayes dan diikuti dengan prediksi data test. model.nb &lt;- naiveBayes(x = train.bern, y = as.factor(train_label), laplace = 1) pred.nb &lt;- predict(object = model.nb, newdata= test.bern) Dai hasil prediksi data test, kita akan menampilkan Confusion Matrix untuk mengetahui performa model. confusionMatrix(data = as.factor(pred.nb), reference = as.factor(test_label), positive = &quot;1&quot;) #&gt; Confusion Matrix and Statistics #&gt; #&gt; Reference #&gt; Prediction 0 1 #&gt; 0 614 365 #&gt; 1 253 3466 #&gt; #&gt; Accuracy : 0.8685 #&gt; 95% CI : (0.8585, 0.878) #&gt; No Information Rate : 0.8155 #&gt; P-Value [Acc &gt; NIR] : &lt; 0.00000000000000022 #&gt; #&gt; Kappa : 0.5837 #&gt; #&gt; Mcnemar&#39;s Test P-Value : 0.000008004 #&gt; #&gt; Sensitivity : 0.9047 #&gt; Specificity : 0.7082 #&gt; Pos Pred Value : 0.9320 #&gt; Neg Pred Value : 0.6272 #&gt; Prevalence : 0.8155 #&gt; Detection Rate : 0.7378 #&gt; Detection Prevalence : 0.7916 #&gt; Balanced Accuracy : 0.8065 #&gt; #&gt; &#39;Positive&#39; Class : 1 #&gt; 4.1.4 Visualize Data Text Selanjutnya, kita akan coba lakukan prediksi terhadap data test dan juga menampilkan visualisasi text tersebut menggunakan package lime. set.seed(100) idx &lt;- sample(nrow(reviews), nrow(reviews)*0.8) train_lime &lt;- reviews[idx,] test_lime &lt;- reviews[-idx,] tokenize_text &lt;- function(text){ #create corpus data_corpus &lt;- VCorpus(VectorSource(text)) # cleansing data_clean &lt;- data_corpus %&gt;% tm_map(content_transformer(tolower)) %&gt;% tm_map(removePunctuation) %&gt;% tm_map(removeNumbers) %&gt;% tm_map(removeWords, stopwords(&quot;en&quot;)) %&gt;% tm_map(content_transformer(stripWhitespace)) #dtm dtm_text &lt;- DocumentTermMatrix(data_clean) #convert to bernoulli data_text &lt;- apply(dtm_text, MARGIN = 2, FUN = bernoulli_conv) return(data_text) } model_type.naiveBayes &lt;- function(x){ return(&quot;classification&quot;) } predict_model.naiveBayes &lt;- function(x, newdata, type = &quot;raw&quot;) { # return classification probabilities only res &lt;- predict(x, newdata, type = &quot;raw&quot;) %&gt;% as.data.frame() return(res) } text_train &lt;- train_lime$Review.Text %&gt;% as.character() explainer &lt;- lime(text_train, model = model.nb, preprocess = tokenize_text) text_test &lt;- test_lime$Review.Text %&gt;% as.character() set.seed(100) explanation &lt;- explain(text_test[5:10], explainer = explainer, n_labels =1, n_features = 50, single_explanation = F) plot_text_explanations(explanation) Dari hasil output observasi kedua terprediksi product tersebut recommended dengan probability 96.31% dan nilai explainer fit menunjukkan seberapa baik LIME dalam menginterpretasikan prediksi untuk observasi ini sebesar 0.89 artinya dapat dikatakan cukup akurat. Teks berlabel biru menunjukkan kata tersebut meningkatkan kemungkinan product tersebut untuk direkomendasikan, sedangkan teks berlabel merah berarti bahwa kata tersebut bertentangan/mengurangi kemungkinan product tersebut untuk direkomendasikan. "],
["insurance.html", "Chapter 5 Insurance 5.1 Prediction of Total Claim Amount", " Chapter 5 Insurance body { text-align: justify} 5.1 Prediction of Total Claim Amount 5.1.1 Background Seiring tingkat kompetisi yang semakin tinggi di industri asuransi, perusahaan dituntut untuk selalu memberikan terobosan dan strategi untuk memberikan layanan yang terbaik untuk nasabahnya. Salah satu aset utama perusahaan asuransi tentunya adalah data nasabah dan riwayat polis. Tentunya dengan adanya data yang dimiliki oleh perusahaan, dapat dimanfaatkan dalam upaya pengambilan keputusan strategis. Perusahaan memiliki kebutuhan untuk memperhitungkan pembayaran klaim di masa depan. Tanggung jawab tersebut biasa dikenal sebagai cadangan klaim. Karena cadangkan klaim adalah kewajiban yang harus dipersiapkan untuk masa yang akan datang, nilai pastinya tidak diketahui dan harus diperkirakan. Risiko yang dimiliki oleh setiap nasabah tentunya bervariasi, faktor-faktor yang berhubungan dengan risiko tentunya membantu dalam memprediksi biaya klaim yang harus dibayarkan. Tujuan dari analysis ini adalah untuk memprediksi besarnya klaim yang harus diberikan oleh perusahaan untuk setiap nasabahnya, hasil prediksi diperoleh dengan mempelajari karakteristik dan profil dari nasabah tersebut. 5.1.2 Modelling Analysis 5.1.2.1 Import Data Data yang digunakan merupakan profil data nasabah asuransi kendaraan beserta total claim dari masing-masing nasabah yang diperoleh dari link berikut. Data tersebut berisikan 9134 observasi atau sebanyak jumlah nasabah yang dimiliki, beserta 26 kolom. Target variabel pada data ini adalah Total.Claim.Amount, kita akan memprediksi total claim amount untuk setiap nasabah, harapannya perusahaan asuransi dapat mengetahui dana yang harus disiapkan untuk membayar klaim. insurance &lt;- read.csv(&quot;assets/04-insurance/Auto_Insurance_Claims_Sample.csv&quot;) head(insurance) #&gt; Customer Country State.Code State Claim.Amount Response #&gt; 1 BU79786 US KS Kansas 276.3519 No #&gt; 2 QZ44356 US NE Nebraska 697.9536 No #&gt; 3 AI49188 US OK Oklahoma 1288.7432 No #&gt; 4 WW63253 US MO Missouri 764.5862 No #&gt; 5 HB64268 US KS Kansas 281.3693 No #&gt; 6 OC83172 US IA Iowa 825.6298 Yes #&gt; Coverage Education Effective.To.Date EmploymentStatus Gender #&gt; 1 Basic Bachelor 2/24/11 Employed F #&gt; 2 Extended Bachelor 1/31/11 Unemployed F #&gt; 3 Premium Bachelor 2/19/11 Employed F #&gt; 4 Basic Bachelor 1/20/11 Unemployed M #&gt; 5 Basic Bachelor 2/3/11 Employed M #&gt; 6 Basic Bachelor 1/25/11 Employed F #&gt; Income Location.Code Marital.Status Monthly.Premium.Auto #&gt; 1 56274 Suburban Married 69 #&gt; 2 0 Suburban Single 94 #&gt; 3 48767 Suburban Married 108 #&gt; 4 0 Suburban Married 106 #&gt; 5 43836 Rural Single 73 #&gt; 6 62902 Rural Married 69 #&gt; Months.Since.Last.Claim Months.Since.Policy.Inception #&gt; 1 32 5 #&gt; 2 13 42 #&gt; 3 18 38 #&gt; 4 18 65 #&gt; 5 12 44 #&gt; 6 14 94 #&gt; Number.of.Open.Complaints Number.of.Policies Policy.Type #&gt; 1 0 1 Corporate Auto #&gt; 2 0 8 Personal Auto #&gt; 3 0 2 Personal Auto #&gt; 4 0 7 Corporate Auto #&gt; 5 0 1 Personal Auto #&gt; 6 0 2 Personal Auto #&gt; Policy Claim.Reason Sales.Channel Total.Claim.Amount #&gt; 1 Corporate L3 Collision Agent 384.8111 #&gt; 2 Personal L3 Scratch/Dent Agent 1131.4649 #&gt; 3 Personal L3 Collision Agent 566.4722 #&gt; 4 Corporate L2 Collision Call Center 529.8813 #&gt; 5 Personal L1 Collision Agent 138.1309 #&gt; 6 Personal L3 Hail Web 159.3830 #&gt; Vehicle.Class Vehicle.Size #&gt; 1 Two-Door Car Medsize #&gt; 2 Four-Door Car Medsize #&gt; 3 Two-Door Car Medsize #&gt; 4 SUV Medsize #&gt; 5 Four-Door Car Medsize #&gt; 6 Two-Door Car Medsize 5.1.2.2 Exploratory Data Selanjutnya melihat structure data dari masing-masing variabel, jika terdapat variabel yang belum sesuai tipe datanya perlu dilakukan explicit coercion. str(insurance) #&gt; &#39;data.frame&#39;: 9134 obs. of 26 variables: #&gt; $ Customer : Factor w/ 9134 levels &quot;AA10041&quot;,&quot;AA11235&quot;,..: 601 5947 97 8017 2489 4948 8434 756 1352 548 ... #&gt; $ Country : Factor w/ 1 level &quot;US&quot;: 1 1 1 1 1 1 1 1 1 1 ... #&gt; $ State.Code : Factor w/ 5 levels &quot;IA&quot;,&quot;KS&quot;,&quot;MO&quot;,..: 2 4 5 3 2 1 1 4 1 1 ... #&gt; $ State : Factor w/ 5 levels &quot;Iowa&quot;,&quot;Kansas&quot;,..: 2 4 5 3 2 1 1 4 1 1 ... #&gt; $ Claim.Amount : num 276 698 1289 765 281 ... #&gt; $ Response : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 1 1 1 1 1 2 2 1 2 1 ... #&gt; $ Coverage : Factor w/ 3 levels &quot;Basic&quot;,&quot;Extended&quot;,..: 1 2 3 1 1 1 1 3 1 2 ... #&gt; $ Education : Factor w/ 5 levels &quot;Bachelor&quot;,&quot;College&quot;,..: 1 1 1 1 1 1 2 5 1 2 ... #&gt; $ Effective.To.Date : Factor w/ 59 levels &quot;1/1/11&quot;,&quot;1/10/11&quot;,..: 48 25 42 13 53 18 48 10 19 40 ... #&gt; $ EmploymentStatus : Factor w/ 5 levels &quot;Disabled&quot;,&quot;Employed&quot;,..: 2 5 2 5 2 2 2 5 3 2 ... #&gt; $ Gender : Factor w/ 2 levels &quot;F&quot;,&quot;M&quot;: 1 1 1 2 2 1 1 2 2 1 ... #&gt; $ Income : int 56274 0 48767 0 43836 62902 55350 0 14072 28812 ... #&gt; $ Location.Code : Factor w/ 3 levels &quot;Rural&quot;,&quot;Suburban&quot;,..: 2 2 2 2 1 1 2 3 2 3 ... #&gt; $ Marital.Status : Factor w/ 3 levels &quot;Divorced&quot;,&quot;Married&quot;,..: 2 3 2 2 3 2 2 3 1 2 ... #&gt; $ Monthly.Premium.Auto : int 69 94 108 106 73 69 67 101 71 93 ... #&gt; $ Months.Since.Last.Claim : int 32 13 18 18 12 14 0 0 13 17 ... #&gt; $ Months.Since.Policy.Inception: int 5 42 38 65 44 94 13 68 3 7 ... #&gt; $ Number.of.Open.Complaints : int 0 0 0 0 0 0 0 0 0 0 ... #&gt; $ Number.of.Policies : int 1 8 2 7 1 2 9 4 2 8 ... #&gt; $ Policy.Type : Factor w/ 3 levels &quot;Corporate Auto&quot;,..: 1 2 2 1 2 2 1 1 1 3 ... #&gt; $ Policy : Factor w/ 9 levels &quot;Corporate L1&quot;,..: 3 6 6 2 4 6 3 3 3 8 ... #&gt; $ Claim.Reason : Factor w/ 4 levels &quot;Collision&quot;,&quot;Hail&quot;,..: 1 4 1 1 1 2 1 1 1 2 ... #&gt; $ Sales.Channel : Factor w/ 4 levels &quot;Agent&quot;,&quot;Branch&quot;,..: 1 1 1 3 1 4 1 1 1 2 ... #&gt; $ Total.Claim.Amount : num 385 1131 566 530 138 ... #&gt; $ Vehicle.Class : Factor w/ 6 levels &quot;Four-Door Car&quot;,..: 6 1 6 5 1 6 1 1 1 1 ... #&gt; $ Vehicle.Size : Factor w/ 3 levels &quot;Large&quot;,&quot;Medsize&quot;,..: 2 2 2 2 2 2 2 2 2 2 ... Berikutnya kita perlu inspect persebaran data yang dimilih baik data kategorik dan numerik, kita dapat menggunakan package inspectdf untuk eksplorasi berikut ini. insurance %&gt;% inspect_cat() %&gt;% show_plot() insurance %&gt;% inspect_num() %&gt;% show_plot() Dari hasil kedua plot diatas berikutnya membuang variabel yang tidak dibutuhkan dalam model. Variabel customer merupakan data unique dari ID setiap customer, oleh karena itu kita akan membuang variabel tersebut. Variabel country tidak banyak memberikan informasi, karena semua observasi berisikan informasi yang sama. Variabel State.Code juga memberikan informasi yang sama dengan variabel State, oleh karena itu kita akan menggunakan salah satu dari kedua variabel tersebut yaitu variabel State. Sedangkan untuk variabel Policy kita hilangkan karena informasi yang diberikan juga sama dengan variabel Policy.Type. insurance &lt;- insurance %&gt;% select(-c(Customer, Country, State.Code, Effective.To.Date, Policy)) Selanjutnya, split data menjadi data train dan data test dengan proporsi 80:20. set.seed(100) idx &lt;- initial_split(data = insurance,prop = 0.8) claim_train &lt;- training(idx) claim_test &lt;- testing(idx) 5.1.2.3 Modelling Kemudian bentuk model random forest, tentukan target variabel dan prediktor yang digunakan. library(randomForest) forest_claim &lt;- randomForest(Total.Claim.Amount~.,data = claim_train, localImp = TRUE) #saveRDS(forest_claim,&quot;forest_claim.RDS&quot;) forest_claim &lt;- readRDS(&quot;assets/04-insurance/forest_claim.RDS&quot;) forest_claim #&gt; #&gt; Call: #&gt; randomForest(formula = Total.Claim.Amount ~ ., data = claim_train, localImp = TRUE) #&gt; Type of random forest: regression #&gt; Number of trees: 500 #&gt; No. of variables tried at each split: 6 #&gt; #&gt; Mean of squared residuals: 12764.93 #&gt; % Var explained: 84.8 Model memiliki kemampuan menjelaskan variasi data sebesar 84.8%, sedangkan sisanya sebesar 15.2% dijelaskan oleh variabel lain yang tidak digunakan pada model. Untuk mengetahui variabel yang paling berpengaruh pada model, kita dapat melihat variabel importance. varImpPlot(forest_claim, main = &quot;Variable Importance&quot;,n.var = 5) Nilai importance atau tingkat kepentingannya terdapat dua penilaian yaitu IncMSE dan IncNodePurity. Untuk IncMSE diperoleh dari error pada OOB (out of bag) data, kemudian di rata-ratakan untuk semua pohon, dan dinormalisasi dengan standar deviasi. Untuk IncNodePurity merupakan total penurunan impurity dari masing-masing variabel. Untuk kasus klasifikasi node impurity diperoleh dari nilai gini index, sedangkan untuk kasus regresi diperoleh dari SSE (Sum Square Error). Untuk mengetahui peran variabel dalam pembuatan model, kita dapat memanfaatkan package randomForestExplainer yang menyediakan beberapa function untuk memperoleh informasi mengenai variabel importance. mindepth_frame &lt;- min_depth_distribution(forest_claim) #saveRDS(mindepth_frame, &quot;mindepthframe.rds&quot;) mindepth_frame &lt;- readRDS(&quot;assets/04-insurance/mindepthframe.rds&quot;) plot_min_depth_distribution(mindepth_frame, mean_sample = &quot;top_trees&quot;) Plot tersebut memberikan informasi mengenai nilai mean minimal dept untuk setiap variabel. Semakin kecil nilai minimal depth artinya semakin penting variabel tersebut pada model. Semakin besar proporsi minimal dept pada warna merah mudah (mendekati 0), artinya variabel tersebut sering dijadikan sebagai root node, yaitu variabel utama yang digunakan untuk menentukan nilai target. imp_frame &lt;- measure_importance(forest_claim) #saveRDS(imp_frame,&quot;imp_frame.rds&quot;) imp_frame &lt;- readRDS(&quot;assets/04-insurance/imp_frame.rds&quot;) plot_multi_way_importance(imp_frame, size_measure = &quot;no_of_nodes&quot;,no_of_labels = 6) plot_multi_way_importance(imp_frame, x_measure = &quot;mse_increase&quot;, size_measure = &quot;p_value&quot;, no_of_labels = 6) Perbandingan dari ketiga plot, terdapat 5 variabel yaitu location code, monthly premium auto, vehicle class, income, dan claim amount yang selalu muncul dari ketiga plot tersebut. Artinya kelima variabel tersebut dapat dikatakan variabel yang paling berpengaruh dan banyak digunakan dalam pembuatan pohon. Berikutnya lakukan prediksi untuk data test, kemudian cari nilai error dari hasil prediksi claim_test$pred &lt;- predict(object = forest_claim,newdata = claim_test) Mencari nilai RMSE (Root Mean Squared Error) MLmetrics::RMSE(y_pred = claim_test$pred,y_true = claim_test$Total.Claim.Amount) #&gt; [1] 62.59351 RMSE merupakan nilai rata rata dari jumlah kuadrat error yang menyatakan ukuran besarnya kesalahan yang dihasilkan oleh model. Nilai RMSE rendah menunjukkan bahwa variasi nilai yang dihasilkan oleh model mendekasi variasi nilai observasinya. Jika dilihat dari 5 number summary variabel total claim amount, nilai RMSE yang diperoleh sebesar 119.9 dapat dikatakan sudah cukup baik. 5.1.3 Conclusion Untuk memprediksi nilai Total Claim Amount model ini memiliki kemampuan menjelaskan variasi data sebesar 84.8% dan variabel yang paling mempengaruhi target adalah variabel location code, monthly premium auto, vehicle class, income, dan claim amount. Hasil error yang diperoleh dari model tersebut cukup baik dalam memprediksi data. "],
["bioinformatics.html", "Chapter 6 Bioinformatics 6.1 QTL Mapping for Disease Resistance", " Chapter 6 Bioinformatics 6.1 QTL Mapping for Disease Resistance 6.1.1 Backgorund Ketahanan pangan merupakan salah satu prioritas utama dalam Rancangan Pembangunan Jangka Panjang Menengah Nasional. Ketersediaan pangan strategis sangat diandalkan dalam upaya mewujudkan ketahanan pangan. Pangan strategis dapat diartikan sebagai komoditas pangan yang terkait dengan kebutuhan sebagian besar masyarakat. Salah satu contoh komoditas pangan strategis menurut kementerian pertanian Indonesia adalah cabai. Sering kali petani cabai mengalami gagal panen dikarenakan serangan penyakit bakteri dan jamur pada akar dan daun. Salah satu penyakit yang dominan yang menyerang tanaman cabai adalah jamur Phytophthora capsici. Perlu dilakukan sebuah penelitian untuk mengetahui letak gen yang mempengaruhi sifat rentan terhadap jamur Phytophthora capsici. Beberapa hasil penelitian telah membuktikan analisis Quantitative Trait Locus sukses mengidentifikasi sifat pada tanaman. Maka dari itu sebagai bahan kajian peningkatan kualitas tanaman cabai, menggunakan data sekuen DNA yang sudah tersedia, dengan pendekatan metode QTL akan diidentifikasi letak gen yang berhubungan secara signifikan terhadap penyakit Phytophthora capsici. 6.1.2 Modelling Analysis 6.1.2.1 Import Data pacman::p_load(&quot;ASMap&quot;,&quot;qtlcharts&quot;,&quot;qtl&quot;, &quot;ggplot2&quot;,&quot;ggpubr&quot;,&quot;ggdendro&quot;, &quot;dendextend&quot;, &quot;factoextra&quot;, &quot;car&quot;, &quot;igraph&quot;) dataset &lt;- read.cross( format = &quot;csv&quot;, dir = &quot;assets/05-bioinformatics/&quot;, file =&quot;data.csv&quot;, sep = &quot;;&quot;, genotypes = c(&quot;a&quot;,&quot;h&quot;,&quot;b&quot;), alleles = c(&quot;a&quot;,&quot;b&quot;) ) #&gt; --Read the following data: #&gt; 296 individuals #&gt; 26 markers #&gt; 1 phenotypes #&gt; --Cross type: f2 summary(dataset) #&gt; F2 intercross #&gt; #&gt; No. individuals: 296 #&gt; #&gt; No. phenotypes: 1 #&gt; Percent phenotyped: 100 #&gt; #&gt; No. chromosomes: 1 #&gt; Autosomes: 5 #&gt; #&gt; Total markers: 26 #&gt; No. markers: 26 #&gt; Percent genotyped: 100 #&gt; Genotypes (%): aa:24.0 ab:48.4 bb:27.6 not bb:0.0 #&gt; not aa:0.0 Diatas merupakan hasil ringkasan dari data. Populasi yang digunakan hasil persilangan tipe F2 intercross yang menghasilkan individu sebanyak 296 jenis. Data fenotipe menjelaskan tentang skoring ketahanan tanaman cabai terhadap Phytophthora capsici. Interval skornya diantara 0 sampai dengan 5. Tanaman cabai yang memiliki resisten terhadap Phytophthora capsici akan diberi skor 0, sedangkan yang rentan akan diberi skor 5. Sebanyak 100% data fenotipe berhasil terbaca, artinya tidak ada data yang hilang (NA). Pada data ini hanya menggunakan 1 kromosom yang diamati, yaitu kromosom ke-5 dari total 11 kromosom cabai. ggplot(data = dataset$pheno, mapping = aes(x = dataset$pheno$Score)) + geom_density() + labs( title = &quot;Distribusi sebaran skor fenotipe&quot;, x = &quot;Skor ketahanan *Phytophthora capsici*&quot; ) + theme_minimal() shapiro.test(dataset$pheno$Score) #&gt; #&gt; Shapiro-Wilk normality test #&gt; #&gt; data: dataset$pheno$Score #&gt; W = 0.78902, p-value &lt; 0.00000000000000022 Berdasarkan kurva dan uji hipotesis, maka keputusannya data fenotipe tidak berdistribusi normal (distribusi bimodal). Menurut penelitian yang dilakukan Margawati (2015) ketika kurva fenotipe menunjukkan distribusi bimodal, maka itu merupakan indikasi terdapat gen mayor (yang signifikan mempengaruhi). 6.1.2.2 Exploratory Data Analysis cg &lt;- comparegeno(dataset) #dendogram clastering dataclust &lt;- abs(cg - 1) # Dissimilarity matrix df &lt;- scale(dataclust) d &lt;- dist(df, method = &quot;euclidean&quot;) hc3 &lt;- hclust(d, method = &quot;ward.D2&quot;) #phylogenetic tree dend_plot &lt;- fviz_dend( hc3, k = 4, # Cut in four groups cex = 0.5, # label size k_colors = &quot;jco&quot; ) # extract the dendrogram data dend_data &lt;- attr(dend_plot, &quot;dendrogram&quot;) # Cut the dendrogram at height h = 10 dend_cuts &lt;- cut(dend_data, h = 90) # Plot subtree 3 fviz_dend( dend_cuts$lower[[3]], main = &quot;Subtree 3&quot;, lwd = 1.3, ggtheme = theme_bw(), horiz = TRUE ) Berdasarkan hasil clusternig di atas, dapat diketahui individu cabai dengan nomor 173, 185, 202, 247, 251, 291, 14, dan 216 berada pada satu cluster yang sama. Hal ini masuk akal, karena individu tersebut berasal dari tetua yang sama tipenya, sehingga karakteristiknya hampir sama. Jika antar individu tidak memiliki kemiripan genotipe, maka akan terpisah jauh atau berada pada branch yang berbeda. 6.1.2.3 QTL Analysis Interval Mapping atau pemetaan interval menjadi pendekatan yang populer pada analisis QTL. Dalam interval mapping, masing-masing penanda sekuen akan dihitung nilai Logarithm of the Odds (LOD). Mudahnya, skor LOD adalah nilai statistik yang digunakan pada data genetika untuk mengukur apakah 2 gen atau lebih yang sedang diamati cenderung terletak berdekatan satu sama lain atau tidak. Skor LOD 3 atau lebih secara umum dapat dipahami bahwa 2 gen tersebut terletak berdekatan pada kromosom. # Marker regression dataset_rf &lt;- est.rf(dataset, maxit = 200, tol = 1e-8) out.mr &lt;- scanone(dataset_rf, method = &quot;mr&quot;) # Harley-knott regression datalink_1 &lt;- calc.genoprob(dataset_rf, step=1, error.prob=0.001, map.function = &quot;haldane&quot;) out.hk &lt;- scanone(datalink_1, method=&quot;hk&quot;) # Multiple Imputation set.seed(1997) datalink_2 &lt;- sim.geno(dataset_rf, step=1, error.prob=0.001) out.imp &lt;- scanone(datalink_2, method=&quot;imp&quot;) par(mfrow=c(1,1)) plot(out.imp, out.hk, out.mr, ylab=&quot;LOD Score&quot;, lty = c(1,1,2), col = c(&quot;black&quot;, &quot;blue&quot;, &quot;red&quot;), main = &quot;Perbandingan metode IMP, HK, EHK&quot;, lwd = 2.5, ylim = c(0,10)) legend(&quot;topleft&quot;, legend=c(&quot;Multiple Imputation&quot;, &quot;Harley-Knott Regression&quot;,&quot;Extented HK&quot;), col=c(&quot;black&quot;, &quot;blue&quot;, &quot;red&quot;), lty=c(1,1,2), cex=0.7, lwd = 2, title = &quot;Metode&quot;) Pemilihan motode terbaik berdasarkan skor panelized LOD yang tertinggi. Hasilnya, metode Imputation memperoleh LOD yang tertinggi yaitu 11.53. Selanjutnya, mencari formula regresi menggunakan metode imputation: dataqtl.step0 &lt;- sim.geno( cross = dataset_rf, step = 0, error.prob = 0.001, map.function = &quot;haldane&quot;, n.draws = 296 ) set.seed(1) outsw1 &lt;- stepwiseqtl(dataqtl.step0, verbose = TRUE, method = &quot;imp&quot;) outsw1 outsw1 &lt;- readRDS(file = &quot;assets/05-bioinformatics/outsw1.RDS&quot;) chr &lt;- c(5,5,5) pos &lt;- c(117.34, 159.31, 256.48) qtl &lt;- makeqtl(dataqtl.step0, chr, pos) my.formula &lt;- y ~ Q1 + Q2 + Q3 + Q1:Q2 out.fitqtl &lt;- fitqtl(dataqtl.step0, qtl=qtl, formula=my.formula, get.ests = F) summary(out.fitqtl) #&gt; #&gt; fitqtl summary #&gt; #&gt; Method: multiple imputation #&gt; Model: normal phenotype #&gt; Number of observations : 296 #&gt; #&gt; Full model result #&gt; ---------------------------------- #&gt; Model formula: y ~ Q1 + Q2 + Q3 + Q1:Q2 #&gt; #&gt; df SS MS LOD %var Pvalue(Chi2) #&gt; Model 10 367.1198 36.711980 24.81033 32.0229 0 #&gt; Error 285 779.3093 2.734418 #&gt; Total 295 1146.4291 #&gt; Pvalue(F) #&gt; Model 0 #&gt; Error #&gt; Total #&gt; #&gt; #&gt; Drop one QTL at a time ANOVA table: #&gt; ---------------------------------- #&gt; df Type III SS LOD %var F value Pvalue(Chi2) #&gt; 5@117.3 6 207.13 15.150 18.068 12.625 0.000 #&gt; 5@159.3 6 162.96 12.205 14.215 9.933 0.000 #&gt; 5@256.5 2 31.26 2.528 2.726 5.715 0.003 #&gt; 5@117.3:5@159.3 4 151.64 11.428 13.227 13.864 0.000 #&gt; Pvalue(F) #&gt; 5@117.3 0.00000000000125 *** #&gt; 5@159.3 0.00000000058772 *** #&gt; 5@256.5 0.00368 ** #&gt; 5@117.3:5@159.3 0.00000000024024 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Diperoleh formula skor ketahanan terhadap phytophthora capsici sebagai berikut: \\[Formula: y \\sim Q1 + Q2 + Q3 + Q1:Q2 \\] Keterangan: y = Skor fenotipe ketahanan terhadap penyakit phytophthora capsici Qi = Marka QTL ke-i Jika hasil summary model diatas diringkas kedalam bentuk tabel, maka informasinya seperti berikut: tibble( Variabel = c(&quot;Q1&quot;, &quot;Q2&quot;, &quot;Q3&quot;, &quot;Q1:Q2&quot;), `Kode Marka` = c(&quot;PMMCB81&quot;, &quot;PMMCB34&quot;, &quot;MCA32&quot;, &quot;PMMCB81 : PMMCB34&quot;), `% Var` = c(18.25, 13.36, 2.79, 12.62) ) #&gt; # A tibble: 4 x 3 #&gt; Variabel `Kode Marka` `% Var` #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 Q1 PMMCB81 18.2 #&gt; 2 Q2 PMMCB34 13.4 #&gt; 3 Q3 MCA32 2.79 #&gt; 4 Q1:Q2 PMMCB81 : PMMCB34 12.6 Kolom pertama dan kedua menjelaskan tentang simbol model QTL beserta nama markanya. Kolom persentase variansi (%var) adalah estimasi dari variansi fenotipe yang dijelaskan oleh marka PMMCB81, PMMCB34, MCA32, dan interaksi marka PMMCB81:PMMCB34. Total %var sebesar 46,3%. Mempunyai makna bahwa kemampuan seluruh marka dalam model untuk menjelaskan skor variansi fenotipe ketahanan tanaman cabai terhadap bakteri phytophthora capsici adalah sebesar 46,3%, sedangkan sisanya dijelaskan oleh marka lain diluar penelitian. Visualisasi peta genetik dengan model QTL yang signifikan dan hasi skor LOD metode multiple imputation disajikan pada gambar dibawah ini. par(mfrow=c(1,2)) plot(outsw1, col=&quot;red&quot;, justdots = F, show.marker.names = F) plot(out.imp$lod, out.imp$pos, col=&quot;red&quot;, xlab = &quot;LOD&quot;, ylim = c(315, 0),las = 1, ylab = &quot;Map Position (cM)&quot;, type = &quot;l&quot;, lwd = 3, main = &quot;Interval Mapping&quot;) # abline(v=tresh[4], lty = &quot;dotted&quot;, lwd=3, col=&quot;darkgrey&quot;) legend(&quot;bottomright&quot;, legend=&quot;Multiple Imputation&quot;, col=&quot;red&quot;, lty=1, cex=0.7, lwd = 2, title = &quot;Metode&quot;, bty = &quot;n&quot;) 6.1.3 Conclusion Hasil model QTL yang terbentuk dengan (\\(\\alpha\\) = 5%) formulanya y ~ Q1 + Q2 + Q3 + Q1:Q2 yang secara urut merupakan penanda sekuen (marka) dengan kode PMMCB81, PMMCB34, MCA32, dan interaksi PMMCB81 x PMMCB34. Skor LOD masing-masing model sebesar 5,80 (PMMCB81), 2,74 (PMMCB34) dan 8,6 (MCA32). Model QTL tersebut mampu menjelaskan 46,3% variansi skor fenotipe ketahanan tanaman cabai terhadap jamur phytophthora capsici. Rekomendasi yang dapat diberikan, perlu dilakukan investigasi lebih lanjut pada ketiga marka tersebut untuk memperbaiki atau mengembangkan kultivar tanaman cabai yang resisten terhadap penyakit layu yang disebabkan oleh jamur phytophthora capsici. "],
["public-health.html", "Chapter 7 Public Health 7.1 Survival Analysis of Patients with Lung Cancer", " Chapter 7 Public Health 7.1 Survival Analysis of Patients with Lung Cancer Kanker paru merupakan kanker pada organ pernapasan yang menjadi kanker pembunuh nomer satu di dunia dan Indonesia (CNN Indonesia, 2018). Data internasional dari Globocan 2018 menyatakan kanker paru adalah kanker yang paling banyak ditemukan di pria dan wanita di seluruh dunia dibandingkan jenis kanker lainnya. Pasien penderita kanker paru memerlukan penanganan yang terarah. Oleh karena itu akan dilakukan pengamatan, faktor apa saja yang mempengaruhi waktu ketahanan hidup pasien kanker paru. Metode yang digunakan adalah survival analysis, yaitu analisis statistik untuk mengambil keputusan yang berkaitan dengan waktu sampai dengan terjadinya suatu kejadian khusus (failure event/ end point). Pada bidang studi kanker, hal yang sering jadi perhatian peneliti adalah: Berapa probabilitas individu/pasien untuk survive selama 3 tahun? Apakah terdapat perbedaan kemampuan survive antara kelompok demografi pasien? 7.1.1 Import Data library(tidyverse) library(survival) library(SurvRegCensCov) library(survminer) options(scipen = 9999) Data yang digunakan merupakan data dummy rekam medis dari pasien kanker paru-paru. Sebanyak 137 pasien diobservasi dimana 128 mengalami event meninggal dan sisanya tersensor (dirujuk ke rumah sakit lain). Durasi waktu pengamatan menggukan satuan hari. lung &lt;- read.csv(&quot;assets/06-health/data-paru.csv&quot;, sep = &quot;;&quot;) glimpse(lung) #&gt; Observations: 137 #&gt; Variables: 6 #&gt; $ treatment &lt;int&gt; 2, 2, 2, 1, 1, 1, 2, 2, 1, 1, 2, 2, 1, 1, 1, 1... #&gt; $ survival &lt;int&gt; 1, 1, 2, 3, 4, 7, 7, 7, 8, 8, 8, 8, 10, 10, 11... #&gt; $ perform &lt;int&gt; 20, 50, 40, 30, 40, 50, 20, 40, 40, 20, 80, 50... #&gt; $ age &lt;int&gt; 65, 35, 44, 43, 35, 72, 66, 58, 63, 61, 68, 66... #&gt; $ status &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1... #&gt; $ cell &lt;int&gt; 4, 4, 3, 2, 3, 3, 3, 2, 4, 2, 3, 2, 4, 3, 4, 2... Berikut adalah penjelasan mengenai beberapa informasi yang diamati: treatment: 1 (standard), 2 (test) cell type: 1 (large), 2 (adeno), 3 (small), 4 (squamoues) survival: waktu pengamatan dalam hari status: 1 (cencored / berhasil survive), 0 (meninggal) Menurut Kementerian Kesehatan, kelompok usia yang paling berisiko tinggi mencakup pasien yang berusia &gt; 40 tahun. Maka, pada data akan dikelompokkan menjadi dua kelompok usia. lung &lt;- lung %&gt;% rename(time = survival) %&gt;% mutate( treatment = factor(treatment, levels = c(1,2), labels = c(&quot;standard&quot;, &quot;test&quot;)), cell = factor(cell, levels = c(1,2,3,4), labels = c(&quot;large&quot;, &quot;adeno&quot;,&quot;small&quot;,&quot;squamous&quot;)), age = case_when( age &gt; 40 ~ &quot;&gt;40&quot;, TRUE ~ &quot;&lt;=40&quot; ) ) 7.1.2 Exploratory Data Analysis Variabel prediktor (treatment, age, perform, cell) akan dianalisis menggunakan regresi survival. Dimana akan dilihat faktor-faktor apa saja yang mempengaruhi ketahanan hidup pasien sampai mengalami sebuah event: meninggal. Namun sebelumnya akan dianalisis menggunakan pendekatan non parametrik yaitu metode Kaplan Meier dan Log-Rank. Kaplan Meier adalah kurva yang menggambarkan hubungan antara waktu pengamatan (survival) dengan estimasi fungsi survival pada waktu ke-t. Kurva yang terbentuk kemudian dibandingkan menggunakan uji Log Rank. Tujuannya untuk mengetahui apakah terdapat perbedaan peluang survive antara level di setiap variabel kategorik. lung_surv &lt;- survfit(Surv(time = time, event = status) ~ 1, data = lung) tibble( time = lung_surv$time, n_risk = lung_surv$n.risk, n_event = lung_surv$n.event, survival = lung_surv$surv ) #&gt; # A tibble: 101 x 4 #&gt; time n_risk n_event survival #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 137 2 0.985 #&gt; 2 2 135 1 0.978 #&gt; 3 3 134 1 0.971 #&gt; 4 4 133 1 0.964 #&gt; 5 7 132 3 0.942 #&gt; 6 8 129 4 0.912 #&gt; 7 10 125 2 0.898 #&gt; 8 11 123 1 0.891 #&gt; 9 12 122 2 0.876 #&gt; 10 13 120 2 0.861 #&gt; # ... with 91 more rows Baris pertama output menyatakan pada waktu pengamatan hari pertama, ada 137 pasien, 2 diantaranya meninggal pada saat itu. Peluang survive diperoleh menggunakan perhitungan \\((137-2)/137 = 0.985\\). Pada garis kedua yang menyatakan observasi di hari ke-2, terdapat 135 pasien, dimana satu orang diantaranya meninggal pada saat itu. Peluang survivalnya diperoleh dengan perhitungan \\((135-1)/137 = 0.978\\). Tabel informasi diatas jika divisualisasikan tampilannya akan seperti berikut: ggsurvplot( lung_surv, color = &quot;#2E9FDF&quot;, ggtheme = ggthemes::theme_pander()) + labs(title = &quot;Kaplan-Meier Curves all variable&quot;) Hasil di atas adalah kurva survival untuk kesuluruhan parameter. Sumbu vertikal merupakan peluang survival dan sumbu horizontal adalah waktu pengamatan. Berdasarkan grafik, terlihat jelas bahwa makin jauh waktu pengamatan, peluang survive akan semakin kecil. Masing-masing variabel juga dapat dicari fungsi survivalnya, untuk memperoleh insight apakah tiap kelompok variabel terdapat perbedaan peluang survive yang signifikan. km_cell &lt;- survfit(Surv(time = time, event = status) ~ cell, data = lung) ggsurvplot( km_cell, ggtheme = ggthemes::theme_pander()) + labs(title = &quot;Kaplan-Meier Curves for Cell Type Group&quot;) Grafik diatas cukup menjelaskan bahwa keempat kelompok cell type memiliki perbedaan garis yang cukup signifikan. Maka, dapat diduga bahwa kelompok cell type pada data observasi memiliki perbedaan yang signifikan terhadap status survive pasien kanker paru. Akan dilakukan uji eksak, menggunakan Log-Rank untuk memperkuat identifikasi berdasarkan grafik. # log rank cell type lr_cell &lt;- survdiff(Surv(time ,status)~ cell, data = lung) lr_cell #&gt; Call: #&gt; survdiff(formula = Surv(time, status) ~ cell, data = lung) #&gt; #&gt; N Observed Expected (O-E)^2/E (O-E)^2/V #&gt; cell=large 27 26 34.5 2.12 3.02 #&gt; cell=adeno 27 26 15.7 6.77 8.19 #&gt; cell=small 48 45 30.1 7.37 10.20 #&gt; cell=squamous 35 31 47.7 5.82 10.53 #&gt; #&gt; Chisq= 25.4 on 3 degrees of freedom, p= 0.00001 Tingkat signifikansi yang digunakan adalah sebesar 5% (0.05). Berdasarkan uji Log-Rank, diperoleh p-value (0.0001) &lt; alpha (0.05) maka kesimpulannya terdapat perbedaan yang signifikan pada fungsi survival keempat kelompok cell. km_treatment &lt;- survfit(Surv(time = time, event = status) ~ treatment, data = lung) ggsurvplot( km_treatment, ggtheme = ggthemes::theme_pander()) + labs(title = &quot;Kaplan-Meier Curves for Treatment group&quot;) Pada hasil grafik di atas, antara pria dan wanita memiliki jarak yang berdekatan. Artinya tidak terdapat perbedaan yang signifikan untuk waktu survivalnya. Untuk memperkuat interpretasi berdasarkan grafik, perlu dilakukan uji hipotesis dengan menggunakan perhitungan eksak, yakni metode Log-Rank. # log rank treatment lr_treatment &lt;- survdiff(Surv(time ,status)~ treatment, data = lung) lr_treatment #&gt; Call: #&gt; survdiff(formula = Surv(time, status) ~ treatment, data = lung) #&gt; #&gt; N Observed Expected (O-E)^2/E (O-E)^2/V #&gt; treatment=standard 69 64 64.5 0.00388 0.00823 #&gt; treatment=test 68 64 63.5 0.00394 0.00823 #&gt; #&gt; Chisq= 0 on 1 degrees of freedom, p= 0.9 Berdasarkan hasil uji Log-Rank diperoleh p-value (0.9) &gt; alpha (0.05) sehingga kesimpulannya tidak terdapat perbedaan yang signifikan untuk kelompok variabel treatment. Artinya baik treament standard maupun treatment test, tidak mempengaruhi waktu survive dari pasien. Adapun variabel lainnya, kelompok usia dan kelompok performa setelah di uji menggunakan Log-Rank diperoleh kesimpulan yang sama yakni, terdapat perbedaan waktu survive yang signifikan untuk kelompok dua variabel tersebut. # log rank age lr_age &lt;- survdiff(Surv(time ,status)~ age, data = lung) lr_age #&gt; Call: #&gt; survdiff(formula = Surv(time, status) ~ age, data = lung) #&gt; #&gt; N Observed Expected (O-E)^2/E (O-E)^2/V #&gt; age=&lt;=40 12 11 7.44 1.703 1.86 #&gt; age=&gt;40 125 117 120.56 0.105 1.86 #&gt; #&gt; Chisq= 1.9 on 1 degrees of freedom, p= 0.2 # log rank perform lr_perform &lt;- survdiff(Surv(time ,status)~ perform, data = lung) lr_perform #&gt; Call: #&gt; survdiff(formula = Surv(time, status) ~ perform, data = lung) #&gt; #&gt; N Observed Expected (O-E)^2/E (O-E)^2/V #&gt; perform=10 1 1 0.463 0.6223 0.6334 #&gt; perform=20 7 7 1.037 34.2803 35.8740 #&gt; perform=30 14 14 4.957 16.4965 17.9562 #&gt; perform=40 16 15 7.686 6.9613 7.5756 #&gt; perform=50 14 13 12.155 0.0588 0.0668 #&gt; perform=60 27 26 25.341 0.0171 0.0217 #&gt; perform=70 23 21 29.832 2.6146 3.5106 #&gt; perform=75 2 2 1.522 0.1501 0.1540 #&gt; perform=80 24 22 25.989 0.6121 0.7983 #&gt; perform=85 1 1 0.845 0.0284 0.0290 #&gt; perform=90 7 6 17.460 7.5222 9.8316 #&gt; perform=99 1 0 0.713 0.7132 0.7274 #&gt; #&gt; Chisq= 78.2 on 11 degrees of freedom, p= 0.000000000003 7.1.3 Modelling Analysis Pada analisa sebelumnya, hanya dibandingkan tiap pengamatan pasien terhadap masing-masing variabelnya saja. Kali ini akan di uji apakah pasien kanker paru-paru memiliki tingkat ketahanan hidup (survive) yang berbeda berdasarkan penyebab tipe sel, performa, dan usia pasien. Pemodelan menggunakan pendekatan 2 metode, yaitu regresi weibull dan regresi log logistik. Masing-masing hasil ringkasan informasinya dapat dilihat pada output dibawah: regweibull &lt;- survreg(Surv(time, status) ~ age + cell + perform, data = lung, dist = &quot;weibull&quot;) summary(regweibull) #&gt; #&gt; Call: #&gt; survreg(formula = Surv(time, status) ~ age + cell + perform, #&gt; data = lung, dist = &quot;weibull&quot;) #&gt; Value Std. Error z p #&gt; (Intercept) 2.68750 0.44076 6.10 0.000000001078 #&gt; age&gt;40 0.51373 0.29711 1.73 0.0838 #&gt; celladeno -0.78682 0.26017 -3.02 0.0025 #&gt; cellsmall -0.42331 0.23926 -1.77 0.0769 #&gt; cellsquamous 0.29544 0.24794 1.19 0.2334 #&gt; perform 0.02945 0.00455 6.48 0.000000000092 #&gt; Log(scale) -0.07563 0.06617 -1.14 0.2531 #&gt; #&gt; Scale= 0.927 #&gt; #&gt; Weibull distribution #&gt; Loglik(model)= -715.2 Loglik(intercept only)= -748.1 #&gt; Chisq= 65.72 on 5 degrees of freedom, p= 0.0000000000008 #&gt; Number of Newton-Raphson Iterations: 5 #&gt; n= 137 reglog &lt;- survreg(Surv(time, status) ~ age + cell + perform, data = lung, dist = &quot;loglogistic&quot;) summary(reglog) #&gt; #&gt; Call: #&gt; survreg(formula = Surv(time, status) ~ age + cell + perform, #&gt; data = lung, dist = &quot;loglogistic&quot;) #&gt; Value Std. Error z p #&gt; (Intercept) 2.0251 0.4312 4.70 0.00000264074529785 #&gt; age&gt;40 0.5293 0.3245 1.63 0.1028 #&gt; celladeno -0.7763 0.2605 -2.98 0.0029 #&gt; cellsmall -0.7438 0.2446 -3.04 0.0024 #&gt; cellsquamous -0.0388 0.2656 -0.15 0.8839 #&gt; perform 0.0359 0.0044 8.16 0.00000000000000033 #&gt; Log(scale) -0.5509 0.0740 -7.44 0.00000000000009883 #&gt; #&gt; Scale= 0.576 #&gt; #&gt; Log logistic distribution #&gt; Loglik(model)= -711.3 Loglik(intercept only)= -750.3 #&gt; Chisq= 78.02 on 5 degrees of freedom, p= 0.0000000000000022 #&gt; Number of Newton-Raphson Iterations: 4 #&gt; n= 137 Kriteria yang digunakan dalam pemilihan model terbaik dilihat dari nilai Akaike Information Criterion (AIC). Model yang lebih layak digunakan ketika nilai AIC yang semakin rendah. AIC sangat berguna saat harus memilih model terbaik dari himpunan data yang sama. AIC yang diperoleh dari masing-masing metode: AIC(regweibull) #&gt; [1] 1444.466 AIC(reglog) #&gt; [1] 1436.513 Model terbaik diperoleh dari model regresi log logistic, dengan AIC paling terendah yaitu 1435,513: \\[S(t|x)=\\frac{1}{1 + (exp([-2.0251 + 0.5293_{age} - 0.7763_{cellAdeno} - 0.7438_{cellSmall} - 0.0388_{cellSquamous} + 0.0359_{perform})]*t)^{0.576}}\\] Jika dilakukan simulasi perhitungan peluang survive untuk dua pasien dengan karakteristik: Usia 20 tahun, cell Adeno, skor performa = 20; Usia 50 tahun, cell Squamous, skor performa = 70; pada saat hari ke-100, diperoleh hasil sebagai berikut: 1/(exp(-2.0251 + 0.5293 + 0.7438 + 0.0359 * 70)*100)^0.576 #&gt; [1] 0.02555521 # pasien 1 1/(exp(-2.0251 + 0.5293 + 0.7763 + 0.0359 * 70)*100)^0.576 #&gt; [1] 0.02508127 # pasien 2 1/(exp(-2.0251 + 0.5293 + 0.038 + 0.0359 * 70)*100)^0.576 #&gt; [1] 0.03837407 Maka, pasien usia lebih dari 40 tahub dengan tipe cell squamous pada saat 100 hari mengidap kanker paru peluang bertahan hidup (survive) untuk bertahan hidup lebih tinggi dibandingkan dengan tipe adino. 7.1.4 Conclusion Model regresi survival yang sesuai dengan data pengamatan adalah regresi log logistik. Faktor yang signifikan mempengaruhi laju ketahanan hidup pasien paru-paru berdasarkan data yang diamati, antara lain: usia, tipe sel, dan performa. Pasien dengan tipe sel adeno memiliki risiko paling tinggi dibandingkan lainnya. Dengan hasil pemodelan ini harapannya dapat dijadikan kajian awal untuk meningkatkan tingkat ketahanan hidup pasien paru-paru. "],
["media.html", "Chapter 8 Media 8.1 Causal Impact on Leads generation", " Chapter 8 Media 8.1 Causal Impact on Leads generation 8.1.1 Background Dalam proses bisnis, tim marketing mempunyai peran untuk meningkatkan brand awereness sebuah produk yang dijual. Ketika upaya menarik perhatian pelanggan untuk mencari tahu produk atau layanan yang disediakan sukses akan menghasilkan sebuah leads. Simpelnya, leads adalah orang-orang yang tertarik pada produk atau layanan bisnis. Di era digital, leads dapat diartikan sebagai orang yang mengunjungi website secara langsung maupun melalui iklan, orang yang melakukan like, share terhadap konten atau kampanye yang sedang dilakukan. Selanjutnya, prospek ketertarikan ini nantinya akan disimpan untuk kemudian diarahkan kepada tim sales. Banyak sekali upaya yang dapat dilakukan untuk menghasilkan sebuah leads. Mulai dari membuat konten kreatif, iklan, menulis artikel, membagikan ebook, kode prome dan lain sebagainya. Perlu dilakukan analisa seberapa efektif kampanye yang dilakukan untuk menghasilkan peningkatan leads. Causal Impact adalah sebuah analisis yang dapat digunakan untuk mencari kesimpulan secara statistik apakah ada perbedaan yang signifikan untuk lead generation dari periode sebelum kampanye dilakukan. Kesimpulan yang dapat diperoleh, apakah leads tersebut adalah hasil dari kampanye yang dilakukan, atau berasal dari faktor lain yang tidak teramati. 8.1.2 Modelling Analysis library(tidyverse) library(CausalImpact) library(readxl) library(forecast) library(TSstudio) Data berasal dari hasil googleanalytics sebuah website. Untuk pemodelan post-period, akan digunakan 37 hari sebelumnya sebagai data training. Pertanyaan bisnisnya yaitu pada hari ke-38 dan seterusnya, setelah kampanye dilakukan, apakah memperoleh peningkatan leads yang signifikan? data &lt;- read_csv(&quot;assets/09-media/analytics.csv&quot;) glimpse(data) #&gt; Observations: 54 #&gt; Variables: 9 #&gt; $ page &lt;chr&gt; &quot;9fb22be32c347a5acd1d3724b0dae726&quot;, &quot;9... #&gt; $ datetime &lt;dbl&gt; 20180516, 20180517, 20180518, 20180519... #&gt; $ page_display &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,... #&gt; $ unique_views &lt;dbl&gt; 1456, 1392, 1281, 658, 616, 1691, 1488... #&gt; $ average_page_time &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,... #&gt; $ tickets &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,... #&gt; $ bounce_rate &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,... #&gt; $ exit_percentage &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,... #&gt; $ page_value &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,... Kita pilih beberapa kolom yang menjadi fokus analisis ini, yaitu datetime (harian) dan unique views yang berisi informasi berapa banyak orang yang mengunjungi halaman website tersebut. actual &lt;- data %&gt;% mutate( datetime = lubridate::as_datetime(as.character(datetime)) ) %&gt;% dplyr::select(datetime, unique_views) %&gt;% na.omit() head(actual) #&gt; # A tibble: 6 x 2 #&gt; datetime unique_views #&gt; &lt;dttm&gt; &lt;dbl&gt; #&gt; 1 2018-05-16 00:00:00 1456 #&gt; 2 2018-05-17 00:00:00 1392 #&gt; 3 2018-05-18 00:00:00 1281 #&gt; 4 2018-05-19 00:00:00 658 #&gt; 5 2018-05-20 00:00:00 616 #&gt; 6 2018-05-21 00:00:00 1691 Leads generation yang dihasilkan paling tinggi ketika hari Rabu. Informasi tersebut dapat dijadikan pertimbangan penentuan hari untuk memulai kampanye. actual %&gt;% mutate( wdays = lubridate::wday(datetime, label = TRUE) ) %&gt;% group_by(wdays) %&gt;% summarise(total_views = sum(unique_views)) %&gt;% ungroup() %&gt;% mutate( label = scales::comma(total_views) ) %&gt;% ggplot( mapping = aes(x = wdays, y = total_views) ) + geom_col(fill = &quot;steelblue&quot;) + labs( title = &quot;Total Views Per Days&quot;, subtitle = &quot;Period: May to July&quot;, y = NULL, x = &quot;Day of Week&quot; ) + theme_minimal() + geom_text( aes(label = label, y = total_views + max(total_views) * 0.075) , size = 3 ) Seperti yang dijelaskan sebelumnya, akan di-subset 37 hari sebelum kampanye diberikan dan disimpan ke objek pre_campaign. Dari sinilah, kita dapat melihat pergerakan leads dan memperikirakan peningkatan yang dihasilkan sejak kampanye dilakukan. pre_campaign &lt;- actual %&gt;% slice(1:37) Selanjutnya, membuat objek time series dan melakukan pemodelan untuk menjadikan banchmark dari leads yang dapat kita peroleh jika tidak menggunakan kampanye. Kita akan gunakan Holtwinter sebagai metode untuk melakuan peramalan (forecast) 16 hari kedepan. ts_campaign &lt;- ts(pre_campaign$unique_views, frequency = 7) fit_hw &lt;- HoltWinters(ts_campaign) forecast &lt;- forecast(fit_hw, 16) Kita gabungkan data periode sebelum dilakukan kampanye dan hasil ramalannya yang disimpan ke objek append_data. forecast_data &lt;- data.frame( datetime = lubridate::as_datetime(seq.Date(from = as.Date(&quot;2018-06-24&quot;), by = &quot;day&quot;, length.out = 16)), unique_views = forecast$mean ) append_data &lt;- pre_campaign %&gt;% bind_rows(forecast_data) ggplot(data = append_data, mapping = aes(x = datetime, y = unique_views)) + geom_line(color = &quot;steelblue&quot;, size = 1) + geom_point() + labs( title = &quot;Forecast Projection&quot;, y = &quot;Total Unique Views&quot; ) + theme_minimal() Dan kita juga mempunyai data aktual untuk periode tersebut. Kita perhatikan terjadi peningkatan total pengunjung website. actual %&gt;% ggplot(mapping = aes(x = datetime, y = unique_views)) + geom_line(color = &quot;steelblue&quot;, size = 1) + geom_point() + labs( title = &quot;Data aktual jumlah pengunjung website&quot;, subtitle = &quot;Periode 16 Mei hingga 17 Juli&quot;, y = NULL ) + theme_minimal() Untuk memperkirakan efek kausal, kita mulai dengan menentukan periode mana dalam data yang harus digunakan untuk melatih model (periode pra-intervensi) dan periode mana untuk menghitung prediksi kontrafaktual (periode pasca intervensi). pre &lt;- c(1,37) post &lt;- c(38, 53) Sintaks diatas berarti, poin observasi ke-1 sampai dengan 37 akan digunakan untuk training, dan poin observasi ke 38 hingga 53 untuk menghitung prediksi, atau kita juga bisa mendefinisikannya ke format interval tanggal (date). Kemudian ubah datanya menjadi format matriks sebagai syarat analisis dengan packages CausalImpact. pre &lt;- as.Date(c(&quot;2018-05-16&quot;, &quot;2018-06-24&quot;)) post &lt;- as.Date(c(&quot;2018-06-25&quot;, &quot;2018-07-10&quot;)) time.points &lt;- seq.Date(as.Date(&quot;2018-05-16&quot;), by = &quot;days&quot;, length.out = 53) data_ci &lt;- zoo( cbind(actual$unique_views, append_data$unique_views), time.points ) Sekarang kita sudah memiliki data yang siap untuk memverifikasi efek kausal dari kampanye. impact &lt;- CausalImpact(data = data_ci, pre.period = pre, post.period = post) plot(impact) Secara default, plot berisi dari tiga panel. Panel pertama original menunjukkan data dan prediksi kontrafaktual untuk periode pasca kampanye. Panel kedua pointwise menunjukkan perbedaan antara data aktual yang diamati (leads) dan prediksi. Panel ketiga cumulative menggambarkan efek kumulatif dari intervensi (kampanye) yang dilakukan. Hasil ini memiliki asumsi bahwa hubungan antara leads generation dan deret waktu pengamatan, sebagaimana ditetapkan selama pre-period, tetap stabil sepanjang post-period. Kita dapat lihat informasi statistiknya dengan mengunnakan perintah summary(impact). summary(impact) #&gt; Posterior inference {CausalImpact} #&gt; #&gt; Average Cumulative #&gt; Actual 1417 18424 #&gt; Prediction (s.d.) 984 (54) 12786 (697) #&gt; 95% CI [880, 1095] [11437, 14232] #&gt; #&gt; Absolute effect (s.d.) 434 (54) 5638 (697) #&gt; 95% CI [322, 537] [4192, 6987] #&gt; #&gt; Relative effect (s.d.) 44% (5.5%) 44% (5.5%) #&gt; 95% CI [33%, 55%] [33%, 55%] #&gt; #&gt; Posterior tail-area probability p: 0.00103 #&gt; Posterior prob. of a causal effect: 99.89669% #&gt; #&gt; For more details, type: summary(impact, &quot;report&quot;) Kita dapat memperoleh informasi dari actual dan predicted effect (average) serta efek absolut dan relatifnya. Output informasi statistik di atas mengatakan, leads generation setelah dilakukan kampanye mengalami peningkatan sebesar 44%, dari perkiraan rata-rata pengunjung websitenya sebanyak 984 orang menjadi 1417 kenyataannya. Untuk panduan interpretasi yang benar dari hasil tabel ringkasan, packages CausalImpact menyediakan teks interpretasinya, yang dapat kita print menggunakan perintah: interpretasi &lt;- summary(impact, &quot;report&quot;) print(interpretasi) Hasilnya interpretasi teksnya akan seperti berikut: Analysis report {CausalImpact} During the post-intervention period, the response variable had an average value of approx. 1.42K. By contrast, in the absence of an intervention, we would have expected an average response of 0.98K. The 95% interval of this counterfactual prediction is [0.87K, 1.09K]. Subtracting this prediction from the observed response yields an estimate of the causal effect the intervention had on the response variable. This effect is 0.43K with a 95% interval of [0.33K, 0.54K]. For a discussion of the significance of this effect, see below. Summing up the individual data points during the post-intervention period (which can only sometimes be meaningfully interpreted), the response variable had an overall value of 18.42K. By contrast, had the intervention not taken place, we would have expected a sum of 12.79K. The 95% interval of this prediction is [11.35K, 14.19K]. The above results are given in terms of absolute numbers. In relative terms, the response variable showed an increase of +44%. The 95% interval of this percentage is [+33%, +55%]. This means that the positive effect observed during the intervention period is statistically significant and unlikely to be due to random fluctuations. It should be noted, however, that the question of whether this increase also bears substantive significance can only be answered by comparing the absolute effect (0.43K) to the original goal of the underlying intervention. The probability of obtaining this effect by chance is very small (Bayesian one-sided tail-area probability p = 0.001). This means the causal effect can be considered statistically significant. "]
]
