[
["index.html", "Machine Learning Application in Industry Chapter 1 Introduction", " Machine Learning Application in Industry Algoritma Team 28, April 2021 Chapter 1 Introduction Bookdown berikut ini disusun oleh Tim Algoritma yang bertujuan untuk memberikan beberapa contoh use case penerapan Machine Learning dalam beberapa industry. Algoritma adalah Data Science Academy yang berlokasi di Jakarta. Algoritma menyediakan beberapa workshop dan training program untuk membantu student ataupun profesional dalam penguasaan berbagai sub-bidang ilmu data meliputi : Data Visualization, Machine Learning, Data Modelling, Statistical Inference, dan lain-lain. "],
["telecommunication.html", "Chapter 2 Telecommunication 2.1 Customer Churn Prediction", " Chapter 2 Telecommunication 2.1 Customer Churn Prediction 2.1.1 Background Customer Churn didefinisikan sebagai kecenderungan pelanggan untuk berhenti melakukan interaksi dengan sebuah perusahaan. Perusahaan telekomunikasi memiliki kebutuhan untuk mengetahui customer yang akan berhenti berlangganan atau tidak, karena biaya mempertahankan pelanggan yang sudah ada jauh lebih sedikit dibandingkan memperoleh pelanggan baru. Perusahaan biasanya mendefinisikan 2 tipe customer churn, yaitu voluntary churn dan involuntary churn. Voluntary churn merupakan pelanggan yang sengaja berhenti dan beralih ke perusahaan lain, sedangkan involuntary churn merupakan pelanggan yang berhenti karena perpindahan lokasi, kematian, atau alasan lain yang sulit dikontrol. Analisis voluntary churn tentunya tidak sulit untuk mempelajari karakteristik pelanggan yang dapat dilihat dari data profil pelanggan. Permasalah diatas dapat dijawab dengan membuat model prediksi customer churn. Harapannya dengan adanya model prediksi customer churn, dapat mempermudah pihak perusahaan telekomunikasi untuk memperoleh informasi mengenai pelanggan yang berpeluang besar untuk churn. 2.1.2 Modelling Analysis 2.1.2.1 Import Data Data yang digunakan merupakan data profil pelanggan perusahaan telekomunikasi yang diperoleh dar link berikut. Data tersebut berisikan 7043 observasi dengan 21 kolom. Target variabel pada data ini adalah Churn, kita akan memprediksi apakah pelanggan akan berhenti berlangganan produk atau akan tetep berlangganan. customer &lt;- read.csv(&quot;assets/01-telco/WA_Fn-UseC_-Telco-Customer-Churn.csv&quot;) head(customer) #&gt; customerID gender SeniorCitizen Partner Dependents tenure PhoneService MultipleLines #&gt; 1 7590-VHVEG Female 0 Yes No 1 No No phone service #&gt; 2 5575-GNVDE Male 0 No No 34 Yes No #&gt; 3 3668-QPYBK Male 0 No No 2 Yes No #&gt; 4 7795-CFOCW Male 0 No No 45 No No phone service #&gt; 5 9237-HQITU Female 0 No No 2 Yes No #&gt; 6 9305-CDSKC Female 0 No No 8 Yes Yes #&gt; InternetService OnlineSecurity OnlineBackup DeviceProtection TechSupport StreamingTV #&gt; 1 DSL No Yes No No No #&gt; 2 DSL Yes No Yes No No #&gt; 3 DSL Yes Yes No No No #&gt; 4 DSL Yes No Yes Yes No #&gt; 5 Fiber optic No No No No No #&gt; 6 Fiber optic No No Yes No Yes #&gt; StreamingMovies Contract PaperlessBilling PaymentMethod MonthlyCharges #&gt; 1 No Month-to-month Yes Electronic check 29.85 #&gt; 2 No One year No Mailed check 56.95 #&gt; 3 No Month-to-month Yes Mailed check 53.85 #&gt; 4 No One year No Bank transfer (automatic) 42.30 #&gt; 5 No Month-to-month Yes Electronic check 70.70 #&gt; 6 Yes Month-to-month Yes Electronic check 99.65 #&gt; TotalCharges Churn #&gt; 1 29.85 No #&gt; 2 1889.50 No #&gt; 3 108.15 Yes #&gt; 4 1840.75 No #&gt; 5 151.65 Yes #&gt; 6 820.50 Yes Berikut ini merupakan deskripsi untuk setiap variabel: CustomerID: Customer ID Gender: Gender pelanggan yaitu Female dan Male SeniorCitizen: Apakah pelanggan merupakan senio citizen (0: No, 1: Yes) Partner: Apakah pelanggan memiliki partner atau tidak (Yes, No) Dependents: Apakah pelanggan memiliki tanggungan atau tidak (Yes, No) Tenure: Jumlah bulan dalam menggunakan produk perusahaan MultipleLines: Apakah pelanggan memiliki banyak saluran atau tidak (Yes, No, No phone service) OnlineSecurity: Apakah pelanggan memiliki keamanan online atau tidak OnlineBackup: Apakah pelanggan memiliki cadangan online atau tidak DeviceProtection: Apakah pelanggan memiliki perlindungan perangkat atau tidak TechSupport: Apakah pelanggan memiliki dukungan teknis atau tidak StreamingTV: Apakah pelanggan berlangganan TV streaming atau tidak StreamingMovies: Apakah pelanggan berlangganan movies streaming atau tidak Contract: Ketentuan kontrak berlangganan (Month-to-month, One year, Two year) PaperlessBilling: Apakah pelanggan memiliki tagihan tanpa kertas atau tidak (Yes, No) PaymentMethod: Metode pembayaran (Electronic check, Mailed check, Bank transfer (automatic), Credit card (automatic)) MonthlyCharges: Jumlah pembayaran yang dilakukan setiap bulan TotalCharges: Jumlah total yang dibebankan oleh pelanggan Churn: Apakah pelanggan Churn atau tidak (Yes or No) 2.1.2.2 Exploratory Data Sebelum eksplorasi lebih lanjut, perlu diketahui kelengkapan data yang dimiliki: colSums(is.na(customer)) #&gt; customerID gender SeniorCitizen Partner Dependents #&gt; 0 0 0 0 0 #&gt; tenure PhoneService MultipleLines InternetService OnlineSecurity #&gt; 0 0 0 0 0 #&gt; OnlineBackup DeviceProtection TechSupport StreamingTV StreamingMovies #&gt; 0 0 0 0 0 #&gt; Contract PaperlessBilling PaymentMethod MonthlyCharges TotalCharges #&gt; 0 0 0 0 11 #&gt; Churn #&gt; 0 Dari 7043 observasi ternyata terdapat missing values sebanyak 11 observasi pada kolom TotalCharges. Karena jumlah missing values cukup sedikit kita dapat membuat observasi tersebut. Selain itu, perlu kita buang variabel yang tidak dibutuhkan pada pemodelan yaitu customerID dan juga sesuaikan tipe data yang seharusnya. customer &lt;- customer %&gt;% dplyr::select(-customerID) %&gt;% na.omit() %&gt;% mutate(SeniorCitizen = as.factor(SeniorCitizen)) Untuk mengetahui proporsi kelas pada setiap variable kategori, kita dapat menggunakan function inspect_cat dari package inspectdf seperti berikut: customer %&gt;% inspect_cat() %&gt;% show_plot() Dari hasil plot diatas dapat diketahui proporsi kelas untuk target variabel cenderung lebih banyak dikategori No namun masih seimbang. Sedangkan untuk variabel lainnya untuk proporsi setiap level nya mayoritas seimbang. Berikutnya kita dapat eksplorasi persebaran untuk variabel data numerik dengan function inspect_num dari package inspectdf seperti berikut: customer %&gt;% inspect_num() %&gt;% show_plot() Dari ketiga variabel numerik yang dimiliki, persebaran data cukup beragam untuk setiap nilai. 2.1.2.3 Modelling Sebelum masuk ke tahap modelling, kita perlu membagi data menjadi data_train dan data_test dengan proporsi 80:20. set.seed(100) idx &lt;- initial_split(data = customer,prop = 0.8,strata = Churn) data_train &lt;- training(idx) data_test &lt;- testing(idx) Berikutnya bentuk model random forest menggunakan package caret, tentukan banyaknya cross validation dan repetition pada model dan juga target variabel dan prediktor yang digunakan. set.seed(100) ctrl &lt;- trainControl(method=&quot;repeatedcv&quot;, number=5, repeats=3) # model_forest &lt;- train(Churn ~ ., data=data_train, method=&quot;rf&quot;, trControl = ctrl) import model yang sudah dijalankan pada chunk sebelumnya menggunakan readRDS. #saveRDS(model_forest,&quot;assets/01-telco/model_forest.rds&quot;) model_forest &lt;- readRDS(&quot;assets/01-telco/model_forest.rds&quot;) model_forest #&gt; Random Forest #&gt; #&gt; 5627 samples #&gt; 19 predictor #&gt; 2 classes: &#39;No&#39;, &#39;Yes&#39; #&gt; #&gt; No pre-processing #&gt; Resampling: Cross-Validated (5 fold, repeated 3 times) #&gt; Summary of sample sizes: 4501, 4502, 4501, 4502, 4502, 4501, ... #&gt; Resampling results across tuning parameters: #&gt; #&gt; mtry Accuracy Kappa #&gt; 2 0.7837817 0.3252122 #&gt; 16 0.7750746 0.3779712 #&gt; 30 0.7731203 0.3727503 #&gt; #&gt; Accuracy was used to select the optimal model using the largest value. #&gt; The final value used for the model was mtry = 2. Dari hasil yang diperoleh pada model_forest, didapatkan accuraci sebesar 0.78 dengan mtry sebanyak 2. Selanjutnya, akan dilakukan tuning model dengan melakukan upsample data. Artinya, kita akan membuat proporsi dari target variabel sama besar. up_train &lt;- upSample(x = data_train[,-20], y = data_train$Churn, yname = &quot;Churn&quot;) Dilakukan pembuat model random forest dengan data upsample: set.seed(100) # ctrl &lt;- trainControl(method=&quot;repeatedcv&quot;, number=5, repeats=3) # forest_upc &lt;- train(Churn ~ ., data=up_train, method=&quot;rf&quot;, trControl = ctrl) #saveRDS(forest_upc,&quot;assets/01-telco/model_caret.rds&quot;) forest_upc &lt;- readRDS(&quot;assets/01-telco/model_caret.rds&quot;) Dari hasil model kedua diperoleh hasil sebagai berikut: forest_upc #&gt; Random Forest #&gt; #&gt; 8262 samples #&gt; 19 predictor #&gt; 2 classes: &#39;No&#39;, &#39;Yes&#39; #&gt; #&gt; No pre-processing #&gt; Resampling: Cross-Validated (5 fold, repeated 3 times) #&gt; Summary of sample sizes: 6609, 6610, 6609, 6610, 6610, 6610, ... #&gt; Resampling results across tuning parameters: #&gt; #&gt; mtry Accuracy Kappa #&gt; 2 0.7760017 0.5520022 #&gt; 16 0.8911472 0.7822945 #&gt; 30 0.8875167 0.7750336 #&gt; #&gt; Accuracy was used to select the optimal model using the largest value. #&gt; The final value used for the model was mtry = 16. Setelah dilakukan upsample data, terlihat nilai accuracy yang diperoleh lebih besar dibandingkan model sebelumnya sebesar 0.89 dengan mtry sebanyak 16. Selanjutnya, akan dilakukan prediksi terhadap data_test: pred &lt;- predict(forest_upc,newdata = data_test,type = &quot;prob&quot;) pred$result &lt;- as.factor(ifelse(pred$Yes &gt; 0.45, &quot;Yes&quot;,&quot;No&quot;)) confusionMatrix(pred$result, as.factor(data_test$Churn),positive = &quot;Yes&quot;) #&gt; Confusion Matrix and Statistics #&gt; #&gt; Reference #&gt; Prediction No Yes #&gt; No 849 109 #&gt; Yes 183 264 #&gt; #&gt; Accuracy : 0.7922 #&gt; 95% CI : (0.77, 0.8131) #&gt; No Information Rate : 0.7345 #&gt; P-Value [Acc &gt; NIR] : 0.00000031 #&gt; #&gt; Kappa : 0.4989 #&gt; #&gt; Mcnemar&#39;s Test P-Value : 0.00001937 #&gt; #&gt; Sensitivity : 0.7078 #&gt; Specificity : 0.8227 #&gt; Pos Pred Value : 0.5906 #&gt; Neg Pred Value : 0.8862 #&gt; Prevalence : 0.2655 #&gt; Detection Rate : 0.1879 #&gt; Detection Prevalence : 0.3181 #&gt; Balanced Accuracy : 0.7652 #&gt; #&gt; &#39;Positive&#39; Class : Yes #&gt; Pada kasus ini kita ingin memperoleh nila sensitivity/recall yang lebih besar, dengan menggunakan threshold sebesar 0.4 diperoleh nilai recall sebesar 0.70 dengan accuracy sebesar 0.79 dan precision sebesar 0.59. Dari model yang telah terbentuk kita dapat memperoleh nilai AUC pada model: library(ROCR) pred_prob &lt;- predict(object = forest_upc,newdata = data_test,type = &quot;prob&quot;) pred &lt;- prediction(pred_prob[,2],labels = data_test$Churn) perf &lt;- performance(prediction.obj = pred,measure = &quot;tpr&quot;,x.measure = &quot;fpr&quot;) plot(perf) auc &lt;- performance(pred,measure = &quot;auc&quot;) auc@y.values[[1]] #&gt; [1] 0.8513259 2.1.3 Conclusion library(lime) test_x &lt;- data_test %&gt;% dplyr::select(-Churn) explainer &lt;- lime(test_x, forest_upc) explanation &lt;- lime::explain(test_x[1:2,], explainer, labels = c(&quot;Yes&quot;), n_features = 8) plot_features(explanation) Setelah adanya model prediksi customer churn, pihak perusahaan telekomunikasi dapat dengan mudah mengetahui pelanggan yang memiliki kecendurungan akan churn. Kedua plot diatas memperlihatkan prediksi dua customer, kedua customer memiliki peluang besar untuk churn dan kita dapat mengetahui variabel mana yang supports dan contradicts terhadap hasil prediksi. "],
["finance.html", "Chapter 3 Finance 3.1 Credit Risk Analysis 3.2 Evaluating Customer Financial Complaints", " Chapter 3 Finance 3.1 Credit Risk Analysis 3.1.1 Background Credit scoring merupakan sistem yang digunakan oleh bank atau lembaga keuangan lain untuk menentukan apakah seorang nasabah layak atau tidak mendapatkan pinjaman. Credit scoring membutuhkan berbagai data profil calon peminjam sehingga tingkat resiko dapat dihitung dengan tepat. Semakin tepat dan lengkap data yang disediakan, maka semakin akurat perhitungan yang dilakukan. Proses tersebut tentunya merupakan hal yang baik, namun di sisi calon peminjam proses yang harus dilalui dirasa sangat merepotkan dan membutuhkan waktu untuk menunggu dan seiring tingginya tingkat kompetisi yang ada di industri finansial, menjadikan nasabah memiliki banyak alternatif. Semakin cepat proses yang ditawarkan, semakin tinggi kesempatan untuk mendapatkan peminjam. Tantangan pun muncul, bagaimana mendapatkan peminjam dengan proses yang efisien namun akurasi dari credit scoring tetap tinggi. Disinilah machine learning dapat membantu menganalisa data-data profil peminjam dan proses pembayaran sehingga dapat mengetahui profil peminjam yang memiliki peluang besar untuk melunasi pinjaman dengan lancar. Harapannya setelah mempunyai model machine learning dengan perfomance model yang baik, pegawai bank dapat dengan mudah mengidentifikasi karakteristik customer yang memiliki peluang besar untuk melunasi pinjaman dengan lancar. Dengan adanya model machine learning ini tentunya akan mengurangi biaya dan waktu yang lebih cepat. 3.1.2 Modelling Analysis 3.1.2.1 Cleaning data credit &lt;- read_csv(&quot;assets/02-finance/credit_record.csv&quot;) application &lt;- read_csv(&quot;assets/02-finance/application_record.csv&quot;) Data Description: Credit ID : Client number MONTHS_BALANCE : Record month The month of the extracted data is the starting point, backwards, 0 is the current month, -1 is the previous month, and so on STATUS : Status 0: 1-29 days past due 1: 30-59 days past due 2: 60-89 days overdue 3: 90-119 days overdue 4: 120-149 days overdue 5: Overdue or bad debts, write-offs for more than 150 days C: paid off that month X: No loan for the month Application ID : Client number CODE_GENDER : Gender FLAG_OWN_CAR : Is there a car FLAG_OWN_REALTY ; Is there a property CNT_CHILDREN : Number of children AMT_INCOME_TOTAL : Annual income NAME_INCOME_TYPE : Income category NAME_EDUCATION_TYPE : Education level NAME_FAMILY_STATUS : Marital status NAME_HOUSING_TYPE : Way of living DAYS_BIRTH : Birthday Count backwards from current day (0), -1 means yesterday DAYS_EMPLOYED : Start date of employment Count backwards from current day(0). If positive, it means - - the person currently unemployed. FLAG_MOBIL : Is there a mobile phone FLAG_WORK_PHONE : Is there a work phone FLAG_PHONE : Is there a phone FLAG_EMAIL : Is there an email OCCUPATION_TYPE : Occupation CNT_FAM_MEMBERS :Family size Check missing values Pada data credit tidak terdapat missing value colSums(is.na(credit)) #&gt; ID MONTHS_BALANCE STATUS #&gt; 0 0 0 colSums(is.na(application)) #&gt; ID CODE_GENDER FLAG_OWN_CAR FLAG_OWN_REALTY #&gt; 0 0 0 0 #&gt; CNT_CHILDREN AMT_INCOME_TOTAL NAME_INCOME_TYPE NAME_EDUCATION_TYPE #&gt; 0 0 0 0 #&gt; NAME_FAMILY_STATUS NAME_HOUSING_TYPE DAYS_BIRTH DAYS_EMPLOYED #&gt; 0 0 0 0 #&gt; FLAG_MOBIL FLAG_WORK_PHONE FLAG_PHONE FLAG_EMAIL #&gt; 0 0 0 0 #&gt; OCCUPATION_TYPE CNT_FAM_MEMBERS #&gt; 134203 0 Pada data application terdapat variabel OCCUPATION_TYPE yang memiliki banyak data missing, kita dapat membuang variabel tersebut. Serta kita akan membuang variabel DAYS_BIRTH dan DAYS_EMPLOYED yang tidak dibutuhkan pada model. application &lt;- application %&gt;% select(-c(OCCUPATION_TYPE, DAYS_BIRTH, DAYS_EMPLOYED)) Menyesuaikan tipe data Tahap berikutnya adalah menggabunkan data credit dan application serta menyesuaikan tipe data kategorik yang masih terbaca sebagai character. data_clean &lt;- credit %&gt;% left_join(application) %&gt;% na.omit() %&gt;% select(-ID) %&gt;% filter(STATUS != &quot;X&quot;) %&gt;% mutate(STATUS = as.factor(ifelse(STATUS == &quot;C&quot;, &quot;good credit&quot;, &quot;bad credit&quot;))) %&gt;% mutate_at(.vars = c(&quot;FLAG_MOBIL&quot;, &quot;FLAG_WORK_PHONE&quot;, &quot;FLAG_PHONE&quot;, &quot;FLAG_EMAIL&quot;), as.factor) %&gt;% mutate_if(is.character, as.factor) %&gt;% data.frame() str(data_clean) #&gt; &#39;data.frame&#39;:\t631765 obs. of 16 variables: #&gt; $ MONTHS_BALANCE : num 0 -1 -2 -3 -4 -5 -6 -7 -8 -9 ... #&gt; $ STATUS : Factor w/ 2 levels &quot;bad credit&quot;,&quot;good credit&quot;: 2 2 2 2 2 2 2 2 2 2 ... #&gt; $ CODE_GENDER : Factor w/ 2 levels &quot;F&quot;,&quot;M&quot;: 2 2 2 2 2 2 2 2 2 2 ... #&gt; $ FLAG_OWN_CAR : Factor w/ 2 levels &quot;N&quot;,&quot;Y&quot;: 2 2 2 2 2 2 2 2 2 2 ... #&gt; $ FLAG_OWN_REALTY : Factor w/ 2 levels &quot;N&quot;,&quot;Y&quot;: 2 2 2 2 2 2 2 2 2 2 ... #&gt; $ CNT_CHILDREN : num 0 0 0 0 0 0 0 0 0 0 ... #&gt; $ AMT_INCOME_TOTAL : num 427500 427500 427500 427500 427500 ... #&gt; $ NAME_INCOME_TYPE : Factor w/ 5 levels &quot;Commercial associate&quot;,..: 5 5 5 5 5 5 5 5 5 5 ... #&gt; $ NAME_EDUCATION_TYPE: Factor w/ 5 levels &quot;Academic degree&quot;,..: 2 2 2 2 2 2 2 2 2 2 ... #&gt; $ NAME_FAMILY_STATUS : Factor w/ 5 levels &quot;Civil marriage&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... #&gt; $ NAME_HOUSING_TYPE : Factor w/ 6 levels &quot;Co-op apartment&quot;,..: 5 5 5 5 5 5 5 5 5 5 ... #&gt; $ FLAG_MOBIL : Factor w/ 1 level &quot;1&quot;: 1 1 1 1 1 1 1 1 1 1 ... #&gt; $ FLAG_WORK_PHONE : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 2 2 2 2 2 2 2 2 2 2 ... #&gt; $ FLAG_PHONE : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 1 1 1 1 1 1 1 1 1 1 ... #&gt; $ FLAG_EMAIL : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 1 1 1 1 1 1 1 1 1 1 ... #&gt; $ CNT_FAM_MEMBERS : num 2 2 2 2 2 2 2 2 2 2 ... 3.1.2.2 Exploratory Data Analysis (EDA) Pada data EDA kita ingin mengetahui bagaimana sebaran data kategorik maupun numerik. data_clean %&gt;% inspect_cat() %&gt;% show_plot() Pada visualisasi berikut kita akan mendapatkan informasi apakah terdapat variabel yang tidak memiliki banyak informasi pada data, contohnya adalah variabel FLAG_MOBIL dimana keseluruhan data berisikan 1, artinya semua nasabah kita yang melakukan pinjaman memiliki mobil. Data yang tidak memiliki variansi seperti ini tidak diikutsertakan pada model. data_clean &lt;- data_clean %&gt;% select(-c(FLAG_MOBIL,FLAG_EMAIL)) data_clean %&gt;% inspect_num() %&gt;% show_plot() 3.1.2.3 Modelling Random Forest Split data train dan data test dengan proporsi 80:20. Data train akan digunakan untuk modelling, sedangkan data test akan digunakan untuk evaluasi. set.seed(100) index &lt;- initial_split(data = data_clean, prop = 0.8, strata = &quot;STATUS&quot;) train &lt;- training(index) test &lt;- testing(index) Cek proporsi dari target variabel prop.table(table(train$STATUS)) #&gt; #&gt; bad credit good credit #&gt; 0.4960938 0.5039062 Bentuk model random forest dengan 3 k-fold dan 2 repetition # set.seed(100) # # ctrl &lt;- trainControl(method = &quot;repeatedcv&quot;, # number = 3, # repeats = 2, # allowParallel=FALSE) # # model_forest &lt;- caret::train(STATUS ~., # data = train, # method = &quot;rf&quot;, # trControl = ctrl) #saveRDS(model_forest, &quot;model_forest.RDS&quot;) model_forest &lt;- readRDS(&quot;assets/02-finance/model_forest.RDS&quot;) model_forest #&gt; Random Forest #&gt; #&gt; 80001 samples #&gt; 13 predictor #&gt; 2 classes: &#39;bad credit&#39;, &#39;good credit&#39; #&gt; #&gt; No pre-processing #&gt; Resampling: Cross-Validated (3 fold, repeated 2 times) #&gt; Summary of sample sizes: 53335, 53334, 53333, 53334, 53334, 53334, ... #&gt; Resampling results across tuning parameters: #&gt; #&gt; mtry Accuracy Kappa #&gt; 2 0.6432232 0.2846367 #&gt; 14 0.7487656 0.4973803 #&gt; 26 0.7114411 0.4230518 #&gt; #&gt; Accuracy was used to select the optimal model using the largest value. #&gt; The final value used for the model was mtry = 14. Setelah dilakukan 3 repetition pada model, repetition kedua memiliki accuracy paling tinggi dengan jumlah mtry sebanyak 14. Selanjutnya akan dilakukan prediksi untuk data test dan mencari nilai confusion matrix pada hasil prediksi. pred_rf&lt;- predict(model_forest, newdata = test, type = &quot;prob&quot;) %&gt;% mutate(result = as.factor(ifelse(`bad credit` &gt; 0.45, &quot;bad credit&quot;, &quot;good credit&quot;)), actual = ifelse(test$STATUS == &#39;good credit&#39;, 0, 1)) confmat_rf &lt;- confusionMatrix(pred_rf$result, test$STATUS, mode = &quot;prec_recall&quot;, positive = &quot;bad credit&quot;) eval_rf &lt;- tidy(confmat_rf) %&gt;% mutate(model = &quot;Random Forest&quot;) %&gt;% select(model, term, estimate) %&gt;% filter(term %in% c(&quot;accuracy&quot;, &quot;precision&quot;, &quot;recall&quot;, &quot;specificity&quot;)) eval_rf #&gt; [38;5;246m# A tibble: 4 x 3[39m #&gt; model term estimate #&gt; [3m[38;5;246m&lt;chr&gt;[39m[23m [3m[38;5;246m&lt;chr&gt;[39m[23m [3m[38;5;246m&lt;dbl&gt;[39m[23m #&gt; [38;5;250m1[39m Random Forest accuracy 0.810 #&gt; [38;5;250m2[39m Random Forest specificity 0.814 #&gt; [38;5;250m3[39m Random Forest precision 0.810 #&gt; [38;5;250m4[39m Random Forest recall 0.806 3.1.2.4 Modelling XGBoost Tahap selanjutnya kita akan implementasikan data menggunakan model XGBoost, kita perlu menyiapkan data untuk model XGBoost terlebih dahulu data_xgb &lt;- data_clean %&gt;% mutate(STATUS = ifelse(STATUS == &quot;good credit&quot;, 0, 1)) %&gt;% data.frame() set.seed(100) index &lt;- initial_split(data = data_xgb, prop = 0.8, strata = &quot;STATUS&quot;) train_xgb &lt;- training(index) test_xgb &lt;- testing(index) label_train &lt;- as.numeric(train_xgb$STATUS) label_test &lt;- as.numeric(test_xgb$STATUS) train_matrix &lt;- data.matrix(train_xgb[,-2]) test_matrix &lt;- data.matrix(test_xgb[,-2]) # convert data to Dmatrix dtrain &lt;- xgb.DMatrix(data = train_matrix, label = label_train) dtest &lt;- xgb.DMatrix(data = test_matrix, label = label_test) params &lt;- list(booster = &quot;gbtree&quot;, objective = &quot;binary:logistic&quot;, eta=0.7, gamma=10, max_depth=10, min_child_weight=3, subsample=1, colsample_bytree=0.5) xgbcv &lt;- xgb.cv( params = params, data = dtrain, nrounds = 1000, showsd = T, nfold = 10, stratified = T, print_every_n = 50, early_stopping_rounds = 20, maximize = F) #&gt; [15:34:08] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. #&gt; [15:34:08] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. #&gt; [15:34:08] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. #&gt; [15:34:08] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. #&gt; [15:34:08] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. #&gt; [15:34:08] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. #&gt; [15:34:08] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. #&gt; [15:34:08] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. #&gt; [15:34:08] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. #&gt; [15:34:08] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. #&gt; [1]\ttrain-logloss:0.657835+0.008359\ttest-logloss:0.659014+0.008508 #&gt; Multiple eval metrics are present. Will use test_logloss for early stopping. #&gt; Will train until test_logloss hasn&#39;t improved in 20 rounds. #&gt; #&gt; [51]\ttrain-logloss:0.502362+0.002141\ttest-logloss:0.523349+0.004859 #&gt; [101]\ttrain-logloss:0.487041+0.003130\ttest-logloss:0.511456+0.005289 #&gt; [151]\ttrain-logloss:0.480246+0.002629\ttest-logloss:0.506027+0.005960 #&gt; [201]\ttrain-logloss:0.476529+0.001743\ttest-logloss:0.503253+0.005054 #&gt; [251]\ttrain-logloss:0.473638+0.001135\ttest-logloss:0.500887+0.004853 #&gt; [301]\ttrain-logloss:0.470976+0.000924\ttest-logloss:0.499179+0.004563 #&gt; [351]\ttrain-logloss:0.468947+0.001220\ttest-logloss:0.497893+0.004723 #&gt; [401]\ttrain-logloss:0.467192+0.001040\ttest-logloss:0.496554+0.004479 #&gt; [451]\ttrain-logloss:0.465561+0.001008\ttest-logloss:0.495503+0.004603 #&gt; [501]\ttrain-logloss:0.464243+0.001280\ttest-logloss:0.494370+0.004283 #&gt; [551]\ttrain-logloss:0.463161+0.001288\ttest-logloss:0.493679+0.004757 #&gt; [601]\ttrain-logloss:0.462078+0.001320\ttest-logloss:0.492902+0.005234 #&gt; [651]\ttrain-logloss:0.461227+0.001556\ttest-logloss:0.492334+0.005148 #&gt; [701]\ttrain-logloss:0.460708+0.001542\ttest-logloss:0.491907+0.005247 #&gt; [751]\ttrain-logloss:0.460228+0.001541\ttest-logloss:0.491557+0.005346 #&gt; [801]\ttrain-logloss:0.459816+0.001526\ttest-logloss:0.491173+0.005345 #&gt; [851]\ttrain-logloss:0.459306+0.001623\ttest-logloss:0.491024+0.005399 #&gt; [901]\ttrain-logloss:0.459079+0.001518\ttest-logloss:0.490845+0.005412 #&gt; [951]\ttrain-logloss:0.458793+0.001343\ttest-logloss:0.490583+0.005397 #&gt; [1000]\ttrain-logloss:0.458523+0.001349\ttest-logloss:0.490372+0.005506 print(xgbcv) #&gt; ##### xgb.cv 10-folds #&gt; iter train_logloss_mean train_logloss_std test_logloss_mean test_logloss_std #&gt; 1 0.6578350 0.008358933 0.6590142 0.008508178 #&gt; 2 0.6328996 0.009530030 0.6361721 0.009246701 #&gt; 3 0.6146676 0.008940854 0.6200005 0.008948886 #&gt; 4 0.6026426 0.011825006 0.6092987 0.010625445 #&gt; 5 0.5889047 0.009177284 0.5968755 0.007583324 #&gt; --- #&gt; 996 0.4585228 0.001349444 0.4903715 0.005506051 #&gt; 997 0.4585228 0.001349444 0.4903715 0.005506051 #&gt; 998 0.4585228 0.001349444 0.4903716 0.005506139 #&gt; 999 0.4585228 0.001349444 0.4903715 0.005506051 #&gt; 1000 0.4585228 0.001349444 0.4903715 0.005506051 #&gt; Best iteration: #&gt; iter train_logloss_mean train_logloss_std test_logloss_mean test_logloss_std #&gt; 996 0.4585228 0.001349444 0.4903715 0.005506051 xgb1 &lt;- xgb.train (params = params, data = dtrain, nrounds = xgbcv$best_iteration, watchlist = list(val=dtest,train=dtrain), print_every_n = 100, early_stoping_rounds = 10, maximize = F , eval_metric = &quot;error&quot;, verbosity = 0) #&gt; [1]\tval-error:0.408470\ttrain-error:0.404582 #&gt; [101]\tval-error:0.259563\ttrain-error:0.238085 #&gt; [201]\tval-error:0.253613\ttrain-error:0.233447 #&gt; [301]\tval-error:0.251413\ttrain-error:0.228810 #&gt; [401]\tval-error:0.248062\ttrain-error:0.226485 #&gt; [501]\tval-error:0.246562\ttrain-error:0.224297 #&gt; [601]\tval-error:0.246562\ttrain-error:0.223735 #&gt; [701]\tval-error:0.246512\ttrain-error:0.222985 #&gt; [801]\tval-error:0.246312\ttrain-error:0.223010 #&gt; [901]\tval-error:0.245112\ttrain-error:0.222872 #&gt; [996]\tval-error:0.243512\ttrain-error:0.222472 xgbpred_prob &lt;-predict(object = xgb1, newdata = dtest) xgbpred &lt;- ifelse (xgbpred_prob &gt; 0.45,1,0) confmat_xgb &lt;- confusionMatrix(as.factor(xgbpred), as.factor(label_test), positive = &quot;1&quot;) confmat_xgb #&gt; Confusion Matrix and Statistics #&gt; #&gt; Reference #&gt; Prediction 0 1 #&gt; 0 7338 2288 #&gt; 1 2740 7633 #&gt; #&gt; Accuracy : 0.7486 #&gt; 95% CI : (0.7425, 0.7546) #&gt; No Information Rate : 0.5039 #&gt; P-Value [Acc &gt; NIR] : &lt; 0.00000000000000022 #&gt; #&gt; Kappa : 0.4973 #&gt; #&gt; Mcnemar&#39;s Test P-Value : 0.0000000002013 #&gt; #&gt; Sensitivity : 0.7694 #&gt; Specificity : 0.7281 #&gt; Pos Pred Value : 0.7359 #&gt; Neg Pred Value : 0.7623 #&gt; Prevalence : 0.4961 #&gt; Detection Rate : 0.3817 #&gt; Detection Prevalence : 0.5187 #&gt; Balanced Accuracy : 0.7487 #&gt; #&gt; &#39;Positive&#39; Class : 1 #&gt; confmat_rf &lt;- confusionMatrix(pred_rf$result, test$STATUS, mode = &quot;prec_recall&quot;, positive = &quot;bad credit&quot;) eval_rf &lt;- tidy(confmat_rf) %&gt;% mutate(model = &quot;Random Forest&quot;) %&gt;% select(model, term, estimate) %&gt;% filter(term %in% c(&quot;accuracy&quot;, &quot;precision&quot;, &quot;recall&quot;, &quot;specificity&quot;)) confmat_xgb &lt;- confusionMatrix(as.factor(xgbpred), as.factor(label_test), positive = &quot;1&quot;) eval_xgb &lt;- tidy(confmat_xgb) %&gt;% mutate(model = &quot;XGBoost&quot;) %&gt;% select(model, term, estimate) %&gt;% filter(term %in% c(&quot;accuracy&quot;, &quot;precision&quot;, &quot;recall&quot;, &quot;specificity&quot;)) Setelah diperoleh perfomance model XGBoost kita akan membandingkan dengan perfomance model random forest. eval_result &lt;- rbind(eval_rf, eval_xgb) eval_result #&gt; [38;5;246m# A tibble: 8 x 3[39m #&gt; model term estimate #&gt; [3m[38;5;246m&lt;chr&gt;[39m[23m [3m[38;5;246m&lt;chr&gt;[39m[23m [3m[38;5;246m&lt;dbl&gt;[39m[23m #&gt; [38;5;250m1[39m Random Forest accuracy 0.810 #&gt; [38;5;250m2[39m Random Forest specificity 0.814 #&gt; [38;5;250m3[39m Random Forest precision 0.810 #&gt; [38;5;250m4[39m Random Forest recall 0.806 #&gt; [38;5;250m5[39m XGBoost accuracy 0.749 #&gt; [38;5;250m6[39m XGBoost specificity 0.728 #&gt; [38;5;250m7[39m XGBoost precision 0.736 #&gt; [38;5;250m8[39m XGBoost recall 0.769 Metrics evaluasi yang kita utamakan adalah recall karena kita ingin meminimalisir mungkin keadaan dimana data actual nasabah tersebut bad credit namun terprediksi sebagai good credit. Dari hasil evaluasi dapat diketahui model XGBoost memiliki nilai recall lebih tinggi dibandingkan model random forest. var_imp &lt;- xgb.importance(model = xgb1, feature_names = dimnames(dtrain)[[2]]) xgb.ggplot.importance(var_imp,top_n = 10) + theme_minimal()+ theme(legend.position = &quot;none&quot;) Grafik di atas menampilkan informasi mengenai 10 variabel yang paling berpengaruh pada model. Annual income dan months balance merupakan dua variabel terpenting pada model ini. xgb_result &lt;- data.frame(class1 = xgbpred_prob, actual = as.factor(label_test)) auc_xgb &lt;- roc_auc(data = xgb_result, truth = actual,class1) value_roc_xgb &lt;- prediction(predictions = xgbpred_prob, labels = label_test) # ROC curve plot(performance(value_roc_xgb, &quot;tpr&quot;, &quot;fpr&quot;)) value_auc_xgb &lt;- performance(value_roc_xgb, measure = &quot;auc&quot;) value_auc_xgb@y.values #&gt; [[1]] #&gt; [1] 0.8427956 Nilai AUC yang diperoleh pada model model ini sebesar 0.83 artinya model dapat memprediksi dengan baik kedua target class yaitu good credit dan bad credit. Harapannya model ini dapat digunakan oleh pihak bank untuk menentukan credit scoring dengan mengisikan data profil nasabah, kemudian hasil yang diperoleh dapat di visualisasikan sebagai berikut: explainer &lt;- lime(train_matrix %&gt;% as.data.frame(), xgb1) explanation &lt;- explain(test_matrix[11:12,] %&gt;% as.data.frame(), explainer, labels = &quot;1&quot;, n_features = 3, n_permutations = 5000, dist_fun = &quot;manhattan&quot;, kernel_width = 0.75, feature_select = &quot;highest_weights&quot;) plot_features(explanation) Hasil dari visualisasi tersebut untuk nasabah 1 dan 2 memiliki probability 0.22 dan 0.17 artinya kedua nasabah tersebut akan dikategorikan sebagai good credit. Kedua nasabah tersebut memiliki karakteristik yang mirip karena hasil prediksi mereka didukung oleh kepemilikan model dan juga total income. 3.2 Evaluating Customer Financial Complaints 3.2.1 Background Penanganan complain customer pada perusahaan saat ini menjadi salah satu kunci utama suatu perusahaan dapat terus tumbuh dan berkembang, karena apabila nasabah merasa tidak mendapatkan layanan yang baik saat menyampaikan keluhan maka nasabah akan mudah berpindah ke perusahaan lain yang dianggap bisa memberikan layanan terhadap komplain dengan baik. Nasabah yang merasa tidak mendapatkan layanan baik biasanya akan mengajukan keluhan ke Consumer Financial Protection Bureau (CFPB), CFPB merupakan instansi yang bertanggung jawab atas perlindungan konsumen di sektor keuangan. CFPB menyediakan data yang berisi keluhan dari customer financial, data keluhan tersebut dapat dianalisa untuk dijadikan pertimbangan pihak perusahaan untuk mengetahui indikator yang memerlukan perbaikan demi meningkatkan kualitas layanan. 3.2.2 Exploratory Data Analysis customer &lt;- read_csv(&quot;assets/02-finance/data_complaint.csv&quot;)%&gt;% mutate_if(is.character, as.factor) %&gt;% data.frame() Data diperoleh dari Consumer Financial Protection Bureau (CFPB) yang mengatur penawaran dan penyediaan produk atau layanan nasabah keuangan. CFPB menyediakan pertanyaan-pertanyaan umum dan dapat membantu nasabah terhubung dengan perusahaan keuangan yang terlibat. Data tersebut berisikan keluhan nasabah dari berbagai bank di Amerika Serikat. top_company &lt;- customer %&gt;% na.omit(Consumer.complaint.narrative) %&gt;% group_by(Company) %&gt;% summarise(total = n()) %&gt;% arrange(desc(total)) %&gt;% head(1) Dari 4504 perusahaan pada data, perusahaan yang paling banyak memiliki complain adalah Transunion Intermediate Holdings. Perlu diketahui bahwa banyaknya complain yang diperhitungkan tidak mempertimbangkan volume perusahaan. Misalnya, perusahaan dengan lebih banyak customer tentunya memiliki kemungkinan banyak complain dibandingkan perusahaan yang lebih sedikit pelanggannya dan juga pada analisa ini kita hanya memperhitungkan complain yang dilengkapi dengan narasi dari customer tersebut. Berikutnya kita akan fokus untuk menganalisa complai dari perusahaan Transunion Intermediate Holdings yang memiliki paling banyak narasi complain dari data. Setelah memperoleh data observasi, selanjutnya membersihkan data text: data_clean &lt;- data_complaint %&gt;% select(Consumer.complaint.narrative) %&gt;% mutate(Consumer.complaint.narrative = Consumer.complaint.narrative %&gt;% tolower() %&gt;% str_trim() %&gt;% str_remove_all(pattern = &quot;[[:punct:]]&quot;) %&gt;% str_remove_all(pattern = &quot;[0-9]&quot;) %&gt;% str_remove_all(pattern = &quot;xxxx&quot;) %&gt;% replace_contraction() %&gt;% replace_word_elongation() %&gt;% replace_white() %&gt;% str_squish()) head(data_clean) #&gt; Consumer.complaint.narrative #&gt; 1 this legal notice being sent and delivered to you persuant to florida statutes notice of order to cease and desist from using personal and private information fl statute violation title xlvi crimes chapter fraudulent practices view entire chapter criminal use of personal identification information as used in this section the term a access device means any card plate code account number electronic serial number mobile identification number personal identification number or other telecommunications service equipment or instrument identifier or other means of account access that can be used alone or in conjunction with another access device to obtain money goods services or any other thing of value or that can be used to initiate a transfer of funds other than a transfer originated solely by paper instrument b authorization means empowerment permission or competence to act c harass means to engage in conduct directed at a specific person that is intended to cause substantial emotional distress to such person and serves no legitimate purpose harass does not mean to use personal identification information for accepted commercial purposes the term does not include constitutionally protected conduct such as organized protests or the use of personal identification information for accepted commercial purposes d individual means a single human being and does not mean a firm association of individuals corporation partnership joint venture sole proprietorship or any other entity e person means a person as defined in s f personal identification information means any name or number that may be used alone or in conjunction with any other information to identify a specific individual including any name postal or electronic mail address telephone number social security number date of birth mother s maiden name official stateissued or united statesissued driver license or identification number alien registration number government passport number employer or taxpayer identification number or food assistance account number bank account number credit or debit card number or personal identification number or code assigned to the holder of a debit card by the issuer to permit authorized electronic use of such card unique biometric data such as fingerprint voice print retina or iris image or other unique physical representation unique electronic identification number address or routing code medical records telecommunication identifying information or access device or other number or information that can be used to access a person s financial resources g counterfeit or fictitious personal identification information means any counterfeit fictitious or fabricated information in the similitude of the data outlined in paragraph f that although not truthful or accurate would in context lead a reasonably prudent person to credit its truthfulness and accuracy a any person who willfully and without authorization fraudulently uses or possesses with intent to fraudulently use personal identification information concerning an individual without first obtaining that individual s consent commits the offense of fraudulent use of personal identification information which is a felony of the third degree punishable as provided in s s or s b any person who willfully and without authorization fraudulently uses personal identification information concerning an individual without first obtaining that individual s consent commits a felony of the second degree punishable as provided in s s or s if the pecuniary benefit the value of the services #&gt; 2 transunion continues to report inaccurate negative items on my report that i have previously dispu ted ti methey continuos to report account with no account number inaccurate information that is hurting my credit #&gt; 3 my credit report has an incorrect address the address on the report is nv other than when i was in the i have lived in my entire life associated with this lv address are bills from and and both amounts are less than $ every time i dispute these bills and ask for source documents to prove i owe the bills are sold to collection agencies when these receivables are sold as a package the collection agencies reopen the outstanding debt and it reappears on my report i have a substantial credit history with mortgage car loans and credit cards i have never been late on any amount i owe there should be some protection against collection companies windmilling these bogus receivables thank you for all you do #&gt; 4 i recently discovered that reported my account as a charge off on my credit report when i was denied a loan by due to the charge off i immediately contacted by phone in an attempt to rectify the error i was informed by the customer service representative that my last payment was received on and that my account was reported as a charge off on we both agreed that it did not make sense that my account would be reported as a charge off days after my payment was received at the time reported my account as a charge off it was paid in full with a i have spent hours on the phone with numerous customer service representatives the phone calls have been extremely frustrating and have not produced a positive outcome every representative has told me the same thing due to the age of this account they do not have account records and can not provide me with the information that i am requesting yet they refuse to remove the inaccurate information from my credit report i was told that a supervisors would call me back on separate occasions to this day i have not received a call back from a supervisor i have sent numerous certified letters disputing the charge off that is currently being reported on my credit report and negatively effecting my credit i have asked for validation of my account including details explaining my why my account is being reported as a charge off with a balance i have asked for them to send me a copy of my payment history along with account notes the letters that i received in response to my letters have been a one paragraph response stating that they only report accurate information to the credit bureaus #&gt; 5 sent letter to credit reporting agencies telling them the inquiries on my report are unjustified the actions fail to comply with fcra section the letters are attached below credit agencies replied stating they did not need to investigate verify or remove inquiries credit agencies failed to prove requirements and fa iled to remove inquiries from my credit reports #&gt; 6 student loans have been discharged and i have a letter from the stating all loans have been discharged the credit bureaus refuse to accept the letter Setelah membersihkan data text, selanjutnya kita akan melakukan proses tokenization yaitu memecah 1 kalimat menjadi beberapa term, pada proses berikut ini juga diperoleh frekuensi dari setiap term yang muncul. text.dat &lt;- data_clean %&gt;% rowid_to_column(&quot;id&quot;) %&gt;% unnest_tokens(word, Consumer.complaint.narrative) %&gt;% anti_join(stop_words) %&gt;% count(word, sort = T) %&gt;% rename(words = word, freq = n) %&gt;% filter(words != is.na(words), freq &gt; 50) head(text.dat) #&gt; words freq #&gt; 1 credit 1572 #&gt; 2 report 774 #&gt; 3 account 613 #&gt; 4 transunion 529 #&gt; 5 information 503 #&gt; 6 reporting 358 Kata yang sudah diperoleh akan divisualisasikan dengan wordcloud. Semakin sering suatu kata digunakan, maka semakin besar pula ukuran kata tersebut ditampilkan dalam wordcloud. Artinya kita dapat mengetahui kata yang paling sering digunakan oleh customer Transunion Intermediate Holdings. Kata credit, report, dan account merupakan kata yang paling sering digunakan oleh customer saat complain. wordcloud2(data = text.dat, size = 1, color = &#39;random-dark&#39;, shuffle = 1) 3.2.3 Comparing Sentiment Dictionaries Semakin banyak informasi yang ditampilkan, dapat membantu pihak marketing mengembangkan strategi yang efektif dalam meningkatkan pelayanan, berikutnya tidak hanya kata yang sering muncul yang akan ditampilkan, namun juga informasi mengenai kata tersebut merupakan kata positif atau negatif yang digunakan oleh customer saat mengajukan complain. text_dat &lt;- data_clean %&gt;% rowid_to_column(&quot;id&quot;) %&gt;% unnest_tokens(word, Consumer.complaint.narrative) %&gt;% anti_join(stop_words) %&gt;% mutate(word = wordStem(word)) %&gt;% count(word, sort = T) %&gt;% filter(word != is.na(word)) head(text_dat,20) #&gt; word n #&gt; 1 credit 1573 #&gt; 2 report 1462 #&gt; 3 account 869 #&gt; 4 inform 584 #&gt; 5 transunion 533 #&gt; 6 remov 423 #&gt; 7 inquiri 420 #&gt; 8 disput 415 #&gt; 9 file 345 #&gt; 10 request 329 #&gt; 11 letter 309 #&gt; 12 loan 287 #&gt; 13 payment 269 #&gt; 14 bureau 263 #&gt; 15 verifi 242 #&gt; 16 call 228 #&gt; 17 time 227 #&gt; 18 compani 223 #&gt; 19 receiv 218 #&gt; 20 agenc 213 bing_word &lt;- text_dat %&gt;% inner_join(get_sentiments(&quot;bing&quot;)) head(bing_word) #&gt; word n sentiment #&gt; 1 debt 206 negative #&gt; 2 correct 150 positive #&gt; 3 complaint 115 negative #&gt; 4 fraud 96 negative #&gt; 5 incorrect 89 negative #&gt; 6 hard 82 negative library(reshape2) library(wordcloud) bing_word %&gt;% acast(word ~ sentiment, value.var = &quot;n&quot;, fill = 0) %&gt;% comparison.cloud(colors = c(&quot;gray70&quot;,&quot;gray20&quot;), max.words = 200) Sentiment Analysis yang dilakukan sebelumnya kita memperhitungan kemunculan kata positif dan negatif. Salah satu kelemahan pada pendekatan tersebut terkadang dapat disalah artikan penggunaannya pada sebuah kata, misal correct dan support akan dianggap sebagai kata positif. Namun, arti kata tersebut akan berubah jika terdapat kata not didepannya. Pada analisis berikut ini kita akan menggunakan n-gram untuk melihat seberapa sering word1 diikuti oleh word2. Tokenisasi menggunakan n-gram berguna untuk eksplorasi kata yang memiliki hubungan. Ketika kita mengatur n = 2 artinya kita akan menampilkan dua kata berturut-turut atau sering disebut dengam bigrams. Hasil dari visualisasi berikut ini menampilkan kata-kata yang berhubungan dengan kata not. dat_bigrams &lt;- data_clean %&gt;% unnest_tokens(bigram, Consumer.complaint.narrative, token = &quot;ngrams&quot;, n= 2) %&gt;% separate(bigram, c(&quot;word1&quot;,&quot;word2&quot;), sep = &quot; &quot;) %&gt;% filter(word1 == &quot;not&quot;) %&gt;% inner_join(get_sentiments(&quot;afinn&quot;), by = c(word2 = &quot;word&quot;)) %&gt;% count(word1,word2, value, sort = T) %&gt;% mutate(contribution = n*value) %&gt;% arrange(desc(abs(contribution))) %&gt;% group_by(word1) %&gt;% dplyr::slice(seq_len(20)) %&gt;% arrange(word1, desc(contribution)) %&gt;% ungroup() graph_bigram &lt;- dat_bigrams %&gt;% graph_from_data_frame() set.seed(123) a &lt;- grid::arrow(type = &quot;closed&quot;, length = unit(.15, &quot;inches&quot;)) ggraph(graph_bigram, layout = &quot;fr&quot;) + geom_edge_link(alpha = .25) + geom_edge_density(aes(fill = value)) + geom_node_point(color = &quot;lightblue&quot;, size = 5) + geom_node_text(aes(label = name), repel = TRUE) + theme_void() + theme(legend.position = &quot;none&quot;, plot.title = element_text(hjust = 0.5)) + ggtitle(&quot;Negation Bigram Network&quot;) 3.2.4 Correlation Pairs Analisis berikutnya, akan dilakukan eksplorasi untuk mengetahui kata-kata yang memiliki kecenderungan muncul bersama pada complain nasabah dengan mencari nilai korelasi antar kata. data_clean_cor &lt;- data_complaint %&gt;% select(Consumer.complaint.narrative,Issue,Product) %&gt;% mutate(Consumer.complaint.narrative = Consumer.complaint.narrative %&gt;% tolower() %&gt;% str_trim() %&gt;% str_remove_all(pattern = &quot;[[:punct:]]&quot;) %&gt;% str_remove_all(pattern = &quot;[0-9]&quot;) %&gt;% str_remove_all(pattern = &quot;xxxx&quot;) %&gt;% replace_contraction() %&gt;% replace_word_elongation() %&gt;% replace_white() %&gt;% str_squish()) head(data_clean_cor) #&gt; Consumer.complaint.narrative #&gt; 1 this legal notice being sent and delivered to you persuant to florida statutes notice of order to cease and desist from using personal and private information fl statute violation title xlvi crimes chapter fraudulent practices view entire chapter criminal use of personal identification information as used in this section the term a access device means any card plate code account number electronic serial number mobile identification number personal identification number or other telecommunications service equipment or instrument identifier or other means of account access that can be used alone or in conjunction with another access device to obtain money goods services or any other thing of value or that can be used to initiate a transfer of funds other than a transfer originated solely by paper instrument b authorization means empowerment permission or competence to act c harass means to engage in conduct directed at a specific person that is intended to cause substantial emotional distress to such person and serves no legitimate purpose harass does not mean to use personal identification information for accepted commercial purposes the term does not include constitutionally protected conduct such as organized protests or the use of personal identification information for accepted commercial purposes d individual means a single human being and does not mean a firm association of individuals corporation partnership joint venture sole proprietorship or any other entity e person means a person as defined in s f personal identification information means any name or number that may be used alone or in conjunction with any other information to identify a specific individual including any name postal or electronic mail address telephone number social security number date of birth mother s maiden name official stateissued or united statesissued driver license or identification number alien registration number government passport number employer or taxpayer identification number or food assistance account number bank account number credit or debit card number or personal identification number or code assigned to the holder of a debit card by the issuer to permit authorized electronic use of such card unique biometric data such as fingerprint voice print retina or iris image or other unique physical representation unique electronic identification number address or routing code medical records telecommunication identifying information or access device or other number or information that can be used to access a person s financial resources g counterfeit or fictitious personal identification information means any counterfeit fictitious or fabricated information in the similitude of the data outlined in paragraph f that although not truthful or accurate would in context lead a reasonably prudent person to credit its truthfulness and accuracy a any person who willfully and without authorization fraudulently uses or possesses with intent to fraudulently use personal identification information concerning an individual without first obtaining that individual s consent commits the offense of fraudulent use of personal identification information which is a felony of the third degree punishable as provided in s s or s b any person who willfully and without authorization fraudulently uses personal identification information concerning an individual without first obtaining that individual s consent commits a felony of the second degree punishable as provided in s s or s if the pecuniary benefit the value of the services #&gt; 2 transunion continues to report inaccurate negative items on my report that i have previously dispu ted ti methey continuos to report account with no account number inaccurate information that is hurting my credit #&gt; 3 my credit report has an incorrect address the address on the report is nv other than when i was in the i have lived in my entire life associated with this lv address are bills from and and both amounts are less than $ every time i dispute these bills and ask for source documents to prove i owe the bills are sold to collection agencies when these receivables are sold as a package the collection agencies reopen the outstanding debt and it reappears on my report i have a substantial credit history with mortgage car loans and credit cards i have never been late on any amount i owe there should be some protection against collection companies windmilling these bogus receivables thank you for all you do #&gt; 4 i recently discovered that reported my account as a charge off on my credit report when i was denied a loan by due to the charge off i immediately contacted by phone in an attempt to rectify the error i was informed by the customer service representative that my last payment was received on and that my account was reported as a charge off on we both agreed that it did not make sense that my account would be reported as a charge off days after my payment was received at the time reported my account as a charge off it was paid in full with a i have spent hours on the phone with numerous customer service representatives the phone calls have been extremely frustrating and have not produced a positive outcome every representative has told me the same thing due to the age of this account they do not have account records and can not provide me with the information that i am requesting yet they refuse to remove the inaccurate information from my credit report i was told that a supervisors would call me back on separate occasions to this day i have not received a call back from a supervisor i have sent numerous certified letters disputing the charge off that is currently being reported on my credit report and negatively effecting my credit i have asked for validation of my account including details explaining my why my account is being reported as a charge off with a balance i have asked for them to send me a copy of my payment history along with account notes the letters that i received in response to my letters have been a one paragraph response stating that they only report accurate information to the credit bureaus #&gt; 5 sent letter to credit reporting agencies telling them the inquiries on my report are unjustified the actions fail to comply with fcra section the letters are attached below credit agencies replied stating they did not need to investigate verify or remove inquiries credit agencies failed to prove requirements and fa iled to remove inquiries from my credit reports #&gt; 6 student loans have been discharged and i have a letter from the stating all loans have been discharged the credit bureaus refuse to accept the letter #&gt; Issue #&gt; 1 Disclosure verification of debt #&gt; 2 Problem with a credit reporting company&#39;s investigation into an existing problem #&gt; 3 Incorrect information on your report #&gt; 4 Problem with a credit reporting company&#39;s investigation into an existing problem #&gt; 5 Problem with a credit reporting company&#39;s investigation into an existing problem #&gt; 6 Incorrect information on your report #&gt; Product #&gt; 1 Debt collection #&gt; 2 Credit reporting, credit repair services, or other personal consumer reports #&gt; 3 Credit reporting, credit repair services, or other personal consumer reports #&gt; 4 Credit reporting, credit repair services, or other personal consumer reports #&gt; 5 Credit reporting, credit repair services, or other personal consumer reports #&gt; 6 Credit reporting, credit repair services, or other personal consumer reports text_dat_cor &lt;- data_clean_cor %&gt;% rowid_to_column(&quot;id&quot;) %&gt;% unnest_tokens(word,Consumer.complaint.narrative) %&gt;% anti_join(stop_words) Untuk memperoleh korelasi antar kata dapat menggunakan function pairwise_cor() dari package widyr words_cors &lt;- text_dat_cor %&gt;% group_by(word) %&gt;% filter(n() &gt; 100) %&gt;% pairwise_cor(word, Issue, sort = T) Korelasi antar kata dapat kita tampilkan secar visual menggunakan package ggraph. Pada visualisasi berikut kita hanya ingin menampilkan kata yang memiliki korelasi lebih dari 0.9. Artinya korelasi pada visualisasi berikut memiliki kecenderungan muncul bersamaan saat nasabah mengajukan complain. set.seed(100) words_cors %&gt;% filter(correlation &gt; .9) %&gt;% graph_from_data_frame() %&gt;% ggraph(layout = &quot;fr&quot;) + geom_edge_link(aes(edge_alpha = correlation)) + geom_node_point(color = &quot;lightblue&quot;, size = 5) + geom_node_text(aes(label = name), repel = TRUE) + theme_void() + ggtitle(&quot;Correlation between Words&quot;)+ theme(legend.position = &quot;none&quot;, plot.title = element_text(hjust = 0.5,face = &quot;bold&quot;)) Manfaat dari Sentiment Analysis yang telah dilakukan adalah kita dapat mengetahui pesan utama dari pendapat dan pemikiran customer terhadap suatu company atau product. Selain itu, output dari sentiment analysis dapat memberikan gambaran mengenai pelayanan atau product yang belum sesuai. Hal tersebut dapat membantu tim marketing untuk meneliti trend yang dibutuhkan customer dengan lebih baik. Seiring dengan peningkatan kualitas layanan dan pengembangan produk yang lebih baik, tentunya akan mengurangi tingkat churn customer. "],
["retail.html", "Chapter 4 Retail 4.1 E-Commerce Clothing Reviews 4.2 Customer Segmentation with RFM Analysis (in Python Programming Language)", " Chapter 4 Retail 4.1 E-Commerce Clothing Reviews 4.1.1 Background Perkembangan teknologi membuat pergeseran perilaku customer dari pembelian offline menjadi pembelian online atau melalui e-commerce. Perbedaan utama saat berbelanja secara online atau offline adalah saat akan berbelanja secara online, calon customer tidak dapat memeriksa barang yang akan dibeli secara langsung dan biasanya dibantuk oleh gambar atau deskripsi yang diberikan oleh penjual. Tentunya customer akan mencari informasi mengenai produk yang akan dibeli untuk meminimalisir dampak negatif yang didapat. Untuk membantu customer dalam menentukan product yang akan dibeli, mayoritas e-commerce sekarang ini menyediakan fitur online customer review, dimana online customer review ini dijadikan sebagai salah satu media customer mendapatkan informasi tentang produk dari customer yang telah membeli produk tersebut. Meningkatnya e-commerce di Indonesia, kebutuhan analisa mengenai online customer review dirasa perlu dilakukan untuk mendukung agar customer dapat memiliki pengalaman belanja online yang lebih baik daripada belanja offline. Salah satu implementasi data review customer tersebut dapat dimanfaatkan untuk membuat model yang dapat memprediksi apakah product tersebut direkomendasikan atau tidak direkomendasikan. Harapannya setelah perusahaan dapat menilai product mana yang direkomendasikan dan yang tidak direkomendasikan, dapat membantu perusahaan dalam pertimbangan penentuan top seller. Untuk seller yang memiliki banyak product yang direkomendasikan, dapat dijadikan sebagai top seller. reviews &lt;- read.csv(&quot;assets/03-retail/Womens Clothing E-Commerce Reviews.csv&quot;) head(reviews) #&gt; X Clothing.ID Age Title #&gt; 1 0 767 33 #&gt; 2 1 1080 34 #&gt; 3 2 1077 60 Some major design flaws #&gt; 4 3 1049 50 My favorite buy! #&gt; 5 4 847 47 Flattering shirt #&gt; 6 5 1080 49 Not for the very petite #&gt; Review.Text #&gt; 1 Absolutely wonderful - silky and sexy and comfortable #&gt; 2 Love this dress! it&#39;s sooo pretty. i happened to find it in a store, and i&#39;m glad i did bc i never would have ordered it online bc it&#39;s petite. i bought a petite and am 5&#39;8&quot;. i love the length on me- hits just a little below the knee. would definitely be a true midi on someone who is truly petite. #&gt; 3 I had such high hopes for this dress and really wanted it to work for me. i initially ordered the petite small (my usual size) but i found this to be outrageously small. so small in fact that i could not zip it up! i reordered it in petite medium, which was just ok. overall, the top half was comfortable and fit nicely, but the bottom half had a very tight under layer and several somewhat cheap (net) over layers. imo, a major design flaw was the net over layer sewn directly into the zipper - it c #&gt; 4 I love, love, love this jumpsuit. it&#39;s fun, flirty, and fabulous! every time i wear it, i get nothing but great compliments! #&gt; 5 This shirt is very flattering to all due to the adjustable front tie. it is the perfect length to wear with leggings and it is sleeveless so it pairs well with any cardigan. love this shirt!!! #&gt; 6 I love tracy reese dresses, but this one is not for the very petite. i am just under 5 feet tall and usually wear a 0p in this brand. this dress was very pretty out of the package but its a lot of dress. the skirt is long and very full so it overwhelmed my small frame. not a stranger to alterations, shortening and narrowing the skirt would take away from the embellishment of the garment. i love the color and the idea of the style but it just did not work on me. i returned this dress. #&gt; Rating Recommended.IND Positive.Feedback.Count Division.Name Department.Name Class.Name #&gt; 1 4 1 0 Initmates Intimate Intimates #&gt; 2 5 1 4 General Dresses Dresses #&gt; 3 3 0 0 General Dresses Dresses #&gt; 4 5 1 0 General Petite Bottoms Pants #&gt; 5 5 1 6 General Tops Blouses #&gt; 6 2 0 4 General Dresses Dresses Data yang digunakan merupakan data women e-commerce clothing reviews. Terdapat dua variabel yang menjadi fokus analisis ini yaitu Review.Text dan Recommended.IND. Variabel Review.Text merupakan review yang diberikan oleh customer terhadap product dari berbagai e-commerce, sedangkan Recommended.IND merupakan penilaian rekomendasi dari customer, 1 artinya product tersebut recommended dan 0 artinya product tersebut not recommended. Sebelum masuk cleaning data, kita ingin mengetahui proporsi dari target variabel: prop.table(table(reviews$Recommended.IND)) #&gt; #&gt; 0 1 #&gt; 0.1776377 0.8223623 4.1.2 Cleaning Data Untuk mengolah data text, kita perlu mengubah data teks dari vector menjadi corpus dengan function Vcorpus(). reviews_corpus &lt;- VCorpus(VectorSource(reviews$Review.Text)) reviews_corpus #&gt; &lt;&lt;VCorpus&gt;&gt; #&gt; Metadata: corpus specific: 0, document level (indexed): 0 #&gt; Content: documents: 23486 Selanjutnya, kita melakukan text cleansing dengan beberapa langkah sebagai berikut: tolower digunakan untuk mengubah semua karakter menjadi lowercase. removePunctuation digunakan untuk menghilangkan semua tanda baca. removeNumbers digunakan untuk menghilangkan semua angka. stopwords digunakan untuk menghilangkan kata-kata umum (am,and,or,if). stripWhitespace digunakan untuk menghapus karakter spasi yang berlebihan. data_clean &lt;- reviews_corpus %&gt;% tm_map(content_transformer(tolower)) %&gt;% tm_map(removePunctuation) %&gt;% tm_map(removeNumbers) %&gt;% tm_map(removeWords, stopwords(&quot;en&quot;)) %&gt;% tm_map(content_transformer(stripWhitespace)) inspect(data_clean[[1]]) #&gt; &lt;&lt;PlainTextDocument&gt;&gt; #&gt; Metadata: 7 #&gt; Content: chars: 43 #&gt; #&gt; absolutely wonderful silky sexy comfortable Setelah melakukan text cleansing, text tersebut akan diubah menjadi Document Term Matrix(DTM) melalui proses tokenization. Tokenization berfungsi memecah 1 teks atau kalimat menjadi beberapa term. Terim bisa berupa 1 kata, 2 kata, dan seterusnya. Pada format DTM, 1 kata akan menjadi 1 feature, secara default nilainya adalah jumlah kata pada dokumen tersebut. dtm_text &lt;- DocumentTermMatrix(data_clean) Sebelum membentuk model, tentunya kita perlu split data menjadi data train dan data test dengan proporsi 80:20. set.seed(100) idx &lt;- sample(nrow(dtm_text), nrow(dtm_text)*0.8) train &lt;- dtm_text[idx,] test &lt;- dtm_text[-idx,] train_label &lt;- reviews[idx,&quot;Recommended.IND&quot;] test_label &lt;- reviews[-idx,&quot;Recommended.IND&quot;] Term yang digunakan pada model ini, kita hanya mengambil term yang muncul paling sedikit 100 kali dari seluruh observasi dengan findFreqTerms(). freq &lt;- findFreqTerms(dtm_text, 100) train_r &lt;- train[, freq] test_r &lt;- test[, freq] inspect(train_r) #&gt; &lt;&lt;DocumentTermMatrix (documents: 18788, terms: 870)&gt;&gt; #&gt; Non-/sparse entries: 389603/15955957 #&gt; Sparsity : 98% #&gt; Maximal term length: 13 #&gt; Weighting : term frequency (tf) #&gt; Sample : #&gt; Terms #&gt; Docs dress fabric fit great just like love size top wear #&gt; 12348 0 1 0 1 0 1 0 1 0 1 #&gt; 12812 0 1 0 1 0 0 1 0 0 0 #&gt; 15905 0 1 2 1 0 1 0 2 0 1 #&gt; 1775 3 0 0 0 0 0 1 3 3 1 #&gt; 18527 0 1 0 1 1 0 0 0 2 0 #&gt; 19547 4 0 1 0 0 0 0 0 0 0 #&gt; 21091 0 0 0 0 0 1 0 2 0 1 #&gt; 22039 1 0 1 0 0 2 0 1 2 1 #&gt; 4789 1 0 2 0 0 1 0 1 0 1 #&gt; 6317 0 0 0 1 1 2 0 0 2 0 Nilai dari setiap matrix masih berupa angka numerik, dengan range 0-inf. Naive bayes akan memiliki performa lebih bagus ketika variabel numerik diubah menjadi kategorik. Salah satu caranya dengan Bernoulli Converter, yaitu jika jumlah kata yang muncul lebih dari 1, maka kita akan anggap nilainya adalah 1, jika 0 artinya tidak ada kata tersebut. bernoulli_conv &lt;- function(x){ x &lt;- as.factor(ifelse(x &gt; 0, 1, 0)) return(x) } train.bern &lt;- apply(train_r, MARGIN = 2, FUN = bernoulli_conv) test.bern &lt;- apply(test_r, MARGIN = 2, FUN = bernoulli_conv) 4.1.3 Modelling Selanjutnya, pembentukan model menggunakan naive bayes dan diikuti dengan prediksi data test. model.nb &lt;- naiveBayes(x = train.bern, y = as.factor(train_label), laplace = 1) pred.nb &lt;- predict(object = model.nb, newdata= test.bern) Dai hasil prediksi data test, kita akan menampilkan Confusion Matrix untuk mengetahui performa model. confusionMatrix(data = as.factor(pred.nb), reference = as.factor(test_label), positive = &quot;1&quot;) #&gt; Confusion Matrix and Statistics #&gt; #&gt; Reference #&gt; Prediction 0 1 #&gt; 0 647 428 #&gt; 1 220 3403 #&gt; #&gt; Accuracy : 0.8621 #&gt; 95% CI : (0.8519, 0.8718) #&gt; No Information Rate : 0.8155 #&gt; P-Value [Acc &gt; NIR] : &lt; 0.00000000000000022 #&gt; #&gt; Kappa : 0.5806 #&gt; #&gt; Mcnemar&#39;s Test P-Value : 0.0000000000000004232 #&gt; #&gt; Sensitivity : 0.8883 #&gt; Specificity : 0.7463 #&gt; Pos Pred Value : 0.9393 #&gt; Neg Pred Value : 0.6019 #&gt; Prevalence : 0.8155 #&gt; Detection Rate : 0.7244 #&gt; Detection Prevalence : 0.7712 #&gt; Balanced Accuracy : 0.8173 #&gt; #&gt; &#39;Positive&#39; Class : 1 #&gt; 4.1.4 Visualize Data Text Selanjutnya, kita akan coba lakukan prediksi terhadap data test dan juga menampilkan visualisasi text tersebut menggunakan package lime. set.seed(100) idx &lt;- sample(nrow(reviews), nrow(reviews)*0.8) train_lime &lt;- reviews[idx,] test_lime &lt;- reviews[-idx,] tokenize_text &lt;- function(text){ #create corpus data_corpus &lt;- VCorpus(VectorSource(text)) # cleansing data_clean &lt;- data_corpus %&gt;% tm_map(content_transformer(tolower)) %&gt;% tm_map(removePunctuation) %&gt;% tm_map(removeNumbers) %&gt;% tm_map(removeWords, stopwords(&quot;en&quot;)) %&gt;% tm_map(content_transformer(stripWhitespace)) #dtm dtm_text &lt;- DocumentTermMatrix(data_clean) #convert to bernoulli data_text &lt;- apply(dtm_text, MARGIN = 2, FUN = bernoulli_conv) return(data_text) } model_type.naiveBayes &lt;- function(x){ return(&quot;classification&quot;) } predict_model.naiveBayes &lt;- function(x, newdata, type = &quot;raw&quot;) { # return classification probabilities only res &lt;- predict(x, newdata, type = &quot;raw&quot;) %&gt;% as.data.frame() return(res) } text_train &lt;- train_lime$Review.Text %&gt;% as.character() explainer &lt;- lime(text_train, model = model.nb, preprocess = tokenize_text) text_test &lt;- test_lime$Review.Text %&gt;% as.character() set.seed(100) explanation &lt;- explain(text_test[5:10], explainer = explainer, n_labels =1, n_features = 50, single_explanation = F) plot_text_explanations(explanation) Dari hasil output observasi kedua terprediksi product tersebut recommended dengan probability 96.31% dan nilai explainer fit menunjukkan seberapa baik LIME dalam menginterpretasikan prediksi untuk observasi ini sebesar 0.89 artinya dapat dikatakan cukup akurat. Teks berlabel biru menunjukkan kata tersebut meningkatkan kemungkinan product tersebut untuk direkomendasikan, sedangkan teks berlabel merah berarti bahwa kata tersebut bertentangan/mengurangi kemungkinan product tersebut untuk direkomendasikan. 4.2 Customer Segmentation with RFM Analysis (in Python Programming Language) 4.2.1 Background Dalam transaksi jual beli, customer memiliki peran penting dalam eksistensi dan kemajuan sebuah industri. Oleh karenanya berbagai strategi marketing dilakukan untuk menarik perhatian customer baru atau untuk mempertahankan loyalitas customer. Cara yang paling umum dilakukan adalah pemberian diskon pada product tertentu atau pemberian free product untuk customer tertentu. Strategi marketing ini diterapkan sesuai dengan value yang dimiliki oleh customer. Beberapa value dapat dikategorikan menjadi low-value customer (customer dengan frekuensi transaksi rendah dan spend money rendah), medium-value customer (customer dengan frekuensi transaksi tinggi namun spend money rendah atau sebaliknya), dan high-value customer (customer dengan frekuensi transaksi tinggi dan spend money yang tinggi pula). Dalam melakukan segmentasi customer ada beberapa faktor yang harus dipertimbangkan. Faktor tersebut umumnya dianalisis berdasarkan data historical transaksi yang dimiliki oleh customer. Dari data historical tersebut dilakukan analisis lebih lanjut untuk mengetahui pattern data dan kemudian dilakukan modelling dengan bantuan algoritma machine learning agar menghasilkan output yang dapat dipertanggungjawabkan. Rangkaian proses ini nantinya diharapkan dapat menjawab beberapa pertanyaan bisnis seperti : Siapakah customer yang berpotensi untuk *churn*, Siapakah loyal customer, Siapakah potential customer, dan lain-lain. Metode segmentasi yang paling umum digunakan untuk melakukan segmentasi customer adalah RFM analysis. RFM akan melakukan segmentasi berdasarkan 3 poin penting yaitu : Recency : Waktu transaksi terakhir yang dilakukan customer Frequency : Banyak transaksi yang dilakukan oleh customer Monetary : Banyak uang yang dikeluarkan ketika melakukan transaksi Dalam artikel ini, akan dibahas lebih lanjut tentang proses segmentasi customer menggunakan metode RFM dengan bantuan machine learning clustering algorithm. Bahasa yang digunakan adalah bahasa pemrograman python. 4.2.2 Modelling Analysis Pada artikel ini data yang digunakan adalah data online retail di UK yang dapat ditemukan pada link berikut. Data ini adalah data transaksi yang terjadi pada 01/12/2010 sampai 09/12/2011. 4.2.2.1 Import Library and Read Data import pandas as pd import seaborn as sns import matplotlib.pyplot as plt ecom = pd.read_csv(&quot;assets/03-retail/data_ecom_uk.csv&quot;,encoding=&#39;latin1&#39;) ecom.head(2) #&gt; InvoiceNo StockCode ... CustomerID Country #&gt; 0 536365 85123A ... 17850.0 United Kingdom #&gt; 1 536365 71053 ... 17850.0 United Kingdom #&gt; #&gt; [2 rows x 8 columns] ecom.shape #&gt; (541909, 8) Dataframe ini mengandung 541909 observasi dengan jumlah kolom sebanyak 8 yang antara lain adalah : InvoiceNo : Nomor invoice yang terdiri dari 6 digit angka unik. Ketika InvoiceNo diawali dengan character C maka mengindikasikan cancellation transaction. StockCode : Kode product yang terdiri dari 5 digit angka unik. Description : Deskripsi nama product. Quantity : Jumlah product yang dibeli pada setiap transaksi. InvoiceDate : Tanggal transaksi berlangsung. UnitPrice : Harga satuan product. CustomerID : ID Customer yang berisi 5 digit angka unik dan berbeda pada setiap customer. Country : Nama negara. 4.2.2.2 Get only transaction in UK Dikarenakan terdapat beberapa data yang tidak berada pada country United Kingdom (UK), maka perlu dilakukan filter data hanya untuk country daerah UK. ecom_uk = ecom[ecom[&#39;Country&#39;]==&#39;United Kingdom&#39;] ecom_uk.shape #&gt; (495478, 8) ecom_uk.head(2) #&gt; InvoiceNo StockCode ... CustomerID Country #&gt; 0 536365 85123A ... 17850.0 United Kingdom #&gt; 1 536365 71053 ... 17850.0 United Kingdom #&gt; #&gt; [2 rows x 8 columns] 4.2.2.3 Handle Missing Values Missing value adalah masalah yang umum dihadapi ketika melakukan proses pengolahan data. Missing value terjadi ketika terdapat obeservasi kosong pada sebuah data. Pada hasil di bawah ini dapat diketahui informasi bahwa beberapa variable pada data menggandung nilai missing, variable tersebut antara lain adalah Description dan CustomerID. CustomerID adalah variable penting dalam RFM analisis, dikarenakan CustomerID mengandung informasi unik ID member. Sedangkan Description mengandung informasi terkait deskripsi produk. Jika ditelaah lebih jauh, untuk menangani missing values pada kedua variable tersebut dapat dilakukan dengan cara deletion, dikarenakan proses imputasi pada kedua variable tersebut akan menghasilkan informasi yang tidak akurat. ecom_uk.isna().sum() #&gt; InvoiceNo 0 #&gt; StockCode 0 #&gt; Description 1454 #&gt; Quantity 0 #&gt; InvoiceDate 0 #&gt; UnitPrice 0 #&gt; CustomerID 133600 #&gt; Country 0 #&gt; dtype: int64 Berikut ini adalah proses penghapusan missing values pada data : ecom_uk.dropna(inplace=True) #&gt; &lt;string&gt;:1: SettingWithCopyWarning: #&gt; A value is trying to be set on a copy of a slice from a DataFrame #&gt; #&gt; See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy 4.2.2.4 Select Unique Transaction Duplicated values atau duplikasi data adalah nilai berulang pada satu atau lebih observasi. Untuk menangani data yang duplikat dapat dilakukan penghapusan dan hanya mempertahankan salah satu observasi. ecom_uk.drop_duplicates(subset=[&#39;InvoiceNo&#39;, &#39;CustomerID&#39;], keep=&quot;first&quot;, inplace=True) 4.2.2.5 Change Data Types Dalam pengolahan data transformasi tipe data pada format yang sesuai sangat penting untuk dilakukan, hal ini agar nantinya data-data tersebut siap untuk dilakukan manipulasi lebih lanjut. ecom_uk.dtypes #&gt; InvoiceNo object #&gt; StockCode object #&gt; Description object #&gt; Quantity int64 #&gt; InvoiceDate object #&gt; UnitPrice float64 #&gt; CustomerID float64 #&gt; Country object #&gt; dtype: object ecom_uk[&#39;InvoiceDate&#39;] = pd.to_datetime(ecom_uk[&#39;InvoiceDate&#39;]) #&gt; &lt;string&gt;:1: SettingWithCopyWarning: #&gt; A value is trying to be set on a copy of a slice from a DataFrame. #&gt; Try using .loc[row_indexer,col_indexer] = value instead #&gt; #&gt; See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy ecom_uk[&#39;Country&#39;] = ecom_uk[&#39;Country&#39;].astype(&#39;category&#39;) ecom_uk[&#39;CustomerID&#39;] = ecom_uk[&#39;CustomerID&#39;].astype(&#39;int64&#39;) 4.2.2.6 Drop cancelled transaction Karakter pertama “C” pada InvoiceNo menunjukkan bahwa customer melakukan pembatalan terhadap transaksi yang dilakukan. Sehingga data akan kurang relevan jika tetap dipertahankan, maka dari itu perlu dilakukan penghapusan pada observasi tersebut. ecom_uk = ecom_uk.loc[~ecom_uk.iloc[:,0].str.contains(r&#39;C&#39;)] ecom_uk.head() #&gt; InvoiceNo StockCode ... CustomerID Country #&gt; 0 536365 85123A ... 17850 United Kingdom #&gt; 7 536366 22633 ... 17850 United Kingdom #&gt; 9 536367 84879 ... 13047 United Kingdom #&gt; 21 536368 22960 ... 13047 United Kingdom #&gt; 25 536369 21756 ... 13047 United Kingdom #&gt; #&gt; [5 rows x 8 columns] 4.2.3 Exploratory Data Analysis Tahapan Exploratory Data Analysis digunakan untuk mengetahui pattern dari data. 4.2.3.1 Recency Recency adalah faktor yang menyimpan informasi tentang berapa lama sejak customer melakukan pembelian. Untuk melakukan perhitungan recency pada masing-masing customer dapat dilakukan dengan cara memanipulasi tanggal transaksi customer dan kemudian dikurangi dengan tanggal maksimum yang terdapat pada data. Berikut di bawah ini adalah detail langkah-langkahnya : Manipulasi tanggal transaksi dengan mengekstrak informasi tanggal, bulan dan tahun transaksi. ecom_uk[&#39;Date&#39;] = ecom_uk[&#39;InvoiceDate&#39;].dt.date ecom_uk.head(2) #&gt; InvoiceNo StockCode ... Country Date #&gt; 0 536365 85123A ... United Kingdom 2010-12-01 #&gt; 7 536366 22633 ... United Kingdom 2010-12-01 #&gt; #&gt; [2 rows x 9 columns] Mengambil tanggal transaksi maksimum pada keseluruhan observasi last_trans = ecom_uk[&#39;Date&#39;].max() last_trans #&gt; datetime.date(2011, 12, 9) Mengekstrak informasi tanggal transaksi maksimum pada tiap customer. recent = ecom_uk.groupby(by=[&#39;CustomerID&#39;], as_index=False)[&#39;Date&#39;].max() recent.columns = [&#39;CustomerID&#39;,&#39;Last Transaction&#39;] recent.head() #&gt; CustomerID Last Transaction #&gt; 0 12346 2011-01-18 #&gt; 1 12747 2011-12-07 #&gt; 2 12748 2011-12-09 #&gt; 3 12749 2011-12-06 #&gt; 4 12820 2011-12-06 Menghitung selisih tanggal transaksi maksimum dengan tanggal transaksi terakhir pada tiap customer, kemudian menyimpan jumlah hari pada kolom Days Recent. recent[&#39;Days Recent&#39;] = last_trans - recent[&#39;Last Transaction&#39;] recent[&#39;Days Recent&#39;] = recent[&#39;Days Recent&#39;].dt.days recent.head() #&gt; CustomerID Last Transaction Days Recent #&gt; 0 12346 2011-01-18 325 #&gt; 1 12747 2011-12-07 2 #&gt; 2 12748 2011-12-09 0 #&gt; 3 12749 2011-12-06 3 #&gt; 4 12820 2011-12-06 3 recent.drop(columns=[&#39;Last Transaction&#39;], inplace=True) 4.2.3.2 Frequency Frequency mengandung infromasi tentang seberapa sering customer melakukan transaksi pembelian dalam kurun waktu tertentu. Nilai frequency dapat diperoleh dengan cara menghitung jumlah transkasi pada setiap unik customer. temp = ecom_uk[[&#39;CustomerID&#39;,&#39;InvoiceNo&#39;]] trans_cust = temp.groupby(by=[&#39;CustomerID&#39;]).count() trans_cust.rename(columns={&#39;InvoiceNo&#39;:&#39;Number of Transaction&#39;}) #&gt; Number of Transaction #&gt; CustomerID #&gt; 12346 1 #&gt; 12747 11 #&gt; 12748 210 #&gt; 12749 5 #&gt; 12820 4 #&gt; ... ... #&gt; 18280 1 #&gt; 18281 1 #&gt; 18282 2 #&gt; 18283 16 #&gt; 18287 3 #&gt; #&gt; [3921 rows x 1 columns] trans_cust.reset_index() #&gt; CustomerID InvoiceNo #&gt; 0 12346 1 #&gt; 1 12747 11 #&gt; 2 12748 210 #&gt; 3 12749 5 #&gt; 4 12820 4 #&gt; ... ... ... #&gt; 3916 18280 1 #&gt; 3917 18281 1 #&gt; 3918 18282 2 #&gt; 3919 18283 16 #&gt; 3920 18287 3 #&gt; #&gt; [3921 rows x 2 columns] Ouptut di atas menunjukkan jumlah transaksi yang dilakukan pada masing-masing customer. CustomerID 12346 melakukan transaksi sebanyak 1 kali saja, CustomerID 12747 melakukan transaksi sebanyak 11 kali, dan seterusnya. Berikut dibawah ini adalah detail informasi InvoiceNo pada setiap transaksi yang dilakukan oleh customer. table_trans_details = temp.groupby(by=[&#39;CustomerID&#39;,&#39;InvoiceNo&#39;]).count() table_trans_details.head() #&gt; Empty DataFrame #&gt; Columns: [] #&gt; Index: [(12346, 541431), (12747, 537215), (12747, 538537), (12747, 541677), (12747, 545321)] 4.2.3.3 Monetary Monetary adalah faktor yang menyimpan jumlah pengeluaran customer dalam transaksi. Nilai monetary dapat dihitung dari harga barang yang dibeli oleh masing-masing customer pada transaksi tertentu dan kemudian dikalkulasikan dengan jumlah barang yang dibeli. ecom_uk[&#39;Total&#39;] = ecom_uk[&#39;UnitPrice&#39;] * ecom_uk[&#39;Quantity&#39;] ecom_uk.head(2) #&gt; InvoiceNo StockCode ... Date Total #&gt; 0 536365 85123A ... 2010-12-01 15.3 #&gt; 7 536366 22633 ... 2010-12-01 11.1 #&gt; #&gt; [2 rows x 10 columns] monetary = ecom_uk.groupby(by=[&#39;CustomerID&#39;], as_index=False)[&#39;Total&#39;].sum() monetary #&gt; CustomerID Total #&gt; 0 12346 77183.60 #&gt; 1 12747 689.49 #&gt; 2 12748 3841.31 #&gt; 3 12749 98.35 #&gt; 4 12820 58.20 #&gt; ... ... ... #&gt; 3916 18280 23.70 #&gt; 3917 18281 5.04 #&gt; 3918 18282 38.25 #&gt; 3919 18283 66.75 #&gt; 3920 18287 80.40 #&gt; #&gt; [3921 rows x 2 columns] 4.2.3.4 Merge Column based on CustomerID Setelah mendapatkan informasi pada setiap faktor penting, langkah selanjutnya adalah menyimpannya kedalam sebuah dataframe baru. new_ = monetary.merge(trans_cust,on=&#39;CustomerID&#39;) new_data = new_.merge(recent,on=&#39;CustomerID&#39;) new_data.rename(columns={&#39;Total&#39;:&#39;Monetary&#39;,&#39;InvoiceNo&#39;:&#39;Frequency&#39;,&#39;Days Recent&#39;:&#39;Recency&#39;}, inplace=True) new_data.head() #&gt; CustomerID Monetary Frequency Recency #&gt; 0 12346 77183.60 1 325 #&gt; 1 12747 689.49 11 2 #&gt; 2 12748 3841.31 210 0 #&gt; 3 12749 98.35 5 3 #&gt; 4 12820 58.20 4 3 4.2.4 Modelling 4.2.4.1 Clustering Recency, Frequency, and Monetary Proses clustering bertujuan untuk membagi level customer kedalam beberapa segment tertentu meliputi low-value customer, medium-value customer or high-value customer. 4.2.4.2 Recency Pada faktor Recency, customer yang memiliki recent trasaksi akan di kategorikan pada high-value customer. Kenapa? Karena customer tersebut berpotensi untuk melakukan pembelian lagi dibanding dengan customer yang sudah lama tidak melakukan pembelian. new_data[&#39;Recency&#39;].describe() #&gt; count 3921.000000 #&gt; mean 91.722265 #&gt; std 99.528532 #&gt; min 0.000000 #&gt; 25% 17.000000 #&gt; 50% 50.000000 #&gt; 75% 142.000000 #&gt; max 373.000000 #&gt; Name: Recency, dtype: float64 Teknik elbow mwthod untuk menentukan jumlah cluster yang terbentuk. from sklearn.cluster import KMeans sse={} recency = new_data[[&#39;Recency&#39;]] for k in range(1, 10): kmeans = KMeans(n_clusters=k, max_iter=1000).fit(recency) recency[&quot;clusters&quot;] = kmeans.labels_ sse[k] = kmeans.inertia_ #&gt; &lt;string&gt;:3: SettingWithCopyWarning: #&gt; A value is trying to be set on a copy of a slice from a DataFrame. #&gt; Try using .loc[row_indexer,col_indexer] = value instead #&gt; #&gt; See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy #&gt; &lt;string&gt;:3: SettingWithCopyWarning: #&gt; A value is trying to be set on a copy of a slice from a DataFrame. #&gt; Try using .loc[row_indexer,col_indexer] = value instead #&gt; #&gt; See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy #&gt; &lt;string&gt;:3: SettingWithCopyWarning: #&gt; A value is trying to be set on a copy of a slice from a DataFrame. #&gt; Try using .loc[row_indexer,col_indexer] = value instead #&gt; #&gt; See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy #&gt; &lt;string&gt;:3: SettingWithCopyWarning: #&gt; A value is trying to be set on a copy of a slice from a DataFrame. #&gt; Try using .loc[row_indexer,col_indexer] = value instead #&gt; #&gt; See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy #&gt; &lt;string&gt;:3: SettingWithCopyWarning: #&gt; A value is trying to be set on a copy of a slice from a DataFrame. #&gt; Try using .loc[row_indexer,col_indexer] = value instead #&gt; #&gt; See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy #&gt; &lt;string&gt;:3: SettingWithCopyWarning: #&gt; A value is trying to be set on a copy of a slice from a DataFrame. #&gt; Try using .loc[row_indexer,col_indexer] = value instead #&gt; #&gt; See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy #&gt; &lt;string&gt;:3: SettingWithCopyWarning: #&gt; A value is trying to be set on a copy of a slice from a DataFrame. #&gt; Try using .loc[row_indexer,col_indexer] = value instead #&gt; #&gt; See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy #&gt; &lt;string&gt;:3: SettingWithCopyWarning: #&gt; A value is trying to be set on a copy of a slice from a DataFrame. #&gt; Try using .loc[row_indexer,col_indexer] = value instead #&gt; #&gt; See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy #&gt; &lt;string&gt;:3: SettingWithCopyWarning: #&gt; A value is trying to be set on a copy of a slice from a DataFrame. #&gt; Try using .loc[row_indexer,col_indexer] = value instead #&gt; #&gt; See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy plt.figure() #&gt; &lt;Figure size 700x500 with 0 Axes&gt; plt.plot(list(sse.keys()), list(sse.values())) #&gt; [&lt;matplotlib.lines.Line2D object at 0x000001F2B25781C0&gt;] plt.xlabel(&quot;Number of cluster&quot;) #&gt; Text(0.5, 0, &#39;Number of cluster&#39;) plt.show() kmeans = KMeans(n_clusters=3) kmeans.fit(new_data[[&#39;Recency&#39;]]) #&gt; KMeans(n_clusters=3) new_data[&#39;RecencyCluster&#39;] = kmeans.predict(new_data[[&#39;Recency&#39;]]) new_data.groupby(&#39;RecencyCluster&#39;)[&#39;Recency&#39;].describe() #&gt; count mean std ... 50% 75% max #&gt; RecencyCluster ... #&gt; 0 723.0 154.673582 37.878377 ... 155.0 186.00 222.0 #&gt; 1 558.0 293.175627 45.309128 ... 283.5 325.75 373.0 #&gt; 2 2640.0 31.902273 25.336528 ... 25.0 51.00 93.0 #&gt; #&gt; [3 rows x 8 columns] Berdasarkan visualisasi grafik elbow, maka jumlah cluster ideal yang dapat dibentuk adalah sebanyak 3 cluster. Pada hasil di atas menunjukkan bahwa cluster 1 mengandung informasi customer yang melakukan transaksi paling baru (most recent) sedangkan cluster 0 mengandung informasi customer yang sudah lama tidak melakukan transaksi pembelian. Untuk keperluan standarisasi, maka perlu dilakukan re-order cluster sehingga cluster 0 akan memuat informasi low-value customer, cluster 1 medium-value customer dan cluster 2 high-value customer. Dikarenakan step ini adalah step Recency, maka cluster yang memiliki nilai recency rendah akan dikategorikan pada cluster 2. Dibawah ini adalah fungsi untuk melakukan reorder cluster : #function for ordering cluster numbers def order_cluster(cluster_field_name, target_field_name,df,ascending): new_cluster_field_name = &#39;new_&#39; + cluster_field_name df_new = df.groupby(cluster_field_name)[target_field_name].mean().reset_index() df_new = df_new.sort_values(by=target_field_name,ascending=ascending).reset_index(drop=True) df_new[&#39;index&#39;] = df_new.index df_final = pd.merge(df,df_new[[cluster_field_name,&#39;index&#39;]], on=cluster_field_name) df_final = df_final.drop([cluster_field_name],axis=1) df_final = df_final.rename(columns={&quot;index&quot;:cluster_field_name}) return df_final new_data = order_cluster(&#39;RecencyCluster&#39;, &#39;Recency&#39;,new_data,False) 4.2.4.3 Frequency Factor penting selanjutnya adalah Frequency. Pada step frequency, customer yang memiliki banyak transaksi pembelian akan dikategorikan pada level high-value customer. new_data[&#39;Frequency&#39;].describe() #&gt; count 3921.000000 #&gt; mean 4.246111 #&gt; std 7.205750 #&gt; min 1.000000 #&gt; 25% 1.000000 #&gt; 50% 2.000000 #&gt; 75% 5.000000 #&gt; max 210.000000 #&gt; Name: Frequency, dtype: float64 sse={} frequency = new_data[[&#39;Frequency&#39;]] for k in range(1, 10): kmeans = KMeans(n_clusters=k, max_iter=1000).fit(frequency) frequency[&quot;clusters&quot;] = kmeans.labels_ sse[k] = kmeans.inertia_ #&gt; &lt;string&gt;:3: SettingWithCopyWarning: #&gt; A value is trying to be set on a copy of a slice from a DataFrame. #&gt; Try using .loc[row_indexer,col_indexer] = value instead #&gt; #&gt; See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy #&gt; &lt;string&gt;:3: SettingWithCopyWarning: #&gt; A value is trying to be set on a copy of a slice from a DataFrame. #&gt; Try using .loc[row_indexer,col_indexer] = value instead #&gt; #&gt; See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy #&gt; &lt;string&gt;:3: SettingWithCopyWarning: #&gt; A value is trying to be set on a copy of a slice from a DataFrame. #&gt; Try using .loc[row_indexer,col_indexer] = value instead #&gt; #&gt; See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy #&gt; &lt;string&gt;:3: SettingWithCopyWarning: #&gt; A value is trying to be set on a copy of a slice from a DataFrame. #&gt; Try using .loc[row_indexer,col_indexer] = value instead #&gt; #&gt; See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy #&gt; &lt;string&gt;:3: SettingWithCopyWarning: #&gt; A value is trying to be set on a copy of a slice from a DataFrame. #&gt; Try using .loc[row_indexer,col_indexer] = value instead #&gt; #&gt; See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy #&gt; &lt;string&gt;:3: SettingWithCopyWarning: #&gt; A value is trying to be set on a copy of a slice from a DataFrame. #&gt; Try using .loc[row_indexer,col_indexer] = value instead #&gt; #&gt; See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy #&gt; &lt;string&gt;:3: SettingWithCopyWarning: #&gt; A value is trying to be set on a copy of a slice from a DataFrame. #&gt; Try using .loc[row_indexer,col_indexer] = value instead #&gt; #&gt; See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy #&gt; &lt;string&gt;:3: SettingWithCopyWarning: #&gt; A value is trying to be set on a copy of a slice from a DataFrame. #&gt; Try using .loc[row_indexer,col_indexer] = value instead #&gt; #&gt; See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy #&gt; &lt;string&gt;:3: SettingWithCopyWarning: #&gt; A value is trying to be set on a copy of a slice from a DataFrame. #&gt; Try using .loc[row_indexer,col_indexer] = value instead #&gt; #&gt; See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy plt.figure() #&gt; &lt;Figure size 700x500 with 0 Axes&gt; plt.plot(list(sse.keys()), list(sse.values())) #&gt; [&lt;matplotlib.lines.Line2D object at 0x000001F2B26179D0&gt;] plt.xlabel(&quot;Number of cluster&quot;) #&gt; Text(0.5, 0, &#39;Number of cluster&#39;) plt.show() kmeans = KMeans(n_clusters=3) kmeans.fit(new_data[[&#39;Frequency&#39;]]) #&gt; KMeans(n_clusters=3) new_data[&#39;FrequencyCluster&#39;] = kmeans.predict(new_data[[&#39;Frequency&#39;]]) new_data.groupby(&#39;FrequencyCluster&#39;)[&#39;Frequency&#39;].describe() #&gt; count mean std ... 50% 75% max #&gt; FrequencyCluster ... #&gt; 0 3729.0 3.130598 2.672453 ... 2.0 4.00 13.0 #&gt; 1 186.0 22.978495 10.302955 ... 19.0 26.00 63.0 #&gt; 2 6.0 116.833333 47.562240 ... 95.0 117.25 210.0 #&gt; #&gt; [3 rows x 8 columns] Sama halnya dengan tahapan pada step Recency, pada step ini juga perlu dilakukan standarisasi cluster dengan melakukan reorder pada cluster. Sehingga cluster 0 dengan nilai frequency yang rendah akan dikategorikan pada level low-value customer sedangkan cluster 2 dengan nilai frequency tinggi akan dikategorikan pada level high-values customer. new_data = order_cluster(&#39;FrequencyCluster&#39;, &#39;Frequency&#39;,new_data,True) 4.2.4.4 Monetary Faktor penting terakhir pada RFM analysis adalah Monetary. Customer dengan nilai monetary yang tinggi akan dikategorikan pada level high-value customer dikarenakan berkontribusi besar dalam pendapatan yang dihasilkan industry. new_data[&#39;Monetary&#39;].describe() #&gt; count 3921.000000 #&gt; mean 293.299913 #&gt; std 3261.756525 #&gt; min 0.000000 #&gt; 25% 17.700000 #&gt; 50% 45.400000 #&gt; 75% 124.500000 #&gt; max 168471.250000 #&gt; Name: Monetary, dtype: float64 sse={} monetary_ = new_data[[&#39;Monetary&#39;]] for k in range(1, 10): kmeans = KMeans(n_clusters=k, max_iter=1000).fit(monetary_) monetary_[&quot;clusters&quot;] = kmeans.labels_ sse[k] = kmeans.inertia_ #&gt; &lt;string&gt;:3: SettingWithCopyWarning: #&gt; A value is trying to be set on a copy of a slice from a DataFrame. #&gt; Try using .loc[row_indexer,col_indexer] = value instead #&gt; #&gt; See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy #&gt; &lt;string&gt;:3: SettingWithCopyWarning: #&gt; A value is trying to be set on a copy of a slice from a DataFrame. #&gt; Try using .loc[row_indexer,col_indexer] = value instead #&gt; #&gt; See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy #&gt; &lt;string&gt;:3: SettingWithCopyWarning: #&gt; A value is trying to be set on a copy of a slice from a DataFrame. #&gt; Try using .loc[row_indexer,col_indexer] = value instead #&gt; #&gt; See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy #&gt; &lt;string&gt;:3: SettingWithCopyWarning: #&gt; A value is trying to be set on a copy of a slice from a DataFrame. #&gt; Try using .loc[row_indexer,col_indexer] = value instead #&gt; #&gt; See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy #&gt; &lt;string&gt;:3: SettingWithCopyWarning: #&gt; A value is trying to be set on a copy of a slice from a DataFrame. #&gt; Try using .loc[row_indexer,col_indexer] = value instead #&gt; #&gt; See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy #&gt; &lt;string&gt;:3: SettingWithCopyWarning: #&gt; A value is trying to be set on a copy of a slice from a DataFrame. #&gt; Try using .loc[row_indexer,col_indexer] = value instead #&gt; #&gt; See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy #&gt; &lt;string&gt;:3: SettingWithCopyWarning: #&gt; A value is trying to be set on a copy of a slice from a DataFrame. #&gt; Try using .loc[row_indexer,col_indexer] = value instead #&gt; #&gt; See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy #&gt; &lt;string&gt;:3: SettingWithCopyWarning: #&gt; A value is trying to be set on a copy of a slice from a DataFrame. #&gt; Try using .loc[row_indexer,col_indexer] = value instead #&gt; #&gt; See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy #&gt; &lt;string&gt;:3: SettingWithCopyWarning: #&gt; A value is trying to be set on a copy of a slice from a DataFrame. #&gt; Try using .loc[row_indexer,col_indexer] = value instead #&gt; #&gt; See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy plt.figure() #&gt; &lt;Figure size 700x500 with 0 Axes&gt; plt.plot(list(sse.keys()), list(sse.values())) #&gt; [&lt;matplotlib.lines.Line2D object at 0x000001F2B25EAD30&gt;] plt.xlabel(&quot;Number of cluster&quot;) #&gt; Text(0.5, 0, &#39;Number of cluster&#39;) plt.show() kmeans = KMeans(n_clusters=3) kmeans.fit(new_data[[&#39;Monetary&#39;]]) #&gt; KMeans(n_clusters=3) new_data[&#39;MonetaryCluster&#39;] = kmeans.predict(new_data[[&#39;Monetary&#39;]]) new_data.groupby(&#39;MonetaryCluster&#39;)[&#39;Monetary&#39;].describe() #&gt; count mean ... 75% max #&gt; MonetaryCluster ... #&gt; 0 6.0 40051.213333 ... 41519.325 77183.60 #&gt; 1 1.0 168471.250000 ... 168471.250 168471.25 #&gt; 2 3914.0 189.384371 ... 123.200 17895.28 #&gt; #&gt; [3 rows x 8 columns] Reorder cluster untuk standarisasi cluster sehingga cluster 0 dengan nilai monetary rendah akan dikategorikan dalam low-value customer sedangkan cluster 2 dengan nilai monetary tinggi akan dikelompokkan pada high-values customer. new_data = order_cluster(&#39;MonetaryCluster&#39;, &#39;Monetary&#39;,new_data,True) 4.2.4.5 Segmentation Customer based on Cluster Setelah memperoleh nilai cluster terurut pada setiap observasi data, langkah selanjutnya adalah memberikan label pada masing-masing observasi. Label ini bertujuan untuk mengidentifikasi level pada masing-masing customer apakah tergolong pada low-value customer, medium-value customer atau high-value customer. Proses pelabelan terdiri dari beberapa tahapan yang antara lain adalah : new_data.head() #&gt; CustomerID Monetary ... FrequencyCluster MonetaryCluster #&gt; 0 12346 77183.60 ... 0 1 #&gt; 1 15098 39916.50 ... 0 1 #&gt; 2 16029 24384.92 ... 1 1 #&gt; 3 17450 26768.97 ... 1 1 #&gt; 4 17949 29999.69 ... 1 1 #&gt; #&gt; [5 rows x 7 columns] Menghitung score pada masing-masing observasi dengan melakukan penjumlahan pada nilai cluster. new_data[&#39;Score&#39;] = new_data[&#39;RecencyCluster&#39;] + new_data[&#39;FrequencyCluster&#39;] + new_data[&#39;MonetaryCluster&#39;] new_data.head(2) #&gt; CustomerID Monetary Frequency ... FrequencyCluster MonetaryCluster Score #&gt; 0 12346 77183.6 1 ... 0 1 1 #&gt; 1 15098 39916.5 3 ... 0 1 2 #&gt; #&gt; [2 rows x 8 columns] print(new_data[&#39;Score&#39;].min()) #&gt; 0 print(new_data[&#39;Score&#39;].max()) #&gt; 4 Dari hasil di atas diperoleh informasi bahwa minimum score pada data adalah 0, sedangkan maksimum value pada data adalah 4. Sehingga untuk segmentasi label dapat dikategorikan berdasarkan ketentuan berikut : Customer dengan score &lt;= 1 akan masuk dalam kategori low-value customer Customer dengan score &lt;= 3 akan masuk dalam kategori medium-value customer Customer dengan score &gt; 3 akan masuk dalam kategori high-value customer label = [] def label_(data) : if data &lt;= 1 : lab = &quot;Low&quot; elif data &lt;= 3 : lab = &quot;Medium&quot; else : lab = &quot;High&quot; label.append(lab) new_data[&#39;Score&#39;].apply(label_) #&gt; 0 None #&gt; 1 None #&gt; 2 None #&gt; 3 None #&gt; 4 None #&gt; ... #&gt; 3916 None #&gt; 3917 None #&gt; 3918 None #&gt; 3919 None #&gt; 3920 None #&gt; Name: Score, Length: 3921, dtype: object new_data[&#39;Label&#39;] = label new_data.head(2) #&gt; CustomerID Monetary Frequency ... MonetaryCluster Score Label #&gt; 0 12346 77183.6 1 ... 1 1 Low #&gt; 1 15098 39916.5 3 ... 1 2 Medium #&gt; #&gt; [2 rows x 9 columns] 4.2.5 Customer’s behavior in each factor based on their label Setelah memberikan label pada masing-masing customer, apakah sudah cukup membantu untuk tim management dalam menentukan strategi marketing yang tepat? Jawabannya dapat Ya atau Tidak. Tidak dikarenakan management perlu untuk mengetahui informasi detail dari behavior (kebiasaan) customer pada setiap level dalam melakukan pembelanjaan. Oleh karena itu, sebelum melangkah lebih jauh, terlebih dahulu lakukan behavior analisis sebagai berikut : import numpy as np def neg_to_zero(x): if x &lt;= 0: return 1 else: return x new_data[&#39;Recency&#39;] = [neg_to_zero(x) for x in new_data.Recency] new_data[&#39;Monetary&#39;] = [neg_to_zero(x) for x in new_data.Monetary] rfm_log = new_data[[&#39;Recency&#39;, &#39;Frequency&#39;, &#39;Monetary&#39;]].apply(np.log, axis = 1).round(3) from sklearn.preprocessing import StandardScaler scaler = StandardScaler() rfm_scaled = scaler.fit_transform(rfm_log) rfm_scaled = pd.DataFrame(rfm_scaled, index = new_data.index, columns = rfm_log.columns) rfm_scaled.head() #&gt; Recency Frequency Monetary #&gt; 0 1.389971 -1.049966 4.788147 #&gt; 1 0.996792 0.170733 4.359157 #&gt; 2 -0.064790 3.551812 4.038228 #&gt; 3 -1.121628 3.203041 4.098768 #&gt; 4 -2.530970 3.178605 4.172979 rfm_scaled[&#39;Label&#39;] = new_data.Label rfm_scaled[&#39;CustomerID&#39;] = new_data.CustomerID rfm_scaled #&gt; Recency Frequency Monetary Label CustomerID #&gt; 0 1.389971 -1.049966 4.788147 Low 12346 #&gt; 1 0.996792 0.170733 4.359157 Medium 15098 #&gt; 2 -0.064790 3.551812 4.038228 High 16029 #&gt; 3 -1.121628 3.203041 4.098768 High 17450 #&gt; 4 -2.530970 3.178605 4.172979 High 17949 #&gt; ... ... ... ... ... ... #&gt; 3916 -2.061189 4.031650 3.055261 High 13089 #&gt; 3917 -2.530970 3.984999 1.973998 High 14606 #&gt; 3918 -2.530970 3.960563 2.967380 High 15311 #&gt; 3919 -2.530970 4.303780 2.165383 High 17841 #&gt; 3920 -2.530970 -0.280226 5.296556 High 16446 #&gt; #&gt; [3921 rows x 5 columns] rfm_melted = pd.melt(frame= rfm_scaled, id_vars= [&#39;CustomerID&#39;, &#39;Label&#39;], \\ var_name = &#39;Metrics&#39;, value_name = &#39;Value&#39;) rfm_melted #&gt; CustomerID Label Metrics Value #&gt; 0 12346 Low Recency 1.389971 #&gt; 1 15098 Medium Recency 0.996792 #&gt; 2 16029 High Recency -0.064790 #&gt; 3 17450 High Recency -1.121628 #&gt; 4 17949 High Recency -2.530970 #&gt; ... ... ... ... ... #&gt; 11758 13089 High Monetary 3.055261 #&gt; 11759 14606 High Monetary 1.973998 #&gt; 11760 15311 High Monetary 2.967380 #&gt; 11761 17841 High Monetary 2.165383 #&gt; 11762 16446 High Monetary 5.296556 #&gt; #&gt; [11763 rows x 4 columns] Visualisasi behavior customer pada setiap level. import seaborn as sns # a snake plot with RFM sns.lineplot(x = &#39;Metrics&#39;, y = &#39;Value&#39;, hue = &#39;Label&#39;, data = rfm_melted) #&gt; &lt;AxesSubplot:xlabel=&#39;Metrics&#39;, ylabel=&#39;Value&#39;&gt; plt.title(&#39;Customer Behavior based on their Label&#39;) #&gt; Text(0.5, 1.0, &#39;Customer Behavior based on their Label&#39;) plt.legend(loc = &#39;upper right&#39;) #&gt; &lt;matplotlib.legend.Legend object at 0x000001F2B2802670&gt; Berdasarkan visualisasi di atas diperoleh detail informasi bahwa : Customer dengan high-value labels memiliki kecenderungan untuk menghabiskan banyak uang dalam berbelanja (high monetary) dan sering melakukan pembelanjaan (high frequency) Customer dengan medium-value labels tidak terlalu sering melakukan pembelian dan juga tidak banyak menghabiskan uang selama transaksi. Customer dengan low-value labels hanya menghabiskan sedikit uang selama berbelanja, tidak terlalu sering berbelanja, tetapi memiliki nilai recency yang cukup tinggi dibandingkan level lainnya. Berdasarkan rules di atas, pihak management dapat mempertimbangkan melakukan strategi marketing dengan cara : Memberikan special promotion atau discount untuk low-value customer yang baru-baru saja berkunjung untuk berbelanja, sehingga mereka tertarik untuk berbelanja lagi di lain waktu. Mempertahankan medium-value customer dengan cara memberikan cashback pada pembeliannya. Memberikan reward pada loyal customer (high-value) dengan cara memberikan free product atau cashback pada pembelanjaannya. 4.2.6 Conclusion RFM analysis adalah teknik yang umum digunakan untuk melakukan segmentasi terhadap customer berdasarkan value dan behavior selama bertransaksi. Teknik ini sangat membantu pihak management khususnya marketing team dalam menentukan strategi bisnis yang cocok untuk mempertahankan loyal customer dan menarik customer baru. "],
["insurance.html", "Chapter 5 Insurance 5.1 Prediction of Total Claim Amount", " Chapter 5 Insurance body { text-align: justify} 5.1 Prediction of Total Claim Amount 5.1.1 Background Seiring tingkat kompetisi yang semakin tinggi di industri asuransi, perusahaan dituntut untuk selalu memberikan terobosan dan strategi untuk memberikan layanan yang terbaik untuk nasabahnya. Salah satu aset utama perusahaan asuransi tentunya adalah data nasabah dan riwayat polis. Tentunya dengan adanya data yang dimiliki oleh perusahaan, dapat dimanfaatkan dalam upaya pengambilan keputusan strategis. Perusahaan memiliki kebutuhan untuk memperhitungkan pembayaran klaim di masa depan. Tanggung jawab tersebut biasa dikenal sebagai cadangan klaim. Karena cadangkan klaim adalah kewajiban yang harus dipersiapkan untuk masa yang akan datang, nilai pastinya tidak diketahui dan harus diperkirakan. Risiko yang dimiliki oleh setiap nasabah tentunya bervariasi, faktor-faktor yang berhubungan dengan risiko tentunya membantu dalam memprediksi biaya klaim yang harus dibayarkan. Tujuan dari analysis ini adalah untuk memprediksi besarnya klaim yang harus diberikan oleh perusahaan untuk setiap nasabahnya, hasil prediksi diperoleh dengan mempelajari karakteristik dan profil dari nasabah tersebut. 5.1.2 Modelling Analysis 5.1.2.1 Import Data Data yang digunakan merupakan profil data nasabah asuransi kendaraan beserta total claim dari masing-masing nasabah yang diperoleh dari link berikut. Data tersebut berisikan 9134 observasi atau sebanyak jumlah nasabah yang dimiliki, beserta 26 kolom. Target variabel pada data ini adalah Total.Claim.Amount, kita akan memprediksi total claim amount untuk setiap nasabah, harapannya perusahaan asuransi dapat mengetahui dana yang harus disiapkan untuk membayar klaim. insurance &lt;- read.csv(&quot;assets/04-insurance/Auto_Insurance_Claims_Sample.csv&quot;) head(insurance) #&gt; Customer Country State.Code State Claim.Amount Response Coverage Education #&gt; 1 BU79786 US KS Kansas 276.3519 No Basic Bachelor #&gt; 2 QZ44356 US NE Nebraska 697.9536 No Extended Bachelor #&gt; 3 AI49188 US OK Oklahoma 1288.7432 No Premium Bachelor #&gt; 4 WW63253 US MO Missouri 764.5862 No Basic Bachelor #&gt; 5 HB64268 US KS Kansas 281.3693 No Basic Bachelor #&gt; 6 OC83172 US IA Iowa 825.6298 Yes Basic Bachelor #&gt; Effective.To.Date EmploymentStatus Gender Income Location.Code Marital.Status #&gt; 1 2/24/11 Employed F 56274 Suburban Married #&gt; 2 1/31/11 Unemployed F 0 Suburban Single #&gt; 3 2/19/11 Employed F 48767 Suburban Married #&gt; 4 1/20/11 Unemployed M 0 Suburban Married #&gt; 5 2/3/11 Employed M 43836 Rural Single #&gt; 6 1/25/11 Employed F 62902 Rural Married #&gt; Monthly.Premium.Auto Months.Since.Last.Claim Months.Since.Policy.Inception #&gt; 1 69 32 5 #&gt; 2 94 13 42 #&gt; 3 108 18 38 #&gt; 4 106 18 65 #&gt; 5 73 12 44 #&gt; 6 69 14 94 #&gt; Number.of.Open.Complaints Number.of.Policies Policy.Type Policy Claim.Reason #&gt; 1 0 1 Corporate Auto Corporate L3 Collision #&gt; 2 0 8 Personal Auto Personal L3 Scratch/Dent #&gt; 3 0 2 Personal Auto Personal L3 Collision #&gt; 4 0 7 Corporate Auto Corporate L2 Collision #&gt; 5 0 1 Personal Auto Personal L1 Collision #&gt; 6 0 2 Personal Auto Personal L3 Hail #&gt; Sales.Channel Total.Claim.Amount Vehicle.Class Vehicle.Size #&gt; 1 Agent 384.8111 Two-Door Car Medsize #&gt; 2 Agent 1131.4649 Four-Door Car Medsize #&gt; 3 Agent 566.4722 Two-Door Car Medsize #&gt; 4 Call Center 529.8813 SUV Medsize #&gt; 5 Agent 138.1309 Four-Door Car Medsize #&gt; 6 Web 159.3830 Two-Door Car Medsize 5.1.2.2 Exploratory Data Selanjutnya melihat structure data dari masing-masing variabel, jika terdapat variabel yang belum sesuai tipe datanya perlu dilakukan explicit coercion. str(insurance) #&gt; &#39;data.frame&#39;:\t9134 obs. of 26 variables: #&gt; $ Customer : chr &quot;BU79786&quot; &quot;QZ44356&quot; &quot;AI49188&quot; &quot;WW63253&quot; ... #&gt; $ Country : chr &quot;US&quot; &quot;US&quot; &quot;US&quot; &quot;US&quot; ... #&gt; $ State.Code : chr &quot;KS&quot; &quot;NE&quot; &quot;OK&quot; &quot;MO&quot; ... #&gt; $ State : chr &quot;Kansas&quot; &quot;Nebraska&quot; &quot;Oklahoma&quot; &quot;Missouri&quot; ... #&gt; $ Claim.Amount : num 276 698 1289 765 281 ... #&gt; $ Response : chr &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; ... #&gt; $ Coverage : chr &quot;Basic&quot; &quot;Extended&quot; &quot;Premium&quot; &quot;Basic&quot; ... #&gt; $ Education : chr &quot;Bachelor&quot; &quot;Bachelor&quot; &quot;Bachelor&quot; &quot;Bachelor&quot; ... #&gt; $ Effective.To.Date : chr &quot;2/24/11&quot; &quot;1/31/11&quot; &quot;2/19/11&quot; &quot;1/20/11&quot; ... #&gt; $ EmploymentStatus : chr &quot;Employed&quot; &quot;Unemployed&quot; &quot;Employed&quot; &quot;Unemployed&quot; ... #&gt; $ Gender : chr &quot;F&quot; &quot;F&quot; &quot;F&quot; &quot;M&quot; ... #&gt; $ Income : int 56274 0 48767 0 43836 62902 55350 0 14072 28812 ... #&gt; $ Location.Code : chr &quot;Suburban&quot; &quot;Suburban&quot; &quot;Suburban&quot; &quot;Suburban&quot; ... #&gt; $ Marital.Status : chr &quot;Married&quot; &quot;Single&quot; &quot;Married&quot; &quot;Married&quot; ... #&gt; $ Monthly.Premium.Auto : int 69 94 108 106 73 69 67 101 71 93 ... #&gt; $ Months.Since.Last.Claim : int 32 13 18 18 12 14 0 0 13 17 ... #&gt; $ Months.Since.Policy.Inception: int 5 42 38 65 44 94 13 68 3 7 ... #&gt; $ Number.of.Open.Complaints : int 0 0 0 0 0 0 0 0 0 0 ... #&gt; $ Number.of.Policies : int 1 8 2 7 1 2 9 4 2 8 ... #&gt; $ Policy.Type : chr &quot;Corporate Auto&quot; &quot;Personal Auto&quot; &quot;Personal Auto&quot; &quot;Corporate Auto&quot; ... #&gt; $ Policy : chr &quot;Corporate L3&quot; &quot;Personal L3&quot; &quot;Personal L3&quot; &quot;Corporate L2&quot; ... #&gt; $ Claim.Reason : chr &quot;Collision&quot; &quot;Scratch/Dent&quot; &quot;Collision&quot; &quot;Collision&quot; ... #&gt; $ Sales.Channel : chr &quot;Agent&quot; &quot;Agent&quot; &quot;Agent&quot; &quot;Call Center&quot; ... #&gt; $ Total.Claim.Amount : num 385 1131 566 530 138 ... #&gt; $ Vehicle.Class : chr &quot;Two-Door Car&quot; &quot;Four-Door Car&quot; &quot;Two-Door Car&quot; &quot;SUV&quot; ... #&gt; $ Vehicle.Size : chr &quot;Medsize&quot; &quot;Medsize&quot; &quot;Medsize&quot; &quot;Medsize&quot; ... Berikutnya kita perlu inspect persebaran data yang dimilih baik data kategorik dan numerik, kita dapat menggunakan package inspectdf untuk eksplorasi berikut ini. insurance %&gt;% inspect_cat() %&gt;% show_plot() insurance %&gt;% inspect_num() %&gt;% show_plot() Dari hasil kedua plot diatas berikutnya membuang variabel yang tidak dibutuhkan dalam model. Variabel customer merupakan data unique dari ID setiap customer, oleh karena itu kita akan membuang variabel tersebut. Variabel country tidak banyak memberikan informasi, karena semua observasi berisikan informasi yang sama. Variabel State.Code juga memberikan informasi yang sama dengan variabel State, oleh karena itu kita akan menggunakan salah satu dari kedua variabel tersebut yaitu variabel State. Sedangkan untuk variabel Policy kita hilangkan karena informasi yang diberikan juga sama dengan variabel Policy.Type. insurance &lt;- insurance %&gt;% select(-c(Customer, Country, State.Code, Effective.To.Date, Policy)) Selanjutnya, split data menjadi data train dan data test dengan proporsi 80:20. set.seed(100) idx &lt;- initial_split(data = insurance,prop = 0.8) claim_train &lt;- training(idx) claim_test &lt;- testing(idx) 5.1.2.3 Modelling Kemudian bentuk model random forest, tentukan target variabel dan prediktor yang digunakan. library(randomForest) forest_claim &lt;- randomForest(Total.Claim.Amount~.,data = claim_train, localImp = TRUE) #saveRDS(forest_claim,&quot;forest_claim.RDS&quot;) forest_claim &lt;- readRDS(&quot;assets/04-insurance/forest_claim.RDS&quot;) forest_claim #&gt; #&gt; Call: #&gt; randomForest(formula = Total.Claim.Amount ~ ., data = claim_train, localImp = TRUE) #&gt; Type of random forest: regression #&gt; Number of trees: 500 #&gt; No. of variables tried at each split: 6 #&gt; #&gt; Mean of squared residuals: 13616.34 #&gt; % Var explained: 84.36 Model memiliki kemampuan menjelaskan variasi data sebesar 84.8%, sedangkan sisanya sebesar 15.2% dijelaskan oleh variabel lain yang tidak digunakan pada model. Untuk mengetahui variabel yang paling berpengaruh pada model, kita dapat melihat variabel importance. varImpPlot(forest_claim, main = &quot;Variable Importance&quot;,n.var = 5) Nilai importance atau tingkat kepentingannya terdapat dua penilaian yaitu IncMSE dan IncNodePurity. Untuk IncMSE diperoleh dari error pada OOB (out of bag) data, kemudian di rata-ratakan untuk semua pohon, dan dinormalisasi dengan standar deviasi. Untuk IncNodePurity merupakan total penurunan impurity dari masing-masing variabel. Untuk kasus klasifikasi node impurity diperoleh dari nilai gini index, sedangkan untuk kasus regresi diperoleh dari SSE (Sum Square Error). Untuk mengetahui peran variabel dalam pembuatan model, kita dapat memanfaatkan package randomForestExplainer yang menyediakan beberapa function untuk memperoleh informasi mengenai variabel importance. mindepth_frame &lt;- min_depth_distribution(forest_claim) #saveRDS(mindepth_frame, &quot;mindepthframe.rds&quot;) mindepth_frame &lt;- readRDS(&quot;assets/04-insurance/mindepthframe.rds&quot;) plot_min_depth_distribution(mindepth_frame, mean_sample = &quot;top_trees&quot;) Plot tersebut memberikan informasi mengenai nilai mean minimal dept untuk setiap variabel. Semakin kecil nilai minimal depth artinya semakin penting variabel tersebut pada model. Semakin besar proporsi minimal dept pada warna merah mudah (mendekati 0), artinya variabel tersebut sering dijadikan sebagai root node, yaitu variabel utama yang digunakan untuk menentukan nilai target. imp_frame &lt;- measure_importance(forest_claim) #saveRDS(imp_frame,&quot;imp_frame.rds&quot;) imp_frame &lt;- readRDS(&quot;assets/04-insurance/imp_frame.rds&quot;) plot_multi_way_importance(imp_frame, size_measure = &quot;no_of_nodes&quot;,no_of_labels = 6) plot_multi_way_importance(imp_frame, x_measure = &quot;mse_increase&quot;, size_measure = &quot;p_value&quot;, no_of_labels = 6) Perbandingan dari ketiga plot, terdapat 5 variabel yaitu location code, monthly premium auto, vehicle class, income, dan claim amount yang selalu muncul dari ketiga plot tersebut. Artinya kelima variabel tersebut dapat dikatakan variabel yang paling berpengaruh dan banyak digunakan dalam pembuatan pohon. Berikutnya lakukan prediksi untuk data test, kemudian cari nilai error dari hasil prediksi claim_test$pred &lt;- predict(object = forest_claim,newdata = claim_test) Mencari nilai RMSE (Root Mean Squared Error) MLmetrics::RMSE(y_pred = claim_test$pred,y_true = claim_test$Total.Claim.Amount) #&gt; [1] 106.1973 RMSE merupakan nilai rata rata dari jumlah kuadrat error yang menyatakan ukuran besarnya kesalahan yang dihasilkan oleh model. Nilai RMSE rendah menunjukkan bahwa variasi nilai yang dihasilkan oleh model mendekasi variasi nilai observasinya. Jika dilihat dari 5 number summary variabel total claim amount, nilai RMSE yang diperoleh sebesar 119.9 dapat dikatakan sudah cukup baik. 5.1.3 Conclusion Untuk memprediksi nilai Total Claim Amount model ini memiliki kemampuan menjelaskan variasi data sebesar 84.8% dan variabel yang paling mempengaruhi target adalah variabel location code, monthly premium auto, vehicle class, income, dan claim amount. Hasil error yang diperoleh dari model tersebut cukup baik dalam memprediksi data. "],
["bioinformatics.html", "Chapter 6 Bioinformatics 6.1 QTL Mapping for Disease Resistance", " Chapter 6 Bioinformatics 6.1 QTL Mapping for Disease Resistance 6.1.1 Backgorund Ketahanan pangan merupakan salah satu prioritas utama dalam Rancangan Pembangunan Jangka Panjang Menengah Nasional. Ketersediaan pangan strategis sangat diandalkan dalam upaya mewujudkan ketahanan pangan. Pangan strategis dapat diartikan sebagai komoditas pangan yang terkait dengan kebutuhan sebagian besar masyarakat. Salah satu contoh komoditas pangan strategis menurut kementerian pertanian Indonesia adalah cabai. Sering kali petani cabai mengalami gagal panen dikarenakan serangan penyakit bakteri dan jamur pada akar dan daun. Salah satu penyakit yang dominan yang menyerang tanaman cabai adalah jamur Phytophthora capsici. Perlu dilakukan sebuah penelitian untuk mengetahui letak gen yang mempengaruhi sifat rentan terhadap jamur Phytophthora capsici. Beberapa hasil penelitian telah membuktikan analisis Quantitative Trait Locus sukses mengidentifikasi sifat pada tanaman. Maka dari itu sebagai bahan kajian peningkatan kualitas tanaman cabai, menggunakan data sekuen DNA yang sudah tersedia, dengan pendekatan metode QTL akan diidentifikasi letak gen yang berhubungan secara signifikan terhadap penyakit Phytophthora capsici. 6.1.2 Modelling Analysis 6.1.2.1 Import Data pacman::p_load(&quot;ASMap&quot;,&quot;qtlcharts&quot;,&quot;qtl&quot;, &quot;ggplot2&quot;,&quot;ggpubr&quot;,&quot;ggdendro&quot;, &quot;dendextend&quot;, &quot;factoextra&quot;, &quot;car&quot;, &quot;igraph&quot;) dataset &lt;- read.cross( format = &quot;csv&quot;, dir = &quot;assets/05-bioinformatics/&quot;, file =&quot;data.csv&quot;, sep = &quot;;&quot;, genotypes = c(&quot;a&quot;,&quot;h&quot;,&quot;b&quot;), alleles = c(&quot;a&quot;,&quot;b&quot;) ) #&gt; --Read the following data: #&gt; 296 individuals #&gt; 26 markers #&gt; 1 phenotypes #&gt; --Cross type: f2 summary(dataset) #&gt; F2 intercross #&gt; #&gt; No. individuals: 296 #&gt; #&gt; No. phenotypes: 1 #&gt; Percent phenotyped: 100 #&gt; #&gt; No. chromosomes: 1 #&gt; Autosomes: 5 #&gt; #&gt; Total markers: 26 #&gt; No. markers: 26 #&gt; Percent genotyped: 100 #&gt; Genotypes (%): aa:24.0 ab:48.4 bb:27.6 not bb:0.0 not aa:0.0 Diatas merupakan hasil ringkasan dari data. Populasi yang digunakan hasil persilangan tipe F2 intercross yang menghasilkan individu sebanyak 296 jenis. Data fenotipe menjelaskan tentang skoring ketahanan tanaman cabai terhadap Phytophthora capsici. Interval skornya diantara 0 sampai dengan 5. Tanaman cabai yang memiliki resisten terhadap Phytophthora capsici akan diberi skor 0, sedangkan yang rentan akan diberi skor 5. Sebanyak 100% data fenotipe berhasil terbaca, artinya tidak ada data yang hilang (NA). Pada data ini hanya menggunakan 1 kromosom yang diamati, yaitu kromosom ke-5 dari total 11 kromosom cabai. ggplot(data = dataset$pheno, mapping = aes(x = dataset$pheno$Score)) + geom_density() + labs( title = &quot;Distribusi sebaran skor fenotipe&quot;, x = &quot;Skor ketahanan *Phytophthora capsici*&quot; ) + theme_minimal() shapiro.test(dataset$pheno$Score) #&gt; #&gt; Shapiro-Wilk normality test #&gt; #&gt; data: dataset$pheno$Score #&gt; W = 0.78902, p-value &lt; 0.00000000000000022 Berdasarkan kurva dan uji hipotesis, maka keputusannya data fenotipe tidak berdistribusi normal (distribusi bimodal). Menurut penelitian yang dilakukan Margawati (2015) ketika kurva fenotipe menunjukkan distribusi bimodal, maka itu merupakan indikasi terdapat gen mayor (yang signifikan mempengaruhi). 6.1.2.2 Exploratory Data Analysis cg &lt;- comparegeno(dataset) #dendogram clastering dataclust &lt;- abs(cg - 1) # Dissimilarity matrix df &lt;- scale(dataclust) d &lt;- dist(df, method = &quot;euclidean&quot;) hc3 &lt;- hclust(d, method = &quot;ward.D2&quot;) #phylogenetic tree dend_plot &lt;- fviz_dend( hc3, k = 4, # Cut in four groups cex = 0.5, # label size k_colors = &quot;jco&quot; ) # extract the dendrogram data dend_data &lt;- attr(dend_plot, &quot;dendrogram&quot;) # Cut the dendrogram at height h = 10 dend_cuts &lt;- cut(dend_data, h = 90) # Plot subtree 3 fviz_dend( dend_cuts$lower[[3]], main = &quot;Subtree 3&quot;, lwd = 1.3, ggtheme = theme_bw(), horiz = TRUE ) Berdasarkan hasil clusternig di atas, dapat diketahui individu cabai dengan nomor 173, 185, 202, 247, 251, 291, 14, dan 216 berada pada satu cluster yang sama. Hal ini masuk akal, karena individu tersebut berasal dari tetua yang sama tipenya, sehingga karakteristiknya hampir sama. Jika antar individu tidak memiliki kemiripan genotipe, maka akan terpisah jauh atau berada pada branch yang berbeda. 6.1.2.3 QTL Analysis Interval Mapping atau pemetaan interval menjadi pendekatan yang populer pada analisis QTL. Dalam interval mapping, masing-masing penanda sekuen akan dihitung nilai Logarithm of the Odds (LOD). Mudahnya, skor LOD adalah nilai statistik yang digunakan pada data genetika untuk mengukur apakah 2 gen atau lebih yang sedang diamati cenderung terletak berdekatan satu sama lain atau tidak. Skor LOD 3 atau lebih secara umum dapat dipahami bahwa 2 gen tersebut terletak berdekatan pada kromosom. # Marker regression dataset_rf &lt;- est.rf(dataset, maxit = 200, tol = 1e-8) out.mr &lt;- scanone(dataset_rf, method = &quot;mr&quot;) # Harley-knott regression datalink_1 &lt;- calc.genoprob(dataset_rf, step=1, error.prob=0.001, map.function = &quot;haldane&quot;) out.hk &lt;- scanone(datalink_1, method=&quot;hk&quot;) # Multiple Imputation set.seed(1997) datalink_2 &lt;- sim.geno(dataset_rf, step=1, error.prob=0.001) out.imp &lt;- scanone(datalink_2, method=&quot;imp&quot;) par(mfrow=c(1,1)) plot(out.imp, out.hk, out.mr, ylab=&quot;LOD Score&quot;, lty = c(1,1,2), col = c(&quot;black&quot;, &quot;blue&quot;, &quot;red&quot;), main = &quot;Perbandingan metode IMP, HK, EHK&quot;, lwd = 2.5, ylim = c(0,10)) legend(&quot;topleft&quot;, legend=c(&quot;Multiple Imputation&quot;, &quot;Harley-Knott Regression&quot;,&quot;Extented HK&quot;), col=c(&quot;black&quot;, &quot;blue&quot;, &quot;red&quot;), lty=c(1,1,2), cex=0.7, lwd = 2, title = &quot;Metode&quot;) Pemilihan motode terbaik berdasarkan skor panelized LOD yang tertinggi. Hasilnya, metode Imputation memperoleh LOD yang tertinggi yaitu 11.53. Selanjutnya, mencari formula regresi menggunakan metode imputation: dataqtl.step0 &lt;- sim.geno( cross = dataset_rf, step = 0, error.prob = 0.001, map.function = &quot;haldane&quot;, n.draws = 296 ) set.seed(1) outsw1 &lt;- stepwiseqtl(dataqtl.step0, verbose = TRUE, method = &quot;imp&quot;) outsw1 outsw1 &lt;- readRDS(file = &quot;assets/05-bioinformatics/outsw1.RDS&quot;) chr &lt;- c(5,5,5) pos &lt;- c(117.34, 159.31, 256.48) qtl &lt;- makeqtl(dataqtl.step0, chr, pos) my.formula &lt;- y ~ Q1 + Q2 + Q3 + Q1:Q2 out.fitqtl &lt;- fitqtl(dataqtl.step0, qtl=qtl, formula=my.formula, get.ests = F) summary(out.fitqtl) #&gt; #&gt; fitqtl summary #&gt; #&gt; Method: multiple imputation #&gt; Model: normal phenotype #&gt; Number of observations : 296 #&gt; #&gt; Full model result #&gt; ---------------------------------- #&gt; Model formula: y ~ Q1 + Q2 + Q3 + Q1:Q2 #&gt; #&gt; df SS MS LOD %var Pvalue(Chi2) Pvalue(F) #&gt; Model 10 367.1198 36.711980 24.81033 32.0229 0 0 #&gt; Error 285 779.3093 2.734418 #&gt; Total 295 1146.4291 #&gt; #&gt; #&gt; Drop one QTL at a time ANOVA table: #&gt; ---------------------------------- #&gt; df Type III SS LOD %var F value Pvalue(Chi2) Pvalue(F) #&gt; 5@117.3 6 207.13 15.150 18.068 12.625 0.000 0.00000000000125 *** #&gt; 5@159.3 6 162.96 12.205 14.215 9.933 0.000 0.00000000058772 *** #&gt; 5@256.5 2 31.26 2.528 2.726 5.715 0.003 0.00368 ** #&gt; 5@117.3:5@159.3 4 151.64 11.428 13.227 13.864 0.000 0.00000000024024 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Diperoleh formula skor ketahanan terhadap phytophthora capsici sebagai berikut: \\[Formula: y \\sim Q1 + Q2 + Q3 + Q1:Q2 \\] Keterangan: y = Skor fenotipe ketahanan terhadap penyakit phytophthora capsici Qi = Marka QTL ke-i Jika hasil summary model diatas diringkas kedalam bentuk tabel, maka informasinya seperti berikut: tibble( Variabel = c(&quot;Q1&quot;, &quot;Q2&quot;, &quot;Q3&quot;, &quot;Q1:Q2&quot;), `Kode Marka` = c(&quot;PMMCB81&quot;, &quot;PMMCB34&quot;, &quot;MCA32&quot;, &quot;PMMCB81 : PMMCB34&quot;), `% Var` = c(18.25, 13.36, 2.79, 12.62) ) #&gt; [38;5;246m# A tibble: 4 x 3[39m #&gt; Variabel `Kode Marka` `% Var` #&gt; [3m[38;5;246m&lt;chr&gt;[39m[23m [3m[38;5;246m&lt;chr&gt;[39m[23m [3m[38;5;246m&lt;dbl&gt;[39m[23m #&gt; [38;5;250m1[39m Q1 PMMCB81 18.2 #&gt; [38;5;250m2[39m Q2 PMMCB34 13.4 #&gt; [38;5;250m3[39m Q3 MCA32 2.79 #&gt; [38;5;250m4[39m Q1:Q2 PMMCB81 : PMMCB34 12.6 Kolom pertama dan kedua menjelaskan tentang simbol model QTL beserta nama markanya. Kolom persentase variansi (%var) adalah estimasi dari variansi fenotipe yang dijelaskan oleh marka PMMCB81, PMMCB34, MCA32, dan interaksi marka PMMCB81:PMMCB34. Total %var sebesar 46,3%. Mempunyai makna bahwa kemampuan seluruh marka dalam model untuk menjelaskan skor variansi fenotipe ketahanan tanaman cabai terhadap bakteri phytophthora capsici adalah sebesar 46,3%, sedangkan sisanya dijelaskan oleh marka lain diluar penelitian. Visualisasi peta genetik dengan model QTL yang signifikan dan hasi skor LOD metode multiple imputation disajikan pada gambar dibawah ini. par(mfrow=c(1,2)) plot(outsw1, col=&quot;red&quot;, justdots = F, show.marker.names = F) plot(out.imp$lod, out.imp$pos, col=&quot;red&quot;, xlab = &quot;LOD&quot;, ylim = c(315, 0),las = 1, ylab = &quot;Map Position (cM)&quot;, type = &quot;l&quot;, lwd = 3, main = &quot;Interval Mapping&quot;) # abline(v=tresh[4], lty = &quot;dotted&quot;, lwd=3, col=&quot;darkgrey&quot;) legend(&quot;bottomright&quot;, legend=&quot;Multiple Imputation&quot;, col=&quot;red&quot;, lty=1, cex=0.7, lwd = 2, title = &quot;Metode&quot;, bty = &quot;n&quot;) 6.1.3 Conclusion Hasil model QTL yang terbentuk dengan (\\(\\alpha\\) = 5%) formulanya y ~ Q1 + Q2 + Q3 + Q1:Q2 yang secara urut merupakan penanda sekuen (marka) dengan kode PMMCB81, PMMCB34, MCA32, dan interaksi PMMCB81 x PMMCB34. Skor LOD masing-masing model sebesar 5,80 (PMMCB81), 2,74 (PMMCB34) dan 8,6 (MCA32). Model QTL tersebut mampu menjelaskan 46,3% variansi skor fenotipe ketahanan tanaman cabai terhadap jamur phytophthora capsici. Rekomendasi yang dapat diberikan, perlu dilakukan investigasi lebih lanjut pada ketiga marka tersebut untuk memperbaiki atau mengembangkan kultivar tanaman cabai yang resisten terhadap penyakit layu yang disebabkan oleh jamur phytophthora capsici. "],
["public-health.html", "Chapter 7 Public Health 7.1 Survival Analysis of Patients with Lung Cancer", " Chapter 7 Public Health 7.1 Survival Analysis of Patients with Lung Cancer Kanker paru merupakan kanker pada organ pernapasan yang menjadi kanker pembunuh nomer satu di dunia dan Indonesia (CNN Indonesia, 2018). Data internasional dari Globocan 2018 menyatakan kanker paru adalah kanker yang paling banyak ditemukan di pria dan wanita di seluruh dunia dibandingkan jenis kanker lainnya. Pasien penderita kanker paru memerlukan penanganan yang terarah. Oleh karena itu akan dilakukan pengamatan, faktor apa saja yang mempengaruhi waktu ketahanan hidup pasien kanker paru. Metode yang digunakan adalah survival analysis, yaitu analisis statistik untuk mengambil keputusan yang berkaitan dengan waktu sampai dengan terjadinya suatu kejadian khusus (failure event/ end point). Pada bidang studi kanker, hal yang sering jadi perhatian peneliti adalah: Berapa probabilitas individu/pasien untuk survive selama 3 tahun? Apakah terdapat perbedaan kemampuan survive antara kelompok demografi pasien? 7.1.1 Import Data library(tidyverse) library(survival) library(SurvRegCensCov) library(survminer) options(scipen = 9999) Data yang digunakan merupakan data dummy rekam medis dari pasien kanker paru-paru. Sebanyak 137 pasien diobservasi dimana 128 mengalami event meninggal dan sisanya tersensor (dirujuk ke rumah sakit lain). Durasi waktu pengamatan menggukan satuan hari. lung &lt;- read.csv(&quot;assets/06-health/data-paru.csv&quot;, sep = &quot;;&quot;) glimpse(lung) #&gt; Rows: 137 #&gt; Columns: 6 #&gt; $ treatment [3m[38;5;246m&lt;int&gt;[39m[23m 2, 2, 2, 1, 1, 1, 2, 2, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 2, 2~ #&gt; $ survival [3m[38;5;246m&lt;int&gt;[39m[23m 1, 1, 2, 3, 4, 7, 7, 7, 8, 8, 8, 8, 10, 10, 11, 12, 12, 13, 13, 15, 15, 16, ~ #&gt; $ perform [3m[38;5;246m&lt;int&gt;[39m[23m 20, 50, 40, 30, 40, 50, 20, 40, 40, 20, 80, 50, 20, 40, 70, 50, 40, 60, 30, ~ #&gt; $ age [3m[38;5;246m&lt;int&gt;[39m[23m 65, 35, 44, 43, 35, 72, 66, 58, 63, 61, 68, 66, 49, 67, 48, 63, 68, 56, 62, ~ #&gt; $ status [3m[38;5;246m&lt;int&gt;[39m[23m 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1~ #&gt; $ cell [3m[38;5;246m&lt;int&gt;[39m[23m 4, 4, 3, 2, 3, 3, 3, 2, 4, 2, 3, 2, 4, 3, 4, 2, 1, 3, 3, 4, 1, 3, 3, 3, 2, 2~ Berikut adalah penjelasan mengenai beberapa informasi yang diamati: treatment: 1 (standard), 2 (test) cell type: 1 (large), 2 (adeno), 3 (small), 4 (squamoues) survival: waktu pengamatan dalam hari status: 1 (cencored / berhasil survive), 0 (meninggal) Menurut Kementerian Kesehatan, kelompok usia yang paling berisiko tinggi mencakup pasien yang berusia &gt; 40 tahun. Maka, pada data akan dikelompokkan menjadi dua kelompok usia. lung &lt;- lung %&gt;% rename(time = survival) %&gt;% mutate( treatment = factor(treatment, levels = c(1,2), labels = c(&quot;standard&quot;, &quot;test&quot;)), cell = factor(cell, levels = c(1,2,3,4), labels = c(&quot;large&quot;, &quot;adeno&quot;,&quot;small&quot;,&quot;squamous&quot;)), age = case_when( age &gt; 40 ~ &quot;&gt;40&quot;, TRUE ~ &quot;&lt;=40&quot; ) ) 7.1.2 Exploratory Data Analysis Variabel prediktor (treatment, age, perform, cell) akan dianalisis menggunakan regresi survival. Dimana akan dilihat faktor-faktor apa saja yang mempengaruhi ketahanan hidup pasien sampai mengalami sebuah event: meninggal. Namun sebelumnya akan dianalisis menggunakan pendekatan non parametrik yaitu metode Kaplan Meier dan Log-Rank. Kaplan Meier adalah kurva yang menggambarkan hubungan antara waktu pengamatan (survival) dengan estimasi fungsi survival pada waktu ke-t. Kurva yang terbentuk kemudian dibandingkan menggunakan uji Log Rank. Tujuannya untuk mengetahui apakah terdapat perbedaan peluang survive antara level di setiap variabel kategorik. lung_surv &lt;- survfit(Surv(time = time, event = status) ~ 1, data = lung) tibble( time = lung_surv$time, n_risk = lung_surv$n.risk, n_event = lung_surv$n.event, survival = lung_surv$surv ) #&gt; [38;5;246m# A tibble: 101 x 4[39m #&gt; time n_risk n_event survival #&gt; [3m[38;5;246m&lt;dbl&gt;[39m[23m [3m[38;5;246m&lt;dbl&gt;[39m[23m [3m[38;5;246m&lt;dbl&gt;[39m[23m [3m[38;5;246m&lt;dbl&gt;[39m[23m #&gt; [38;5;250m 1[39m 1 137 2 0.985 #&gt; [38;5;250m 2[39m 2 135 1 0.978 #&gt; [38;5;250m 3[39m 3 134 1 0.971 #&gt; [38;5;250m 4[39m 4 133 1 0.964 #&gt; [38;5;250m 5[39m 7 132 3 0.942 #&gt; [38;5;250m 6[39m 8 129 4 0.912 #&gt; [38;5;250m 7[39m 10 125 2 0.898 #&gt; [38;5;250m 8[39m 11 123 1 0.891 #&gt; [38;5;250m 9[39m 12 122 2 0.876 #&gt; [38;5;250m10[39m 13 120 2 0.861 #&gt; [38;5;246m# ... with 91 more rows[39m Baris pertama output menyatakan pada waktu pengamatan hari pertama, ada 137 pasien, 2 diantaranya meninggal pada saat itu. Peluang survive diperoleh menggunakan perhitungan \\((137-2)/137 = 0.985\\). Pada garis kedua yang menyatakan observasi di hari ke-2, terdapat 135 pasien, dimana satu orang diantaranya meninggal pada saat itu. Peluang survivalnya diperoleh dengan perhitungan \\((135-1)/137 = 0.978\\). Tabel informasi diatas jika divisualisasikan tampilannya akan seperti berikut: ggsurvplot( lung_surv, color = &quot;#2E9FDF&quot;, ggtheme = ggthemes::theme_pander()) + labs(title = &quot;Kaplan-Meier Curves all variable&quot;) Hasil di atas adalah kurva survival untuk kesuluruhan parameter. Sumbu vertikal merupakan peluang survival dan sumbu horizontal adalah waktu pengamatan. Berdasarkan grafik, terlihat jelas bahwa makin jauh waktu pengamatan, peluang survive akan semakin kecil. Masing-masing variabel juga dapat dicari fungsi survivalnya, untuk memperoleh insight apakah tiap kelompok variabel terdapat perbedaan peluang survive yang signifikan. km_cell &lt;- survfit(Surv(time = time, event = status) ~ cell, data = lung) ggsurvplot( km_cell, ggtheme = ggthemes::theme_pander()) + labs(title = &quot;Kaplan-Meier Curves for Cell Type Group&quot;) Grafik diatas cukup menjelaskan bahwa keempat kelompok cell type memiliki perbedaan garis yang cukup signifikan. Maka, dapat diduga bahwa kelompok cell type pada data observasi memiliki perbedaan yang signifikan terhadap status survive pasien kanker paru. Akan dilakukan uji eksak, menggunakan Log-Rank untuk memperkuat identifikasi berdasarkan grafik. # log rank cell type lr_cell &lt;- survdiff(Surv(time ,status)~ cell, data = lung) lr_cell #&gt; Call: #&gt; survdiff(formula = Surv(time, status) ~ cell, data = lung) #&gt; #&gt; N Observed Expected (O-E)^2/E (O-E)^2/V #&gt; cell=large 27 26 34.5 2.12 3.02 #&gt; cell=adeno 27 26 15.7 6.77 8.19 #&gt; cell=small 48 45 30.1 7.37 10.20 #&gt; cell=squamous 35 31 47.7 5.82 10.53 #&gt; #&gt; Chisq= 25.4 on 3 degrees of freedom, p= 0.00001 Tingkat signifikansi yang digunakan adalah sebesar 5% (0.05). Berdasarkan uji Log-Rank, diperoleh p-value (0.0001) &lt; alpha (0.05) maka kesimpulannya terdapat perbedaan yang signifikan pada fungsi survival keempat kelompok cell. km_treatment &lt;- survfit(Surv(time = time, event = status) ~ treatment, data = lung) ggsurvplot( km_treatment, ggtheme = ggthemes::theme_pander()) + labs(title = &quot;Kaplan-Meier Curves for Treatment group&quot;) Pada hasil grafik di atas, antara pria dan wanita memiliki jarak yang berdekatan. Artinya tidak terdapat perbedaan yang signifikan untuk waktu survivalnya. Untuk memperkuat interpretasi berdasarkan grafik, perlu dilakukan uji hipotesis dengan menggunakan perhitungan eksak, yakni metode Log-Rank. # log rank treatment lr_treatment &lt;- survdiff(Surv(time ,status)~ treatment, data = lung) lr_treatment #&gt; Call: #&gt; survdiff(formula = Surv(time, status) ~ treatment, data = lung) #&gt; #&gt; N Observed Expected (O-E)^2/E (O-E)^2/V #&gt; treatment=standard 69 64 64.5 0.00388 0.00823 #&gt; treatment=test 68 64 63.5 0.00394 0.00823 #&gt; #&gt; Chisq= 0 on 1 degrees of freedom, p= 0.9 Berdasarkan hasil uji Log-Rank diperoleh p-value (0.9) &gt; alpha (0.05) sehingga kesimpulannya tidak terdapat perbedaan yang signifikan untuk kelompok variabel treatment. Artinya baik treament standard maupun treatment test, tidak mempengaruhi waktu survive dari pasien. Adapun variabel lainnya, kelompok usia dan kelompok performa setelah di uji menggunakan Log-Rank diperoleh kesimpulan yang sama yakni, terdapat perbedaan waktu survive yang signifikan untuk kelompok dua variabel tersebut. # log rank age lr_age &lt;- survdiff(Surv(time ,status)~ age, data = lung) lr_age #&gt; Call: #&gt; survdiff(formula = Surv(time, status) ~ age, data = lung) #&gt; #&gt; N Observed Expected (O-E)^2/E (O-E)^2/V #&gt; age=&lt;=40 12 11 7.44 1.703 1.86 #&gt; age=&gt;40 125 117 120.56 0.105 1.86 #&gt; #&gt; Chisq= 1.9 on 1 degrees of freedom, p= 0.2 # log rank perform lr_perform &lt;- survdiff(Surv(time ,status)~ perform, data = lung) lr_perform #&gt; Call: #&gt; survdiff(formula = Surv(time, status) ~ perform, data = lung) #&gt; #&gt; N Observed Expected (O-E)^2/E (O-E)^2/V #&gt; perform=10 1 1 0.463 0.6223 0.6334 #&gt; perform=20 7 7 1.037 34.2803 35.8740 #&gt; perform=30 14 14 4.957 16.4965 17.9562 #&gt; perform=40 16 15 7.686 6.9613 7.5756 #&gt; perform=50 14 13 12.155 0.0588 0.0668 #&gt; perform=60 27 26 25.341 0.0171 0.0217 #&gt; perform=70 23 21 29.832 2.6146 3.5106 #&gt; perform=75 2 2 1.522 0.1501 0.1540 #&gt; perform=80 24 22 25.989 0.6121 0.7983 #&gt; perform=85 1 1 0.845 0.0284 0.0290 #&gt; perform=90 7 6 17.460 7.5222 9.8316 #&gt; perform=99 1 0 0.713 0.7132 0.7274 #&gt; #&gt; Chisq= 78.2 on 11 degrees of freedom, p= 0.000000000003 7.1.3 Modelling Analysis Pada analisa sebelumnya, hanya dibandingkan tiap pengamatan pasien terhadap masing-masing variabelnya saja. Kali ini akan di uji apakah pasien kanker paru-paru memiliki tingkat ketahanan hidup (survive) yang berbeda berdasarkan penyebab tipe sel, performa, dan usia pasien. Pemodelan menggunakan pendekatan 2 metode, yaitu regresi weibull dan regresi log logistik. Masing-masing hasil ringkasan informasinya dapat dilihat pada output dibawah: regweibull &lt;- survreg(Surv(time, status) ~ age + cell + perform, data = lung, dist = &quot;weibull&quot;) summary(regweibull) #&gt; #&gt; Call: #&gt; survreg(formula = Surv(time, status) ~ age + cell + perform, #&gt; data = lung, dist = &quot;weibull&quot;) #&gt; Value Std. Error z p #&gt; (Intercept) 2.68750 0.44076 6.10 0.000000001078 #&gt; age&gt;40 0.51373 0.29711 1.73 0.0838 #&gt; celladeno -0.78682 0.26017 -3.02 0.0025 #&gt; cellsmall -0.42331 0.23926 -1.77 0.0769 #&gt; cellsquamous 0.29544 0.24794 1.19 0.2334 #&gt; perform 0.02945 0.00455 6.48 0.000000000092 #&gt; Log(scale) -0.07563 0.06617 -1.14 0.2531 #&gt; #&gt; Scale= 0.927 #&gt; #&gt; Weibull distribution #&gt; Loglik(model)= -715.2 Loglik(intercept only)= -748.1 #&gt; Chisq= 65.72 on 5 degrees of freedom, p= 0.0000000000008 #&gt; Number of Newton-Raphson Iterations: 5 #&gt; n= 137 reglog &lt;- survreg(Surv(time, status) ~ age + cell + perform, data = lung, dist = &quot;loglogistic&quot;) summary(reglog) #&gt; #&gt; Call: #&gt; survreg(formula = Surv(time, status) ~ age + cell + perform, #&gt; data = lung, dist = &quot;loglogistic&quot;) #&gt; Value Std. Error z p #&gt; (Intercept) 2.0251 0.4312 4.70 0.00000264074529785 #&gt; age&gt;40 0.5293 0.3245 1.63 0.1028 #&gt; celladeno -0.7763 0.2605 -2.98 0.0029 #&gt; cellsmall -0.7438 0.2446 -3.04 0.0024 #&gt; cellsquamous -0.0388 0.2656 -0.15 0.8839 #&gt; perform 0.0359 0.0044 8.16 0.00000000000000033 #&gt; Log(scale) -0.5509 0.0740 -7.44 0.00000000000009883 #&gt; #&gt; Scale= 0.576 #&gt; #&gt; Log logistic distribution #&gt; Loglik(model)= -711.3 Loglik(intercept only)= -750.3 #&gt; Chisq= 78.02 on 5 degrees of freedom, p= 0.0000000000000022 #&gt; Number of Newton-Raphson Iterations: 4 #&gt; n= 137 Kriteria yang digunakan dalam pemilihan model terbaik dilihat dari nilai Akaike Information Criterion (AIC). Model yang lebih layak digunakan ketika nilai AIC yang semakin rendah. AIC sangat berguna saat harus memilih model terbaik dari himpunan data yang sama. AIC yang diperoleh dari masing-masing metode: AIC(regweibull) #&gt; [1] 1444.466 AIC(reglog) #&gt; [1] 1436.513 Model terbaik diperoleh dari model regresi log logistic, dengan AIC paling terendah yaitu 1435,513: \\[S(t|x)=\\frac{1}{1 + (exp([-2.0251 + 0.5293_{age} - 0.7763_{cellAdeno} - 0.7438_{cellSmall} - 0.0388_{cellSquamous} + 0.0359_{perform})]*t)^{0.576}}\\] Jika dilakukan simulasi perhitungan peluang survive untuk dua pasien dengan karakteristik: Usia 20 tahun, cell Adeno, skor performa = 20; Usia 50 tahun, cell Squamous, skor performa = 70; pada saat hari ke-100, diperoleh hasil sebagai berikut: 1/(exp(-2.0251 + 0.5293 + 0.7438 + 0.0359 * 70)*100)^0.576 #&gt; [1] 0.02555521 # pasien 1 1/(exp(-2.0251 + 0.5293 + 0.7763 + 0.0359 * 70)*100)^0.576 #&gt; [1] 0.02508127 # pasien 2 1/(exp(-2.0251 + 0.5293 + 0.038 + 0.0359 * 70)*100)^0.576 #&gt; [1] 0.03837407 Maka, pasien usia lebih dari 40 tahub dengan tipe cell squamous pada saat 100 hari mengidap kanker paru peluang bertahan hidup (survive) untuk bertahan hidup lebih tinggi dibandingkan dengan tipe adino. 7.1.4 Conclusion Model regresi survival yang sesuai dengan data pengamatan adalah regresi log logistik. Faktor yang signifikan mempengaruhi laju ketahanan hidup pasien paru-paru berdasarkan data yang diamati, antara lain: usia, tipe sel, dan performa. Pasien dengan tipe sel adeno memiliki risiko paling tinggi dibandingkan lainnya. Dengan hasil pemodelan ini harapannya dapat dijadikan kajian awal untuk meningkatkan tingkat ketahanan hidup pasien paru-paru. "],
["media.html", "Chapter 8 Media 8.1 Causal Impact on Leads generation", " Chapter 8 Media 8.1 Causal Impact on Leads generation 8.1.1 Background Dalam proses bisnis, tim marketing mempunyai peran untuk meningkatkan brand awereness sebuah produk yang dijual. Ketika upaya menarik perhatian pelanggan untuk mencari tahu produk atau layanan yang disediakan sukses akan menghasilkan sebuah leads. Simpelnya, leads adalah orang-orang yang tertarik pada produk atau layanan bisnis. Di era digital, leads dapat diartikan sebagai orang yang mengunjungi website secara langsung maupun melalui iklan, orang yang melakukan like, share terhadap konten atau kampanye yang sedang dilakukan. Selanjutnya, prospek ketertarikan ini nantinya akan disimpan untuk kemudian diarahkan kepada tim sales. Banyak sekali upaya yang dapat dilakukan untuk menghasilkan sebuah leads. Mulai dari membuat konten kreatif, iklan, menulis artikel, membagikan ebook, kode prome dan lain sebagainya. Perlu dilakukan analisa seberapa efektif kampanye yang dilakukan untuk menghasilkan peningkatan leads. Causal Impact adalah sebuah analisis yang dapat digunakan untuk mencari kesimpulan secara statistik apakah ada perbedaan yang signifikan untuk lead generation dari periode sebelum kampanye dilakukan. Kesimpulan yang dapat diperoleh, apakah leads tersebut adalah hasil dari kampanye yang dilakukan, atau berasal dari faktor lain yang tidak teramati. 8.1.2 Modelling Analysis library(tidyverse) library(CausalImpact) library(readxl) library(forecast) library(TSstudio) Data berasal dari hasil googleanalytics sebuah website. Untuk pemodelan post-period, akan digunakan 37 hari sebelumnya sebagai data training. Pertanyaan bisnisnya yaitu pada hari ke-38 dan seterusnya, setelah kampanye dilakukan, apakah memperoleh peningkatan leads yang signifikan? data &lt;- read_csv(&quot;assets/09-media/analytics.csv&quot;) glimpse(data) #&gt; Rows: 54 #&gt; Columns: 9 #&gt; $ page [3m[38;5;246m&lt;chr&gt;[39m[23m &quot;9fb22be32c347a5acd1d3724b0dae726&quot;, &quot;9fb22be32c347a5acd1d3724b0dae72~ #&gt; $ datetime [3m[38;5;246m&lt;dbl&gt;[39m[23m 20180516, 20180517, 20180518, 20180519, 20180520, 20180521, 20180522~ #&gt; $ page_display [3m[38;5;246m&lt;dbl&gt;[39m[23m 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~ #&gt; $ unique_views [3m[38;5;246m&lt;dbl&gt;[39m[23m 1456, 1392, 1281, 658, 616, 1691, 1488, 1316, 1224, 1140, 60, 1549, ~ #&gt; $ average_page_time [3m[38;5;246m&lt;dbl&gt;[39m[23m 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~ #&gt; $ tickets [3m[38;5;246m&lt;dbl&gt;[39m[23m 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~ #&gt; $ bounce_rate [3m[38;5;246m&lt;dbl&gt;[39m[23m 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~ #&gt; $ exit_percentage [3m[38;5;246m&lt;dbl&gt;[39m[23m 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~ #&gt; $ page_value [3m[38;5;246m&lt;dbl&gt;[39m[23m 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~ Kita pilih beberapa kolom yang menjadi fokus analisis ini, yaitu datetime (harian) dan unique views yang berisi informasi berapa banyak orang yang mengunjungi halaman website tersebut. actual &lt;- data %&gt;% mutate( datetime = lubridate::as_datetime(as.character(datetime)) ) %&gt;% dplyr::select(datetime, unique_views) %&gt;% na.omit() head(actual) #&gt; [38;5;246m# A tibble: 6 x 2[39m #&gt; datetime unique_views #&gt; [3m[38;5;246m&lt;dttm&gt;[39m[23m [3m[38;5;246m&lt;dbl&gt;[39m[23m #&gt; [38;5;250m1[39m 2018-05-16 [38;5;246m00:00:00[39m [4m1[24m456 #&gt; [38;5;250m2[39m 2018-05-17 [38;5;246m00:00:00[39m [4m1[24m392 #&gt; [38;5;250m3[39m 2018-05-18 [38;5;246m00:00:00[39m [4m1[24m281 #&gt; [38;5;250m4[39m 2018-05-19 [38;5;246m00:00:00[39m 658 #&gt; [38;5;250m5[39m 2018-05-20 [38;5;246m00:00:00[39m 616 #&gt; [38;5;250m6[39m 2018-05-21 [38;5;246m00:00:00[39m [4m1[24m691 Leads generation yang dihasilkan paling tinggi ketika hari Rabu. Informasi tersebut dapat dijadikan pertimbangan penentuan hari untuk memulai kampanye. actual %&gt;% mutate( wdays = lubridate::wday(datetime, label = TRUE) ) %&gt;% group_by(wdays) %&gt;% summarise(total_views = sum(unique_views)) %&gt;% ungroup() %&gt;% mutate( label = scales::comma(total_views) ) %&gt;% ggplot( mapping = aes(x = wdays, y = total_views) ) + geom_col(fill = &quot;steelblue&quot;) + labs( title = &quot;Total Views Per Days&quot;, subtitle = &quot;Period: May to July&quot;, y = NULL, x = &quot;Day of Week&quot; ) + theme_minimal() + geom_text( aes(label = label, y = total_views + max(total_views) * 0.075) , size = 3 ) Seperti yang dijelaskan sebelumnya, akan di-subset 37 hari sebelum kampanye diberikan dan disimpan ke objek pre_campaign. Dari sinilah, kita dapat melihat pergerakan leads dan memperikirakan peningkatan yang dihasilkan sejak kampanye dilakukan. pre_campaign &lt;- actual %&gt;% dplyr::slice(1:37) Selanjutnya, membuat objek time series dan melakukan pemodelan untuk menjadikan banchmark dari leads yang dapat kita peroleh jika tidak menggunakan kampanye. Kita akan gunakan Holtwinter sebagai metode untuk melakuan peramalan (forecast) 16 hari kedepan. ts_campaign &lt;- ts(pre_campaign$unique_views, frequency = 7) fit_hw &lt;- HoltWinters(ts_campaign) forecast &lt;- forecast(fit_hw, 16) Kita gabungkan data periode sebelum dilakukan kampanye dan hasil ramalannya yang disimpan ke objek append_data. forecast_data &lt;- data.frame( datetime = lubridate::as_datetime(seq.Date(from = as.Date(&quot;2018-06-24&quot;), by = &quot;day&quot;, length.out = 16)), unique_views = forecast$mean ) append_data &lt;- pre_campaign %&gt;% bind_rows(forecast_data) ggplot(data = append_data, mapping = aes(x = datetime, y = unique_views)) + geom_line(color = &quot;steelblue&quot;, size = 1) + geom_point() + labs( title = &quot;Forecast Projection&quot;, y = &quot;Total Unique Views&quot; ) + theme_minimal() Dan kita juga mempunyai data aktual untuk periode tersebut. Kita perhatikan terjadi peningkatan total pengunjung website. actual %&gt;% ggplot(mapping = aes(x = datetime, y = unique_views)) + geom_line(color = &quot;steelblue&quot;, size = 1) + geom_point() + labs( title = &quot;Data aktual jumlah pengunjung website&quot;, subtitle = &quot;Periode 16 Mei hingga 17 Juli&quot;, y = NULL ) + theme_minimal() Untuk memperkirakan efek kausal, kita mulai dengan menentukan periode mana dalam data yang harus digunakan untuk melatih model (periode pra-intervensi) dan periode mana untuk menghitung prediksi kontrafaktual (periode pasca intervensi). pre &lt;- c(1,37) post &lt;- c(38, 53) Sintaks diatas berarti, poin observasi ke-1 sampai dengan 37 akan digunakan untuk training, dan poin observasi ke 38 hingga 53 untuk menghitung prediksi, atau kita juga bisa mendefinisikannya ke format interval tanggal (date). Kemudian ubah datanya menjadi format matriks sebagai syarat analisis dengan packages CausalImpact. pre &lt;- as.Date(c(&quot;2018-05-16&quot;, &quot;2018-06-24&quot;)) post &lt;- as.Date(c(&quot;2018-06-25&quot;, &quot;2018-07-10&quot;)) time.points &lt;- seq.Date(as.Date(&quot;2018-05-16&quot;), by = &quot;days&quot;, length.out = 53) data_ci &lt;- zoo( cbind(actual$unique_views, append_data$unique_views), time.points ) Sekarang kita sudah memiliki data yang siap untuk memverifikasi efek kausal dari kampanye. impact &lt;- CausalImpact(data = data_ci, pre.period = pre, post.period = post) plot(impact) Secara default, plot berisi dari tiga panel. Panel pertama original menunjukkan data dan prediksi kontrafaktual untuk periode pasca kampanye. Panel kedua pointwise menunjukkan perbedaan antara data aktual yang diamati (leads) dan prediksi. Panel ketiga cumulative menggambarkan efek kumulatif dari intervensi (kampanye) yang dilakukan. Hasil ini memiliki asumsi bahwa hubungan antara leads generation dan deret waktu pengamatan, sebagaimana ditetapkan selama pre-period, tetap stabil sepanjang post-period. Kita dapat lihat informasi statistiknya dengan mengunnakan perintah summary(impact). summary(impact) #&gt; Posterior inference {CausalImpact} #&gt; #&gt; Average Cumulative #&gt; Actual 1417 18424 #&gt; Prediction (s.d.) 984 (54) 12786 (697) #&gt; 95% CI [880, 1095] [11437, 14232] #&gt; #&gt; Absolute effect (s.d.) 434 (54) 5638 (697) #&gt; 95% CI [322, 537] [4192, 6987] #&gt; #&gt; Relative effect (s.d.) 44% (5.5%) 44% (5.5%) #&gt; 95% CI [33%, 55%] [33%, 55%] #&gt; #&gt; Posterior tail-area probability p: 0.00103 #&gt; Posterior prob. of a causal effect: 99.89669% #&gt; #&gt; For more details, type: summary(impact, &quot;report&quot;) Kita dapat memperoleh informasi dari actual dan predicted effect (average) serta efek absolut dan relatifnya. Output informasi statistik di atas mengatakan, leads generation setelah dilakukan kampanye mengalami peningkatan sebesar 44%, dari perkiraan rata-rata pengunjung websitenya sebanyak 984 orang menjadi 1417 kenyataannya. Untuk panduan interpretasi yang benar dari hasil tabel ringkasan, packages CausalImpact menyediakan teks interpretasinya, yang dapat kita print menggunakan perintah: interpretasi &lt;- summary(impact, &quot;report&quot;) print(interpretasi) Hasilnya interpretasi teksnya akan seperti berikut: Analysis report {CausalImpact} During the post-intervention period, the response variable had an average value of approx. 1.42K. By contrast, in the absence of an intervention, we would have expected an average response of 0.98K. The 95% interval of this counterfactual prediction is [0.87K, 1.09K]. Subtracting this prediction from the observed response yields an estimate of the causal effect the intervention had on the response variable. This effect is 0.43K with a 95% interval of [0.33K, 0.54K]. For a discussion of the significance of this effect, see below. Summing up the individual data points during the post-intervention period (which can only sometimes be meaningfully interpreted), the response variable had an overall value of 18.42K. By contrast, had the intervention not taken place, we would have expected a sum of 12.79K. The 95% interval of this prediction is [11.35K, 14.19K]. The above results are given in terms of absolute numbers. In relative terms, the response variable showed an increase of +44%. The 95% interval of this percentage is [+33%, +55%]. This means that the positive effect observed during the intervention period is statistically significant and unlikely to be due to random fluctuations. It should be noted, however, that the question of whether this increase also bears substantive significance can only be answered by comparing the absolute effect (0.43K) to the original goal of the underlying intervention. The probability of obtaining this effect by chance is very small (Bayesian one-sided tail-area probability p = 0.001). This means the causal effect can be considered statistically significant. "]
]
